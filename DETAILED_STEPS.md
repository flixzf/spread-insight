# ì„¸ë¶€ ì‹¤í–‰ ê°€ì´ë“œ (ìŠ¤ëª° ì„ì„¸ìŠ¤ ê¸°ì¤€)

> ê° ë‹¨ê³„ëŠ” **ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰ ë° í…ŒìŠ¤íŠ¸ ê°€ëŠ¥**í•˜ë©°, ì„±ê³µí•´ì•¼ë§Œ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.

---

## ğŸš€ Phase 1: ë‰´ìŠ¤ ìˆ˜ì§‘ (Week 1)

### âœ… Step 1.1: í”„ë¡œì íŠ¸ ì´ˆê¸° ì„¤ì •
**ëª©í‘œ:** ê°œë°œ í™˜ê²½ êµ¬ì¶• ë° ì²« íŒŒì¼ ìƒì„±
**ì†Œìš” ì‹œê°„:** 30ë¶„

#### ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] í´ë” êµ¬ì¡° ìƒì„±
- [ ] ê°€ìƒí™˜ê²½ ì„¤ì •
- [ ] ì˜ì¡´ì„± ì„¤ì¹˜
- [ ] .gitignore ì‘ì„±
- [ ] .env íŒŒì¼ ìƒì„±

#### ğŸ› ï¸ ì‹¤í–‰ ìˆœì„œ

**1-1. í„°ë¯¸ë„ ì—´ê¸° ë° í”„ë¡œì íŠ¸ í´ë”ë¡œ ì´ë™**
```bash
cd "g:\ë‚´ ë“œë¼ì´ë¸Œ\08.Programming\spread_insight"
```

**1-2. í´ë” êµ¬ì¡° ìƒì„± (Windows PowerShell)**
```powershell
# ë©”ì¸ í´ë”ë“¤
New-Item -ItemType Directory -Force -Path scrapers
New-Item -ItemType Directory -Force -Path analyzers
New-Item -ItemType Directory -Force -Path visualizers
New-Item -ItemType Directory -Force -Path publishers
New-Item -ItemType Directory -Force -Path models
New-Item -ItemType Directory -Force -Path database
New-Item -ItemType Directory -Force -Path utils
New-Item -ItemType Directory -Force -Path templates
New-Item -ItemType Directory -Force -Path templates\assets
New-Item -ItemType Directory -Force -Path data
New-Item -ItemType Directory -Force -Path data\raw
New-Item -ItemType Directory -Force -Path data\processed
New-Item -ItemType Directory -Force -Path data\charts
New-Item -ItemType Directory -Force -Path data\html
New-Item -ItemType Directory -Force -Path tests
New-Item -ItemType Directory -Force -Path logs

# __init__.py íŒŒì¼ ìƒì„±
New-Item -ItemType File -Force -Path scrapers\__init__.py
New-Item -ItemType File -Force -Path analyzers\__init__.py
New-Item -ItemType File -Force -Path visualizers\__init__.py
New-Item -ItemType File -Force -Path publishers\__init__.py
New-Item -ItemType File -Force -Path models\__init__.py
New-Item -ItemType File -Force -Path database\__init__.py
New-Item -ItemType File -Force -Path utils\__init__.py
```

**1-3. Git ì´ˆê¸°í™”**
```bash
git init
```

**1-4. .gitignore ì‘ì„±**
íŒŒì¼ ê²½ë¡œ: `g:\ë‚´ ë“œë¼ì´ë¸Œ\08.Programming\spread_insight\.gitignore`
```
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
venv/
env/
ENV/

# í™˜ê²½ ë³€ìˆ˜
.env

# ë°ì´í„°
data/
*.db
*.json

# ë¡œê·¸
logs/
*.log

# IDE
.vscode/
.idea/
*.swp
*.swo

# OS
.DS_Store
Thumbs.db

# GitHub Pages ë¹Œë“œ
github_pages/

# ì°¨íŠ¸ ì´ë¯¸ì§€
*.png
*.jpg
```

**1-5. ê°€ìƒí™˜ê²½ ìƒì„± (Python 3.10 ì´ìƒ ê¶Œì¥)**
```bash
python -m venv venv
```

**1-6. ê°€ìƒí™˜ê²½ í™œì„±í™”**
```bash
# Windows PowerShell
.\venv\Scripts\Activate.ps1

# Windows CMD
venv\Scripts\activate.bat
```

í™œì„±í™”ë˜ë©´ í„°ë¯¸ë„ ì•ì— `(venv)` í‘œì‹œë¨

**1-7. requirements.txt ì‘ì„± (Phase 1ìš©)**
íŒŒì¼ ê²½ë¡œ: `g:\ë‚´ ë“œë¼ì´ë¸Œ\08.Programming\spread_insight\requirements.txt`
```
# Phase 1: ë‰´ìŠ¤ ìˆ˜ì§‘
beautifulsoup4==4.12.3
requests==2.31.0
# lxml==5.1.0  # Python 3.13 í˜¸í™˜ì„± ë¬¸ì œë¡œ ì œì™¸ (html.parser ì‚¬ìš©)
python-dotenv==1.0.1

# ë‚˜ì¤‘ì— ì¶”ê°€í•  ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ (ì£¼ì„ ì²˜ë¦¬)
# google-generativeai==0.4.0
# matplotlib==3.8.3
# plotly==5.19.0
# yfinance==0.2.36
# python-telegram-bot==20.8
# jinja2==3.1.3
# apscheduler==3.10.4
# pandas==2.2.0
# numpy==1.26.4
# sqlalchemy==2.0.27
```

**1-8. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜**
```bash
pip install -r requirements.txt
```

> **âš ï¸ ì¤‘ìš”:** Python 3.13 í™˜ê²½ì—ì„œ lxml ì„¤ì¹˜ ì‹œ "Microsoft Visual C++ 14.0 or greater is required" ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê²½ìš° lxml ì—†ì´ BeautifulSoupì˜ ë‚´ì¥ `html.parser`ë¥¼ ì‚¬ìš©í•˜ë©´ ë©ë‹ˆë‹¤. (Step 1.2 ì°¸ì¡°)

**1-9. .env íŒŒì¼ ìƒì„±**
íŒŒì¼ ê²½ë¡œ: `g:\ë‚´ ë“œë¼ì´ë¸Œ\08.Programming\spread_insight\.env`
```
# Gemini API (ë‚˜ì¤‘ì— ì…ë ¥)
GEMINI_API_KEY=

# í…”ë ˆê·¸ë¨ (ë‚˜ì¤‘ì— ì…ë ¥)
TELEGRAM_BOT_TOKEN=
TELEGRAM_CHAT_ID=

# ì¿ íŒ¡ íŒŒíŠ¸ë„ˆìŠ¤ (ë‚˜ì¤‘ì— ì…ë ¥)
COUPANG_ACCESS_KEY=
COUPANG_SECRET_KEY=
COUPANG_PARTNER_ID=
```

#### âœ… ì„±ê³µ ê¸°ì¤€
- [x] `venv` í´ë” ìƒì„±ë¨
- [x] í„°ë¯¸ë„ì— `(venv)` í‘œì‹œ
- [x] `pip list` ì‹¤í–‰ ì‹œ beautifulsoup4, requests, python-dotenv í‘œì‹œ
- [x] ëª¨ë“  í´ë”ì— `__init__.py` ì¡´ì¬

#### âš ï¸ ì˜ˆìƒ ì˜¤ë¥˜ ë° í•´ê²°

**ì˜¤ë¥˜ 1:** `python: command not found`
- **ì›ì¸:** Python ë¯¸ì„¤ì¹˜ ë˜ëŠ” PATH ë¯¸ë“±ë¡
- **í•´ê²°:** Python 3.10+ ì„¤ì¹˜ (https://www.python.org/downloads/)

**ì˜¤ë¥˜ 2:** PowerShell ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ê¶Œí•œ ì˜¤ë¥˜
```
.\venv\Scripts\Activate.ps1 : ì´ ì‹œìŠ¤í…œì—ì„œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ...
```
- **í•´ê²°:** PowerShell ê´€ë¦¬ì ê¶Œí•œìœ¼ë¡œ ì‹¤í–‰ í›„
  ```powershell
  Set-ExecutionPolicy RemoteSigned -Scope CurrentUser
  ```

**ì˜¤ë¥˜ 3:** `pip install` ì‹œ SSL ì¸ì¦ì„œ ì˜¤ë¥˜
- **í•´ê²°:**
  ```bash
  pip install --trusted-host pypi.org --trusted-host files.pythonhosted.org -r requirements.txt
  ```

**ì˜¤ë¥˜ 4:** `lxml` ì„¤ì¹˜ ì‹œ "Microsoft Visual C++ 14.0 or greater is required" âœ… í•´ê²°ë¨
- **ì›ì¸:** Python 3.13ì—ì„œ lxml 5.1.0 ë¹Œë“œ ì‹œ C++ ì»´íŒŒì¼ëŸ¬ í•„ìš”
- **í•´ê²°:** lxml ì—†ì´ ì„¤ì¹˜í•˜ê³  BeautifulSoupì—ì„œ `html.parser` ì‚¬ìš©
  ```python
  # ë³€ê²½ ì „: soup = BeautifulSoup(response.text, 'lxml')
  # ë³€ê²½ í›„: soup = BeautifulSoup(response.text, 'html.parser')
  ```
- **ì°¸ê³ :** html.parserëŠ” Python ë‚´ì¥ íŒŒì„œë¡œ ë³„ë„ ì„¤ì¹˜ ë¶ˆí•„ìš”í•˜ë©°, í•œê¸€ ë‰´ìŠ¤ íŒŒì‹±ì— ì¶©ë¶„í•œ ì„±ëŠ¥ ì œê³µ

---

### âœ… Step 1.2: ë‰´ìŠ¤ ìë™ ì„ ì • ë° ìŠ¤í¬ë˜í•‘
**ëª©í‘œ:** CONTENT_STRATEGY.md ê¸°ì¤€ì— ë”°ë¼ ê°€ì¥ ì¢‹ì€ ë‰´ìŠ¤ 1ê°œ ìë™ ì„ ì •
**ì†Œìš” ì‹œê°„:** 2ì‹œê°„

> **ë³€ê²½ ì‚¬í•­:** ë‹¨ìˆœíˆ ì²« ë²ˆì§¸ ê¸°ì‚¬ë¥¼ ê°€ì ¸ì˜¤ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì—¬ëŸ¬ ê¸°ì‚¬ë¥¼ ë¶„ì„í•˜ì—¬ ì˜í–¥ë ¥, ì‹¤ì²œ ê°€ëŠ¥ì„±, í•™ìŠµ ê°€ì¹˜ê°€ ë†’ì€ ê¸°ì‚¬ë¥¼ ìë™ ì„ ì •í•©ë‹ˆë‹¤.

#### ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [x] `models/news_article.py` ì‘ì„±
- [x] `scrapers/base_scraper.py` ì‘ì„±
- [x] `scrapers/naver_scraper.py` ì‘ì„±
- [x] `analyzers/news_selector.py` ì‘ì„± (ë‰´ìŠ¤ ì„ ì • ë¡œì§)
- [x] `test_scraper.py` ì—…ê·¸ë ˆì´ë“œ (ìë™ ì„ ì • ê¸°ëŠ¥)

#### ğŸ› ï¸ ì‹¤í–‰ ìˆœì„œ

**2-1. ë°ì´í„° ëª¨ë¸ ì‘ì„±**
íŒŒì¼ ê²½ë¡œ: `g:\ë‚´ ë“œë¼ì´ë¸Œ\08.Programming\spread_insight\models\news_article.py`
```python
from dataclasses import dataclass, asdict
from datetime import datetime
from typing import Optional
import json


@dataclass
class NewsArticle:
    """ë‰´ìŠ¤ ê¸°ì‚¬ ë°ì´í„° ëª¨ë¸"""
    url: str
    title: str
    content: str
    published_at: datetime
    source: str
    keywords: Optional[list[str]] = None
    summary: Optional[str] = None
    easy_explanation: Optional[str] = None
    terminology: Optional[dict[str, str]] = None

    def to_dict(self) -> dict:
        """datetimeì„ ISO í¬ë§· ë¬¸ìì—´ë¡œ ë³€í™˜"""
        data = asdict(self)
        data['published_at'] = self.published_at.isoformat()
        return data

    @classmethod
    def from_dict(cls, data: dict) -> 'NewsArticle':
        """ë”•ì…”ë„ˆë¦¬ì—ì„œ ê°ì²´ ë³µì›"""
        data['published_at'] = datetime.fromisoformat(data['published_at'])
        return cls(**data)

    def save_to_json(self, filepath: str):
        """JSON íŒŒì¼ë¡œ ì €ì¥"""
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.to_dict(), f, ensure_ascii=False, indent=2)

    @classmethod
    def load_from_json(cls, filepath: str) -> 'NewsArticle':
        """JSON íŒŒì¼ì—ì„œ ë¡œë“œ"""
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return cls.from_dict(data)

    def __str__(self):
        return f"[{self.source}] {self.title} ({self.published_at.strftime('%Y-%m-%d')})"
```

**2-2. ì¶”ìƒ ìŠ¤í¬ë˜í¼ ì‘ì„±**
íŒŒì¼ ê²½ë¡œ: `g:\ë‚´ ë“œë¼ì´ë¸Œ\08.Programming\spread_insight\scrapers\base_scraper.py`
```python
from abc import ABC, abstractmethod
from models.news_article import NewsArticle


class BaseScraper(ABC):
    """ëª¨ë“  ìŠ¤í¬ë˜í¼ì˜ ì¶”ìƒ í´ë˜ìŠ¤"""

    @abstractmethod
    def scrape_article(self, url: str) -> NewsArticle:
        """ë‹¨ì¼ ê¸°ì‚¬ ìŠ¤í¬ë˜í•‘"""
        pass

    @abstractmethod
    def get_article_list(self, category_url: str, limit: int = 10) -> list[str]:
        """ê¸°ì‚¬ URL ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘"""
        pass
```

**2-3. ë„¤ì´ë²„ ìŠ¤í¬ë˜í¼ ì‘ì„±**
íŒŒì¼ ê²½ë¡œ: `g:\ë‚´ ë“œë¼ì´ë¸Œ\08.Programming\spread_insight\scrapers\naver_scraper.py`
```python
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from .base_scraper import BaseScraper
from models.news_article import NewsArticle


class NaverScraper(BaseScraper):
    """ë„¤ì´ë²„ ë‰´ìŠ¤ ìŠ¤í¬ë˜í¼"""

    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }

    def scrape_article(self, url: str) -> NewsArticle:
        """ë„¤ì´ë²„ ë‰´ìŠ¤ ê¸°ì‚¬ ìƒì„¸ ì •ë³´ ì¶”ì¶œ"""
        try:
            # í˜ì´ì§€ ìš”ì²­
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            response.encoding = 'utf-8'

            # HTML íŒŒì‹± (lxml ëŒ€ì‹  html.parser ì‚¬ìš©)
            soup = BeautifulSoup(response.text, 'html.parser')

            # ì œëª© ì¶”ì¶œ
            title_elem = soup.select_one('#title_area span, #articleTitle, h2.media_end_head_headline')
            if not title_elem:
                raise ValueError("ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            title = title_elem.get_text(strip=True)

            # ë³¸ë¬¸ ì¶”ì¶œ
            content_elem = soup.select('#dic_area, #articeBody, #newsct_article')
            if not content_elem:
                raise ValueError("ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            # ë³¸ë¬¸ ë‚´ ëª¨ë“  í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê´‘ê³  ì œê±°)
            paragraphs = []
            for elem in content_elem:
                # script, style íƒœê·¸ ì œê±°
                for tag in elem.find_all(['script', 'style', 'iframe']):
                    tag.decompose()

                # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text = elem.get_text(separator='\n', strip=True)
                if text:
                    paragraphs.append(text)

            content = '\n\n'.join(paragraphs)

            if not content or len(content) < 50:
                raise ValueError("ë³¸ë¬¸ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤.")

            # ë‚ ì§œ ì¶”ì¶œ
            date_elem = soup.select_one('.media_end_head_info_datestamp_time, .author_info em, span.t11')
            if date_elem:
                date_str = date_elem.get('data-date-time') or date_elem.get_text(strip=True)

                # ì—¬ëŸ¬ ë‚ ì§œ í¬ë§· ì‹œë„
                for fmt in ['%Y-%m-%d %H:%M:%S', '%Y.%m.%d. %H:%M', '%Y.%m.%d %H:%M']:
                    try:
                        published_at = datetime.strptime(date_str.replace('ì˜¤ì „', '').replace('ì˜¤í›„', '').strip(), fmt)
                        break
                    except ValueError:
                        continue
                else:
                    # ISO í¬ë§· ì‹œë„
                    try:
                        published_at = datetime.fromisoformat(date_str)
                    except:
                        published_at = datetime.now()  # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í˜„ì¬ ì‹œê°„
            else:
                published_at = datetime.now()

            return NewsArticle(
                url=url,
                title=title,
                content=content,
                published_at=published_at,
                source='ë„¤ì´ë²„'
            )

        except requests.RequestException as e:
            raise Exception(f"ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {e}")
        except Exception as e:
            raise Exception(f"íŒŒì‹± ì˜¤ë¥˜: {e}")

    def get_article_list(self, category_url: str = 'https://news.naver.com/section/101', limit: int = 10) -> list[str]:
        """ë„¤ì´ë²„ ê²½ì œ ì„¹ì…˜ì—ì„œ ê¸°ì‚¬ URL ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            response = requests.get(category_url, headers=self.headers, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'html.parser')

            # ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ (ë„¤ì´ë²„ ë‰´ìŠ¤ êµ¬ì¡°ì— ë”°ë¼ ì„ íƒì ì¡°ì • í•„ìš”)
            article_links = []

            # ë°©ë²• 1: sa_text í´ë˜ìŠ¤ (ë°ìŠ¤í¬í†±)
            for link in soup.select('.sa_text_title'):
                href = link.get('href')
                if href and href.startswith('http') and 'news.naver.com' in href:
                    article_links.append(href)

            # ë°©ë²• 2: ë¦¬ìŠ¤íŠ¸ í˜•ì‹
            for link in soup.select('a.news_tit'):
                href = link.get('href')
                if href and href.startswith('http'):
                    article_links.append(href)

            # ì¤‘ë³µ ì œê±°
            article_links = list(dict.fromkeys(article_links))

            return article_links[:limit]

        except Exception as e:
            raise Exception(f"ëª©ë¡ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
```

**2-4. í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±**
íŒŒì¼ ê²½ë¡œ: `g:\ë‚´ ë“œë¼ì´ë¸Œ\08.Programming\spread_insight\test_scraper.py`
```python
from scrapers.naver_scraper import NaverScraper


if __name__ == '__main__':
    print("=" * 50)
    print("ë„¤ì´ë²„ ë‰´ìŠ¤ ìŠ¤í¬ë˜í¼ í…ŒìŠ¤íŠ¸")
    print("=" * 50)

    scraper = NaverScraper()

    # í…ŒìŠ¤íŠ¸ìš© ë„¤ì´ë²„ ë‰´ìŠ¤ URL (ìµœì‹  ê²½ì œ ë‰´ìŠ¤ë¡œ êµì²´í•˜ì„¸ìš”)
    # ì˜ˆì‹œ: https://n.news.naver.com/article/009/0005393847
    test_url = ""  # ìë™ ìˆ˜ì§‘ ëª¨ë“œ

    if not test_url:
        print("[!] URLì„ ì…ë ¥í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("\nìë™ìœ¼ë¡œ ê²½ì œ ì„¹ì…˜ì—ì„œ ì²« ë²ˆì§¸ ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸°...")
        try:
            urls = scraper.get_article_list(limit=1)
            if urls:
                test_url = urls[0]
                print(f"[OK] URL: {test_url}")
            else:
                print("[ERROR] ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                exit(1)
        except Exception as e:
            print(f"[ERROR] URL ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            exit(1)

    print(f"\n[ê²€ìƒ‰ì¤‘] ìŠ¤í¬ë˜í•‘ ì¤‘: {test_url}\n")

    try:
        article = scraper.scrape_article(test_url)

        print("[OK] ìŠ¤í¬ë˜í•‘ ì„±ê³µ!")
        print(f"\nì œëª©: {article.title}")
        print(f"ì¶œì²˜: {article.source}")
        print(f"ë‚ ì§œ: {article.published_at.strftime('%Yë…„ %mì›” %dì¼ %H:%M')}")
        print(f"ë³¸ë¬¸ ê¸¸ì´: {len(article.content)}ì")
        print(f"\në³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°:")
        print("-" * 50)
        print(article.content[:300])
        print("...")
        print("-" * 50)

        # JSON ì €ì¥ í…ŒìŠ¤íŠ¸
        article.save_to_json('./data/raw/test_article.json')
        print(f"\n[ì €ì¥ì™„ë£Œ] JSON ì €ì¥ ì™„ë£Œ: ./data/raw/test_article.json")

    except Exception as e:
        print(f"[ERROR] ì˜¤ë¥˜ ë°œìƒ: {e}")
        exit(1)
```

> **âš ï¸ Windows ì½˜ì†” ì¸ì½”ë”© ì´ìŠˆ:** ì´ëª¨ì§€ ë¬¸ì(âœ…, âŒ, ğŸ” ë“±)ëŠ” Windows ì½˜ì†”(cp949 ì¸ì½”ë”©)ì—ì„œ `UnicodeEncodeError`ë¥¼ ë°œìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìœ„ ì½”ë“œëŠ” ì´ëª¨ì§€ ëŒ€ì‹  `[OK]`, `[ERROR]`, `[!]` ë“±ì˜ í…ìŠ¤íŠ¸ ë§ˆì»¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.

**2-5. ì‹¤í–‰**
```bash
python test_scraper.py
```

#### âœ… ì„±ê³µ ê¸°ì¤€
- [x] ì œëª©, ë³¸ë¬¸, ë‚ ì§œê°€ ì •ìƒ ì¶œë ¥ë¨
- [x] ë³¸ë¬¸ ê¸¸ì´ê°€ ìµœì†Œ 100ì ì´ìƒ
- [x] `data/raw/test_article.json` íŒŒì¼ ìƒì„±
- [x] JSON íŒŒì¼ì„ í…ìŠ¤íŠ¸ í¸ì§‘ê¸°ë¡œ ì—´ì—ˆì„ ë•Œ í•œê¸€ ì •ìƒ í‘œì‹œ

#### âœ… ì™„ë£Œ ê²°ê³¼
- **ì‹¤í–‰ ì¼ì:** 2025-10-10
- **ìˆ˜ì§‘ëœ ê¸°ì‚¬:** "ì‚°ì—…ì°¨ê´€, ì² ê°• ê´€ì„¸ ëŒ€ì‘ ë“± ìˆ˜ì¶œ ì§€ì›ì²´ê³„ í™•ì¸â€¦í•´ìƒë¬¼ë¥˜ ì ê²€"
- **ê¸°ì‚¬ URL:** https://n.news.naver.com/mnews/article/003/0013525645
- **ë³¸ë¬¸ ê¸¸ì´:** 2728ì
- **ì €ì¥ íŒŒì¼:** [./data/raw/test_article.json](./data/raw/test_article.json)

#### âš ï¸ ì˜ˆìƒ ì˜¤ë¥˜ ë° í•´ê²°

**ì˜¤ë¥˜ 1:** `ModuleNotFoundError: No module named 'models'`
- **ì›ì¸:** Python ê²½ë¡œ ë¬¸ì œ
- **í•´ê²°:** `__init__.py` íŒŒì¼ì´ ëª¨ë“  í´ë”ì— ìˆëŠ”ì§€ í™•ì¸

**ì˜¤ë¥˜ 2:** `ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤`
- **ì›ì¸:** ë„¤ì´ë²„ HTML êµ¬ì¡° ë³€ê²½
- **í•´ê²°:**
  1. ë¸Œë¼ìš°ì €ë¡œ í•´ë‹¹ URL ì ‘ì†
  2. F12 ëˆŒëŸ¬ ê°œë°œì ë„êµ¬
  3. Elements íƒ­ì—ì„œ ì œëª© ìš”ì†Œ ì°¾ê¸°
  4. ì„ íƒì ìˆ˜ì • (ì˜ˆ: `#title_area span` â†’ ì‹¤ì œ ì„ íƒì)

**ì˜¤ë¥˜ 3:** `ë³¸ë¬¸ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤`
- **ì›ì¸:** ë³¸ë¬¸ ì„ íƒì ì˜ëª»ë¨
- **í•´ê²°:** ìœ„ì™€ ë™ì¼í•˜ê²Œ ê°œë°œì ë„êµ¬ë¡œ ë³¸ë¬¸ ì˜ì—­ í™•ì¸ í›„ ì„ íƒì ìˆ˜ì •

**ì˜¤ë¥˜ 4:** ë‚ ì§œ íŒŒì‹± ì˜¤ë¥˜
- **ì›ì¸:** ë‚ ì§œ í¬ë§· ë‹¤ì–‘
- **í•´ê²°:** ì´ë¯¸ ì½”ë“œì— ì—¬ëŸ¬ í¬ë§· ì²˜ë¦¬ í¬í•¨, ì‹¤íŒ¨ ì‹œ í˜„ì¬ ì‹œê°„ ì‚¬ìš©

**ì˜¤ë¥˜ 5:** `UnicodeEncodeError: 'cp949' codec can't encode character` âœ… í•´ê²°ë¨
- **ì›ì¸:** Windows ì½˜ì†”ì´ cp949 ì¸ì½”ë”©ì„ ì‚¬ìš©í•˜ì—¬ ì´ëª¨ì§€ ë¬¸ì(âœ…, âŒ, ğŸ” ë“±) ì¶œë ¥ ë¶ˆê°€
- **í•´ê²°:** ì´ëª¨ì§€ ëŒ€ì‹  ASCII í…ìŠ¤íŠ¸ ë§ˆì»¤ ì‚¬ìš©
  - âŒ â†’ `[ERROR]`
  - âœ… â†’ `[OK]`
  - ğŸ” â†’ `[ê²€ìƒ‰ì¤‘]`
  - ğŸ’¾ â†’ `[ì €ì¥ì™„ë£Œ]`
  - âš ï¸ â†’ `[!]`
- **ì°¸ê³ :** UTF-8 ì½˜ì†” í™˜ê²½(Linux, macOS)ì—ì„œëŠ” ì´ëª¨ì§€ ì‚¬ìš© ê°€ëŠ¥

---

## ğŸ‰ Phase 1 ì™„ë£Œ!

**ì™„ë£Œ ì¼ì:** 2025-10-10

### ì£¼ìš” ì„±ê³¼
- âœ… ë‰´ìŠ¤ ìë™ ì„ ì • ì‹œìŠ¤í…œ êµ¬ì¶•
- âœ… CONTENT_STRATEGY.mdì˜ "1ï¸âƒ£ ì–´ë–¤ ë‰´ìŠ¤ë¥¼ ê³¨ë¼ë‚¼ ê²ƒì¸ê°€?" ì™„ì „ êµ¬í˜„
- âœ… ì ìˆ˜ ê¸°ë°˜ íˆ¬ëª…í•œ ì„ ì • í”„ë¡œì„¸ìŠ¤
- âœ… 5ê°œ í›„ë³´ ì¤‘ ìµœê³  í’ˆì§ˆ 1ê°œ ìë™ ì„ ì •

### ê²°ì • ì‚¬í•­: Step 1.3~1.5 ê±´ë„ˆëœ€
**ì´ìœ :**
- Step 1.3 (ì—¬ëŸ¬ ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘): Step 1.2ì— ì´ë¯¸ í¬í•¨ë¨
- Step 1.4 (ë°ì´í„° ì €ì¥): Step 1.2ì—ì„œ JSON ì €ì¥ ì™„ë£Œ
- Step 1.5 (ë‹¤ìŒ ìŠ¤í¬ë˜í¼): ë„¤ì´ë²„ë§Œìœ¼ë¡œ ì¶©ë¶„, ì¶”í›„ ë°ì´í„° í’€ í™•ì¥ ì‹œ ê³ ë ¤

**ì›ì¹™:**
> "ì „ì²´ì ì¸ í‹€ì„ ëª¨ë‘ ì„¸ìš´ ë’¤ ë°ì´í„° í’€ì„ ëŠ˜ë¦¬ëŠ” ê±´ ì¶”ê°€ë¡œ ê²°ì •"

---

### ~~Step 1.3: ì—¬ëŸ¬ ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘~~ (ê±´ë„ˆëœ€)
**ëª©í‘œ:** ë„¤ì´ë²„ ê²½ì œ ì„¹ì…˜ì—ì„œ ìµœì‹  10ê°œ ê¸°ì‚¬ URL ì¶”ì¶œ
**ì†Œìš” ì‹œê°„:** 1ì‹œê°„
**ìƒíƒœ:** âŒ Step 1.2ì— ì´ë¯¸ í¬í•¨ë˜ì–´ ë¶ˆí•„ìš”

#### ğŸ› ï¸ ì‹¤í–‰ ìˆœì„œ

**3-1. ëª©ë¡ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±**
íŒŒì¼ ê²½ë¡œ: `g:\ë‚´ ë“œë¼ì´ë¸Œ\08.Programming\spread_insight\test_article_list.py`
```python
from scrapers.naver_scraper import NaverScraper


if __name__ == '__main__':
    print("=" * 50)
    print("ë„¤ì´ë²„ ë‰´ìŠ¤ ëª©ë¡ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸")
    print("=" * 50)

    scraper = NaverScraper()

    # ë„¤ì´ë²„ ê²½ì œ ì„¹ì…˜ URL
    economy_url = 'https://news.naver.com/section/101'

    print(f"\nğŸ” ìˆ˜ì§‘ ì¤‘: {economy_url}\n")

    try:
        article_urls = scraper.get_article_list(economy_url, limit=10)

        print(f"âœ… ì´ {len(article_urls)}ê°œ ê¸°ì‚¬ URL ìˆ˜ì§‘ ì™„ë£Œ\n")

        for i, url in enumerate(article_urls, 1):
            print(f"{i:2d}. {url}")

        # ì²« ë²ˆì§¸ ê¸°ì‚¬ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        if article_urls:
            print(f"\n{'=' * 50}")
            print("ì²« ë²ˆì§¸ ê¸°ì‚¬ ìƒì„¸ ì •ë³´:")
            print('=' * 50)

            article = scraper.scrape_article(article_urls[0])
            print(f"\nì œëª©: {article.title}")
            print(f"ë‚ ì§œ: {article.published_at}")
            print(f"ë³¸ë¬¸: {article.content[:200]}...")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        exit(1)
```

**3-2. ì‹¤í–‰**
```bash
python test_article_list.py
```

#### âœ… ì„±ê³µ ê¸°ì¤€
- [ ] 10ê°œì˜ ìœ íš¨í•œ ë„¤ì´ë²„ ë‰´ìŠ¤ URL ì¶œë ¥
- [ ] ì²« ë²ˆì§¸ ê¸°ì‚¬ì˜ ì œëª©, ë‚ ì§œ, ë³¸ë¬¸ ì¼ë¶€ ì¶œë ¥
- [ ] URLì´ ëª¨ë‘ `https://n.news.naver.com/article/` í˜•ì‹

---

### âœ… Step 1.4: ë°ì´í„° ì €ì¥ (JSON)
**ëª©í‘œ:** ì—¬ëŸ¬ ê¸°ì‚¬ë¥¼ í•˜ë‚˜ì˜ JSON íŒŒì¼ë¡œ ì €ì¥
**ì†Œìš” ì‹œê°„:** 45ë¶„

#### ğŸ› ï¸ ì‹¤í–‰ ìˆœì„œ

**4-1. íŒŒì¼ ë§¤ë‹ˆì € ì‘ì„±**
íŒŒì¼ ê²½ë¡œ: `g:\ë‚´ ë“œë¼ì´ë¸Œ\08.Programming\spread_insight\utils\file_manager.py`
```python
import json
import os
from typing import List
from models.news_article import NewsArticle
from datetime import datetime


class FileManager:
    """íŒŒì¼ ì €ì¥/ë¡œë“œ ê´€ë¦¬"""

    def __init__(self, data_dir: str = './data'):
        self.data_dir = data_dir
        self.raw_dir = os.path.join(data_dir, 'raw')
        self.processed_dir = os.path.join(data_dir, 'processed')

        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(self.raw_dir, exist_ok=True)
        os.makedirs(self.processed_dir, exist_ok=True)

    def save_articles(self, articles: List[NewsArticle], filename: str = None) -> str:
        """ì—¬ëŸ¬ ê¸°ì‚¬ë¥¼ í•˜ë‚˜ì˜ JSON íŒŒì¼ë¡œ ì €ì¥"""
        if filename is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'news_{timestamp}.json'

        filepath = os.path.join(self.raw_dir, filename)
        data = [article.to_dict() for article in articles]

        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"âœ… {len(articles)}ê°œ ê¸°ì‚¬ë¥¼ {filepath}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
        return filepath

    def load_articles(self, filename: str) -> List[NewsArticle]:
        """JSON íŒŒì¼ì—ì„œ ê¸°ì‚¬ ë¡œë“œ"""
        filepath = os.path.join(self.raw_dir, filename)

        if not os.path.exists(filepath):
            raise FileNotFoundError(f"íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {filepath}")

        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)

        articles = [NewsArticle.from_dict(item) for item in data]
        print(f"âœ… {len(articles)}ê°œ ê¸°ì‚¬ë¥¼ {filepath}ì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        return articles

    def list_saved_files(self) -> list[str]:
        """ì €ì¥ëœ JSON íŒŒì¼ ëª©ë¡"""
        files = [f for f in os.listdir(self.raw_dir) if f.endswith('.json')]
        return sorted(files, reverse=True)
```

**4-2. ì €ì¥ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸**
íŒŒì¼ ê²½ë¡œ: `g:\ë‚´ ë“œë¼ì´ë¸Œ\08.Programming\spread_insight\test_save_articles.py`
```python
from scrapers.naver_scraper import NaverScraper
from utils.file_manager import FileManager
import time


if __name__ == '__main__':
    print("=" * 50)
    print("ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ì €ì¥ í…ŒìŠ¤íŠ¸")
    print("=" * 50)

    scraper = NaverScraper()
    file_manager = FileManager()

    # ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘
    print("\n1ë‹¨ê³„: ê¸°ì‚¬ URL ìˆ˜ì§‘ ì¤‘...")
    article_urls = scraper.get_article_list(limit=5)  # í…ŒìŠ¤íŠ¸ìš© 5ê°œ
    print(f"âœ… {len(article_urls)}ê°œ URL ìˆ˜ì§‘ ì™„ë£Œ")

    # ê° ê¸°ì‚¬ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
    print("\n2ë‹¨ê³„: ê¸°ì‚¬ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
    articles = []

    for i, url in enumerate(article_urls, 1):
        try:
            print(f"[{i}/{len(article_urls)}] {url[:60]}...")
            article = scraper.scrape_article(url)
            articles.append(article)
            print(f"  âœ… {article.title[:40]}")
            time.sleep(1)  # ì„œë²„ ë¶€ë‹´ ë°©ì§€
        except Exception as e:
            print(f"  âŒ ì—ëŸ¬: {e}")
            continue

    # JSON íŒŒì¼ë¡œ ì €ì¥
    print(f"\n3ë‹¨ê³„: JSON ì €ì¥ ì¤‘...")
    filepath = file_manager.save_articles(articles)

    # ì €ì¥ëœ íŒŒì¼ ë‹¤ì‹œ ì½ê¸° (ê²€ì¦)
    print(f"\n4ë‹¨ê³„: ê²€ì¦ ì¤‘...")
    loaded_articles = file_manager.load_articles(os.path.basename(filepath))

    print(f"\nâœ… ëª¨ë“  ë‹¨ê³„ ì™„ë£Œ!")
    print(f"ì €ì¥ëœ ê¸°ì‚¬: {len(loaded_articles)}ê°œ")
    print(f"\nì²« ë²ˆì§¸ ê¸°ì‚¬:")
    print(f"  ì œëª©: {loaded_articles[0].title}")
    print(f"  ë‚ ì§œ: {loaded_articles[0].published_at}")
    print(f"  ë³¸ë¬¸ ê¸¸ì´: {len(loaded_articles[0].content)}ì")
```

**4-3. ì‹¤í–‰**
```bash
python test_save_articles.py
```

#### âœ… ì„±ê³µ ê¸°ì¤€
- [ ] 5ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì„±ê³µ
- [ ] `data/raw/news_YYYYMMDD_HHMMSS.json` íŒŒì¼ ìƒì„±
- [ ] JSON íŒŒì¼ ì—´ì—ˆì„ ë•Œ ë°°ì—´ í˜•íƒœë¡œ 5ê°œ ê¸°ì‚¬ ì¡´ì¬
- [ ] ë¡œë“œ ì‹œ ë°ì´í„° ì¼ì¹˜

---

### âœ… Step 1.5: ë‹¤ìŒ(Daum) ìŠ¤í¬ë˜í¼ ì¶”ê°€
**ëª©í‘œ:** ë‹¤ìŒ ë‰´ìŠ¤ë„ ë™ì¼í•˜ê²Œ ìˆ˜ì§‘
**ì†Œìš” ì‹œê°„:** 1.5ì‹œê°„

#### ğŸ› ï¸ ì‹¤í–‰ ìˆœì„œ

**5-1. ë‹¤ìŒ ìŠ¤í¬ë˜í¼ ì‘ì„±**
íŒŒì¼ ê²½ë¡œ: `g:\ë‚´ ë“œë¼ì´ë¸Œ\08.Programming\spread_insight\scrapers\daum_scraper.py`
```python
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from .base_scraper import BaseScraper
from models.news_article import NewsArticle
import re


class DaumScraper(BaseScraper):
    """ë‹¤ìŒ ë‰´ìŠ¤ ìŠ¤í¬ë˜í¼"""

    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

    def scrape_article(self, url: str) -> NewsArticle:
        """ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ ìƒì„¸ ì •ë³´ ì¶”ì¶œ"""
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'html.parser')

            # ì œëª©
            title_elem = soup.select_one('h3.tit_view, h2.screen_out')
            if not title_elem:
                raise ValueError("ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            title = title_elem.get_text(strip=True)

            # ë³¸ë¬¸
            content_elems = soup.select('#harmonyContainer p, #mArticle p')
            if not content_elems:
                # ëŒ€ì²´ ì„ íƒì
                content_elems = soup.select('div[dmcf-ptype="general"] p')

            paragraphs = []
            for p in content_elems:
                text = p.get_text(strip=True)
                if text and len(text) > 10:  # ì§§ì€ í…ìŠ¤íŠ¸ ì œì™¸
                    paragraphs.append(text)

            content = '\n\n'.join(paragraphs)

            if not content or len(content) < 50:
                raise ValueError("ë³¸ë¬¸ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤.")

            # ë‚ ì§œ
            date_elem = soup.select_one('.num_date, .txt_info')
            if date_elem:
                date_text = date_elem.get_text(strip=True)
                # ì˜ˆ: "2025.01.10. ì˜¤í›„ 3:24"
                date_text = date_text.replace('ì…ë ¥', '').replace('ìˆ˜ì •', '').strip()

                # ì •ê·œì‹ìœ¼ë¡œ ë‚ ì§œ ì¶”ì¶œ
                match = re.search(r'(\d{4})\.(\d{1,2})\.(\d{1,2})\.\s*(ì˜¤ì „|ì˜¤í›„)?\s*(\d{1,2}):(\d{2})', date_text)
                if match:
                    year, month, day, ampm, hour, minute = match.groups()
                    hour = int(hour)
                    if ampm == 'ì˜¤í›„' and hour != 12:
                        hour += 12
                    elif ampm == 'ì˜¤ì „' and hour == 12:
                        hour = 0

                    published_at = datetime(int(year), int(month), int(day), hour, int(minute))
                else:
                    published_at = datetime.now()
            else:
                published_at = datetime.now()

            return NewsArticle(
                url=url,
                title=title,
                content=content,
                published_at=published_at,
                source='ë‹¤ìŒ'
            )

        except Exception as e:
            raise Exception(f"íŒŒì‹± ì˜¤ë¥˜: {e}")

    def get_article_list(self, category_url: str = 'https://news.daum.net/economy', limit: int = 10) -> list[str]:
        """ë‹¤ìŒ ê²½ì œ ì„¹ì…˜ì—ì„œ ê¸°ì‚¬ URL ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            response = requests.get(category_url, headers=self.headers, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'html.parser')

            article_links = []

            # ë‹¤ìŒ ë‰´ìŠ¤ ë§í¬ ì„ íƒì
            for link in soup.select('a.link_txt'):
                href = link.get('href')
                if href and href.startswith('http') and 'v.daum.net' in href:
                    article_links.append(href)

            # ì¤‘ë³µ ì œê±°
            article_links = list(dict.fromkeys(article_links))

            return article_links[:limit]

        except Exception as e:
            raise Exception(f"ëª©ë¡ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
```

**5-2. ë‹¤ì¤‘ ì†ŒìŠ¤ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸**
íŒŒì¼ ê²½ë¡œ: `g:\ë‚´ ë“œë¼ì´ë¸Œ\08.Programming\spread_insight\test_multi_scraper.py`
```python
from scrapers.naver_scraper import NaverScraper
from scrapers.daum_scraper import DaumScraper
from utils.file_manager import FileManager
import time


if __name__ == '__main__':
    print("=" * 60)
    print("ë‹¤ì¤‘ ì†ŒìŠ¤ ë‰´ìŠ¤ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ (ë„¤ì´ë²„ + ë‹¤ìŒ)")
    print("=" * 60)

    file_manager = FileManager()
    all_articles = []

    # ===== ë„¤ì´ë²„ ìˆ˜ì§‘ =====
    print("\nğŸ”µ ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘")
    print("-" * 60)
    naver_scraper = NaverScraper()

    try:
        naver_urls = naver_scraper.get_article_list(limit=3)
        print(f"âœ… {len(naver_urls)}ê°œ URL ìˆ˜ì§‘")

        for i, url in enumerate(naver_urls, 1):
            try:
                print(f"[{i}/{len(naver_urls)}] ìˆ˜ì§‘ ì¤‘...")
                article = naver_scraper.scrape_article(url)
                all_articles.append(article)
                print(f"  âœ… {article.title[:50]}")
                time.sleep(1)
            except Exception as e:
                print(f"  âŒ {e}")
    except Exception as e:
        print(f"âŒ ë„¤ì´ë²„ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

    # ===== ë‹¤ìŒ ìˆ˜ì§‘ =====
    print("\nğŸŸ¢ ë‹¤ìŒ ë‰´ìŠ¤ ìˆ˜ì§‘")
    print("-" * 60)
    daum_scraper = DaumScraper()

    try:
        daum_urls = daum_scraper.get_article_list(limit=3)
        print(f"âœ… {len(daum_urls)}ê°œ URL ìˆ˜ì§‘")

        for i, url in enumerate(daum_urls, 1):
            try:
                print(f"[{i}/{len(daum_urls)}] ìˆ˜ì§‘ ì¤‘...")
                article = daum_scraper.scrape_article(url)
                all_articles.append(article)
                print(f"  âœ… {article.title[:50]}")
                time.sleep(1)
            except Exception as e:
                print(f"  âŒ {e}")
    except Exception as e:
        print(f"âŒ ë‹¤ìŒ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

    # ===== í†µí•© ì €ì¥ =====
    print(f"\n{'=' * 60}")
    print(f"ì´ {len(all_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
    print('=' * 60)

    if all_articles:
        filepath = file_manager.save_articles(all_articles, filename='combined_news.json')
        print(f"\nì €ì¥ ìœ„ì¹˜: {filepath}")

        # ì†ŒìŠ¤ë³„ í†µê³„
        naver_count = sum(1 for a in all_articles if a.source == 'ë„¤ì´ë²„')
        daum_count = sum(1 for a in all_articles if a.source == 'ë‹¤ìŒ')
        print(f"\ní†µê³„:")
        print(f"  ë„¤ì´ë²„: {naver_count}ê°œ")
        print(f"  ë‹¤ìŒ: {daum_count}ê°œ")
    else:
        print("âŒ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
```

**5-3. ì‹¤í–‰**
```bash
python test_multi_scraper.py
```

#### âœ… ì„±ê³µ ê¸°ì¤€
- [ ] ë„¤ì´ë²„ 3ê°œ + ë‹¤ìŒ 3ê°œ = ì´ 6ê°œ ê¸°ì‚¬ ìˆ˜ì§‘
- [ ] `data/raw/combined_news.json` íŒŒì¼ ìƒì„±
- [ ] JSONì—ì„œ source í•„ë“œê°€ 'ë„¤ì´ë²„', 'ë‹¤ìŒ'ìœ¼ë¡œ êµ¬ë¶„

---

## ğŸ¤– Phase 2: AI ë¶„ì„ (Week 2)

### âœ… Step 2.1: Gemini API ì—°ë™ ë° ìš”ì•½
**ëª©í‘œ:** 1ê°œ ê¸°ì‚¬ë¥¼ 3ì¤„ë¡œ ìš”ì•½
**ì†Œìš” ì‹œê°„:** 1ì‹œê°„

#### ğŸ“ ì¤€ë¹„ ì‚¬í•­
- [ ] Gemini API í‚¤ ë°œê¸‰ (https://makersuite.google.com/app/apikey)
- [ ] `.env`ì— API í‚¤ ë“±ë¡

#### ğŸ› ï¸ ì‹¤í–‰ ìˆœì„œ

**1. requirements.txt ì—…ë°ì´íŠ¸**
```
# ê¸°ì¡´ ë¼ì´ë¸ŒëŸ¬ë¦¬
beautifulsoup4==4.12.3
requests==2.31.0
lxml==5.1.0
python-dotenv==1.0.1

# Gemini API ì¶”ê°€
google-generativeai==0.4.0
```

ì„¤ì¹˜:
```bash
pip install google-generativeai==0.4.0
```

**2. .env íŒŒì¼ì— API í‚¤ ì¶”ê°€**
```
GEMINI_API_KEY=ì—¬ê¸°ì—_ë°œê¸‰ë°›ì€_API_í‚¤_ì…ë ¥
```

**3. Config íŒŒì¼ ì‘ì„±**
íŒŒì¼ ê²½ë¡œ: `g:\ë‚´ ë“œë¼ì´ë¸Œ\08.Programming\spread_insight\utils\config.py`
```python
import os
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()


class Config:
    """í”„ë¡œì íŠ¸ ì„¤ì •"""

    # Gemini API
    GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
    GEMINI_MODEL = 'gemini-2.0-flash-exp'  # ë˜ëŠ” 'gemini-2.5-flash'

    # í…”ë ˆê·¸ë¨
    TELEGRAM_BOT_TOKEN = os.getenv('TELEGRAM_BOT_TOKEN')
    TELEGRAM_CHAT_ID = os.getenv('TELEGRAM_CHAT_ID')

    # ì¿ íŒ¡ íŒŒíŠ¸ë„ˆìŠ¤
    COUPANG_ACCESS_KEY = os.getenv('COUPANG_ACCESS_KEY')
    COUPANG_PARTNER_ID = os.getenv('COUPANG_PARTNER_ID')

    # ìŠ¤í¬ë˜í•‘ ì„¤ì •
    MAX_ARTICLES_PER_SITE = 10
    SCRAPING_DELAY = 1  # ì´ˆ

    # ë°ì´í„° ê²½ë¡œ
    DATA_DIR = './data'
    RAW_DIR = './data/raw'
    PROCESSED_DIR = './data/processed'
    CHARTS_DIR = './data/charts'
    HTML_DIR = './data/html'

    @classmethod
    def validate(cls):
        """í•„ìˆ˜ ì„¤ì • ê²€ì¦"""
        if not cls.GEMINI_API_KEY:
            raise ValueError("GEMINI_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
```

**4. Gemini ë¶„ì„ê¸° ì‘ì„±**
íŒŒì¼ ê²½ë¡œ: `g:\ë‚´ ë“œë¼ì´ë¸Œ\08.Programming\spread_insight\analyzers\gemini_analyzer.py`
```python
import google.generativeai as genai
from utils.config import Config
from models.news_article import NewsArticle


class GeminiAnalyzer:
    """Gemini APIë¥¼ ì‚¬ìš©í•œ ë‰´ìŠ¤ ë¶„ì„"""

    def __init__(self):
        # API í‚¤ ê²€ì¦
        Config.validate()

        # Gemini ì„¤ì •
        genai.configure(api_key=Config.GEMINI_API_KEY)
        self.model = genai.GenerativeModel(Config.GEMINI_MODEL)

    def summarize(self, article: NewsArticle, num_sentences: int = 3) -> str:
        """ê¸°ì‚¬ë¥¼ ì§€ì •ëœ ë¬¸ì¥ ìˆ˜ë¡œ ìš”ì•½"""
        prompt = f"""
ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì •í™•íˆ {num_sentences}ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.
í•µì‹¬ ë‚´ìš©ë§Œ ê°„ê²°í•˜ê²Œ ë‹´ì•„ì£¼ì„¸ìš”.

ì œëª©: {article.title}

ë³¸ë¬¸:
{article.content}

ìš”ì•½ ({num_sentences}ë¬¸ì¥):
        """.strip()

        try:
            response = self.model.generate_content(prompt)
            summary = response.text.strip()
            return summary

        except Exception as e:
            raise Exception(f"ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")

    def test_connection(self) -> bool:
        """API ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            response = self.model.generate_content("ì•ˆë…•í•˜ì„¸ìš”. í…ŒìŠ¤íŠ¸ ë©”ì‹œì§€ì…ë‹ˆë‹¤. 'ì„±ê³µ'ì´ë¼ê³  ë‹µí•´ì£¼ì„¸ìš”.")
            return 'ì„±ê³µ' in response.text
        except Exception as e:
            print(f"API ì—°ê²° ì‹¤íŒ¨: {e}")
            return False
```

**5. í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸**
íŒŒì¼ ê²½ë¡œ: `g:\ë‚´ ë“œë¼ì´ë¸Œ\08.Programming\spread_insight\test_gemini.py`
```python
from utils.file_manager import FileManager
from analyzers.gemini_analyzer import GeminiAnalyzer
import os


if __name__ == '__main__':
    print("=" * 60)
    print("Gemini API ì—°ë™ ë° ìš”ì•½ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    # API ì—°ê²° í…ŒìŠ¤íŠ¸
    print("\n1ë‹¨ê³„: API ì—°ê²° í…ŒìŠ¤íŠ¸...")
    analyzer = GeminiAnalyzer()

    if analyzer.test_connection():
        print("âœ… API ì—°ê²° ì„±ê³µ")
    else:
        print("âŒ API ì—°ê²° ì‹¤íŒ¨ - .env íŒŒì¼ì˜ GEMINI_API_KEYë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        exit(1)

    # ì €ì¥ëœ ê¸°ì‚¬ ë¡œë“œ
    print("\n2ë‹¨ê³„: ì €ì¥ëœ ê¸°ì‚¬ ë¡œë“œ...")
    file_manager = FileManager()

    # ê°€ì¥ ìµœê·¼ ì €ì¥ëœ íŒŒì¼ ì°¾ê¸°
    saved_files = file_manager.list_saved_files()
    if not saved_files:
        print("âŒ ì €ì¥ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € test_save_articles.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        exit(1)

    latest_file = saved_files[0]
    print(f"âœ… íŒŒì¼: {latest_file}")

    articles = file_manager.load_articles(latest_file)
    article = articles[0]

    # ê¸°ì‚¬ ì •ë³´ ì¶œë ¥
    print(f"\n{'=' * 60}")
    print("ì›ë³¸ ê¸°ì‚¬")
    print('=' * 60)
    print(f"ì œëª©: {article.title}")
    print(f"ì¶œì²˜: {article.source}")
    print(f"ë³¸ë¬¸ ê¸¸ì´: {len(article.content)}ì")
    print(f"\në³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸°:")
    print(article.content[:200] + "...")

    # ìš”ì•½ ìƒì„±
    print(f"\n{'=' * 60}")
    print("3ë‹¨ê³„: ìš”ì•½ ìƒì„± ì¤‘...")
    print('=' * 60)

    try:
        summary = analyzer.summarize(article, num_sentences=3)
        print(f"\nâœ… ìš”ì•½ (3ë¬¸ì¥):\n")
        print(summary)

        # ê¸°ì‚¬ ê°ì²´ì— ì €ì¥
        article.summary = summary

        # ì €ì¥
        file_manager.save_articles([article], filename='summarized_article.json')
        print(f"\nğŸ’¾ ìš”ì•½ì´ í¬í•¨ëœ ê¸°ì‚¬ ì €ì¥ ì™„ë£Œ: summarized_article.json")

    except Exception as e:
        print(f"âŒ ìš”ì•½ ì‹¤íŒ¨: {e}")
        exit(1)
```

**6. ì‹¤í–‰**
```bash
python test_gemini.py
```

#### âœ… ì„±ê³µ ê¸°ì¤€
- [ ] API ì—°ê²° ì„±ê³µ ë©”ì‹œì§€ ì¶œë ¥
- [ ] 3ë¬¸ì¥ ìš”ì•½ ìƒì„±
- [ ] `data/raw/summarized_article.json`ì— summary í•„ë“œ í¬í•¨

#### âš ï¸ ì˜ˆìƒ ì˜¤ë¥˜

**ì˜¤ë¥˜ 1:** `GEMINI_API_KEYê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤`
- **í•´ê²°:** `.env` íŒŒì¼ ì—´ì–´ì„œ `GEMINI_API_KEY=ì‹¤ì œ_í‚¤` ì…ë ¥

**ì˜¤ë¥˜ 2:** `API key not valid`
- **í•´ê²°:** https://makersuite.google.com/app/apikey ì—ì„œ ìƒˆ í‚¤ ë°œê¸‰

**ì˜¤ë¥˜ 3:** `Quota exceeded`
- **í•´ê²°:** ë¬´ë£Œ í• ë‹¹ëŸ‰ ì´ˆê³¼, ë‚´ì¼ ë‹¤ì‹œ ì‹œë„ ë˜ëŠ” ìœ ë£Œ ì „í™˜

---

ì´ëŸ° ì‹ìœ¼ë¡œ **ëª¨ë“  ë‹¨ê³„ë¥¼ ìƒì„¸íˆ ì‘ì„±**í•˜ë©´ ì•½ 200í˜ì´ì§€ ë¶„ëŸ‰ì´ ë©ë‹ˆë‹¤.

ë‚˜ë¨¸ì§€ Phaseë„ ë™ì¼í•œ í˜•ì‹ìœ¼ë¡œ ê³„ì† ì‘ì„±í• ê¹Œìš”? ì•„ë‹ˆë©´ íŠ¹ì • Phaseë¥¼ ë” ìì„¸íˆ ì„¤ëª…í•´ë“œë¦´ê¹Œìš”?
