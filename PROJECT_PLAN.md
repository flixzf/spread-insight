# Spread Insight í”„ë¡œì íŠ¸ ê³„íšì„œ

## í”„ë¡œì íŠ¸ ê°œìš”
ì—¬ëŸ¬ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ë¥¼ ìŠ¤í¬ë˜í•‘í•˜ì—¬ í•µì‹¬ ê²½ì œ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³ , AI(Gemini 2.0/2.5 Flash)ë¥¼ í™œìš©í•´ ì¼ë°˜ì¸ë„ ì´í•´í•˜ê¸° ì‰¬ìš´ ì¸ì‚¬ì´íŠ¸ë¡œ ì¬êµ¬ì„±í•˜ì—¬ í…”ë ˆê·¸ë¨ìœ¼ë¡œ ë°°í¬í•˜ëŠ” ì‹œìŠ¤í…œ. ì¿ íŒ¡ íŒŒíŠ¸ë„ˆìŠ¤ í™œë™ì„ í†µí•œ ìˆ˜ìµí™” í¬í•¨.

### ğŸ¯ í•µì‹¬ ê°€ì¹˜ ì œì•ˆ
**ë‹¨ìˆœ ë‰´ìŠ¤ ì „ë‹¬ì´ ì•„ë‹Œ, ë§¤ì¼ ì„±ì¥í•˜ëŠ” ë…ì ë§Œë“¤ê¸°**

- **Learn (ë°°ì›€):** í•˜ë£¨ 1ê°œì˜ ê²½ì œ ì´ìŠˆ + í•µì‹¬ ìš©ì–´ í•™ìŠµ
- **Insight (í†µì°°):** ê³¼ê±° ë°ì´í„°ì™€ ë¹„êµí•˜ì—¬ íŒ¨í„´ íŒŒì•…
- **Action (ì‹¤ì²œ):** "ë‚˜ëŠ” ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œ?"ë¡œ ì´ì–´ì§€ëŠ” êµ¬ì²´ì  í–‰ë™ ì œì‹œ

> ìì„¸í•œ ì½˜í…ì¸  ì „ëµì€ [CONTENT_STRATEGY.md](CONTENT_STRATEGY.md) ì°¸ì¡°

---

## ê¸°ìˆ  ìŠ¤íƒ ì„ ì •

### í”„ë¡œê·¸ë˜ë° ì–¸ì–´: Python
**ì„ ì • ì´ìœ :**
- ì›¹ ìŠ¤í¬ë˜í•‘ ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒíƒœê³„ê°€ ê°€ì¥ í’ë¶€ (BeautifulSoup4, Selenium, Scrapy)
- AI API ì—°ë™ì´ ê°„í¸ (Gemini, OpenAI ë“± ê³µì‹ SDK ì œê³µ)
- ë°ì´í„° ì²˜ë¦¬ ë° ì‹œê°í™” ë„êµ¬ ì„±ìˆ™ (Pandas, Matplotlib, Plotly)
- í…”ë ˆê·¸ë¨ ë´‡ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì•ˆì •ì  (python-telegram-bot)
- ë°°í¬ ë° ìŠ¤ì¼€ì¤„ë§ ìš©ì´ (APScheduler, cron)
- í´ë¼ìš°ë“œ ë°°í¬ ì˜µì…˜ ë‹¤ì–‘ (AWS Lambda, GCP Cloud Functions)

**C/C++ ì œì™¸ ì´ìœ :**
- ì›¹ ìŠ¤í¬ë˜í•‘, AI API ì—°ë™ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶€ì¡±
- ê°œë°œ ì†ë„ ëŠë¦¼, ë³µì¡ë„ ë†’ìŒ

**C# ì œì™¸ ì´ìœ :**
- ìŠ¤í¬ë˜í•‘ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ Pythonë³´ë‹¤ ì œí•œì 
- Linux í™˜ê²½ ë°°í¬ ì‹œ .NET Core ì˜ì¡´ì„±
- AI/ë°ì´í„° ë¶„ì„ ìƒíƒœê³„ê°€ Pythonë³´ë‹¤ ì•½í•¨

### AI ëª¨ë¸: Google Gemini 2.0/2.5 Flash
- ë¹ ë¥¸ ì‘ë‹µ ì†ë„ (Flash ëª¨ë¸)
- ë¬´ë£Œ í• ë‹¹ëŸ‰ ì¶©ë¶„ (ë¶„ë‹¹ 15 ìš”ì²­)
- Claude ëŒ€ë¹„ 1/10 ê°€ê²©
- í•œêµ­ì–´ ì§€ì› ì–‘í˜¸
- ê¸´ ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ ê°€ëŠ¥

### í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
```
# ì›¹ ìŠ¤í¬ë˜í•‘
beautifulsoup4==4.12.3
selenium==4.18.1
requests==2.31.0
lxml==5.1.0

# AI ì²˜ë¦¬
google-generativeai==0.4.0

# ë°ì´í„° ì²˜ë¦¬
pandas==2.2.0
numpy==1.26.4

# ì‹œê°í™”
plotly==5.19.0
matplotlib==3.8.3

# í…”ë ˆê·¸ë¨
python-telegram-bot==20.8

# ì›¹ ì„œë²„ (ì˜µì…˜)
flask==3.0.2

# ë°ì´í„°ë² ì´ìŠ¤
sqlalchemy==2.0.27

# ìŠ¤ì¼€ì¤„ë§
apscheduler==3.10.4

# ìœ í‹¸ë¦¬í‹°
python-dotenv==1.0.1
pydantic==2.6.1
```

---

## ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ë‰´ìŠ¤ ìŠ¤í¬ë˜í¼   â”‚ (ë„¤ì´ë²„, ë‹¤ìŒ, ì¡°ì„ ë¹„ì¦ˆ, í•œê²½)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ë‰´ìŠ¤ ì„ ì •      â”‚ (í‚¤ì›Œë“œ ì ìˆ˜ + Gemini ìµœì¢… ì„ íƒ)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â† í•˜ë£¨ 1ê°œ ì„ ì •
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ë°ì´í„° ì €ì¥    â”‚ (JSON/SQLite)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘â”‚ (ECOS API, yfinance, ìì²´ DB)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Gemini AI ë¶„ì„ â”‚ (ìš”ì•½, ê³¼ê±° ë¹„êµ, ì¸ì‚¬ì´íŠ¸, í–‰ë™ ì œì•ˆ)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ìš©ì–´ ì¶”ì¶œ/ì„¤ëª…  â”‚ (ìš©ì–´ DB + Gemini ì‰¬ìš´ ì„¤ëª…)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ì‹œê°í™” ìƒì„±     â”‚ (Plotly - ê³¼ê±° íŒ¨í„´ ì°¨íŠ¸)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HTML í˜ì´ì§€     â”‚ (Learn â†’ Insight â†’ Action êµ¬ì¡°)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ GitHub Pages ë°°í¬â”‚ (ë¬´ë£Œ í˜¸ìŠ¤íŒ…)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  í…”ë ˆê·¸ë¨ ì „ì†¡   â”‚ (ë´‡ API - ë§í¬ í¬í•¨ ë©”ì‹œì§€)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
spread_insight/
â”œâ”€â”€ scrapers/                  # ë‰´ìŠ¤ ìˆ˜ì§‘ ëª¨ë“ˆ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_scraper.py       # ì¶”ìƒ í´ë˜ìŠ¤
â”‚   â”œâ”€â”€ naver_scraper.py      # ë„¤ì´ë²„ ë‰´ìŠ¤
â”‚   â”œâ”€â”€ daum_scraper.py       # ë‹¤ìŒ ë‰´ìŠ¤
â”‚   â”œâ”€â”€ chosun_scraper.py     # ì¡°ì„ ë¹„ì¦ˆ
â”‚   â””â”€â”€ hankyung_scraper.py   # í•œêµ­ê²½ì œ
â”‚
â”œâ”€â”€ analyzers/                 # AI ë¶„ì„ ëª¨ë“ˆ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ gemini_analyzer.py    # Gemini API ì—°ë™
â”‚   â”œâ”€â”€ context_builder.py    # ê³¼ê±° ë§¥ë½ êµ¬ì„±
â”‚   â”œâ”€â”€ summarizer.py         # ìš”ì•½ ìƒì„±
â”‚   â””â”€â”€ terminology.py        # ìš©ì–´ í•´ì„¤ ìƒì„±
â”‚
â”œâ”€â”€ visualizers/               # ë°ì´í„° ì‹œê°í™”
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ chart_generator.py    # ê·¸ë˜í”„ ìƒì„±
â”‚   â”œâ”€â”€ data_fetcher.py       # ê²½ì œ ë°ì´í„° ìˆ˜ì§‘
â”‚   â””â”€â”€ templates/            # ê·¸ë˜í”„ í…œí”Œë¦¿
â”‚
â”œâ”€â”€ publishers/                # ë°°í¬ ëª¨ë“ˆ
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ html_generator.py     # HTML ìƒì„±
â”‚   â”œâ”€â”€ github_deployer.py    # GitHub Pages ë°°í¬
â”‚   â”œâ”€â”€ telegram_bot.py       # í…”ë ˆê·¸ë¨ ë´‡
â”‚   â””â”€â”€ coupang_partner.py    # ì¿ íŒ¡ íŒŒíŠ¸ë„ˆìŠ¤ ì—°ë™
â”‚
â”œâ”€â”€ models/                    # ë°ì´í„° ëª¨ë¸
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ news_article.py       # ë‰´ìŠ¤ ê¸°ì‚¬ ëª¨ë¸
â”‚   â””â”€â”€ insight.py            # ì¸ì‚¬ì´íŠ¸ ëª¨ë¸
â”‚
â”œâ”€â”€ database/                  # ë°ì´í„°ë² ì´ìŠ¤
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ db_manager.py         # DB ê´€ë¦¬
â”‚   â””â”€â”€ news.db               # SQLite DB
â”‚
â”œâ”€â”€ utils/                     # ìœ í‹¸ë¦¬í‹°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py             # ì„¤ì • ê´€ë¦¬
â”‚   â”œâ”€â”€ logger.py             # ë¡œê¹…
â”‚   â””â”€â”€ exceptions.py         # ì»¤ìŠ¤í…€ ì˜ˆì™¸
â”‚
â”œâ”€â”€ templates/                 # HTML í…œí”Œë¦¿
â”‚   â”œâ”€â”€ base.html
â”‚   â”œâ”€â”€ news_detail.html
â”‚   â””â”€â”€ assets/
â”‚       â”œâ”€â”€ style.css
â”‚       â””â”€â”€ script.js
â”‚
â”œâ”€â”€ data/                      # ì„ì‹œ ë°ì´í„°
â”‚   â”œâ”€â”€ raw/                  # ì›ë³¸ ë‰´ìŠ¤
â”‚   â”œâ”€â”€ processed/            # ì²˜ë¦¬ëœ ë°ì´í„°
â”‚   â””â”€â”€ charts/               # ìƒì„±ëœ ê·¸ë˜í”„
â”‚
â”œâ”€â”€ tests/                     # í…ŒìŠ¤íŠ¸ ì½”ë“œ
â”‚   â”œâ”€â”€ test_scrapers.py
â”‚   â”œâ”€â”€ test_analyzers.py
â”‚   â””â”€â”€ test_publishers.py
â”‚
â”œâ”€â”€ .env                       # í™˜ê²½ ë³€ìˆ˜ (API í‚¤)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt           # ì˜ì¡´ì„±
â”œâ”€â”€ main.py                    # ë©”ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ scheduler.py               # ìŠ¤ì¼€ì¤„ëŸ¬
â””â”€â”€ README.md
```

---

## ì›Œí¬í”Œë¡œìš° ìƒì„¸

### 1ë‹¨ê³„: ë‰´ìŠ¤ ìˆ˜ì§‘
1. ìŠ¤ì¼€ì¤„ëŸ¬ íŠ¸ë¦¬ê±° (ë§¤ì¼ ì˜¤ì „ 7ì‹œ, ì˜¤í›„ 6ì‹œ)
2. ê° ìŠ¤í¬ë˜í¼ê°€ ìµœê·¼ 24ì‹œê°„ ë‰´ìŠ¤ ìˆ˜ì§‘
3. ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
4. SQLite DB ì €ì¥

### 2ë‹¨ê³„: AI ë¶„ì„
1. ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ì¤‘ ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚° (í‚¤ì›Œë“œ, ì¡°íšŒìˆ˜ ê¸°ë°˜)
2. ìƒìœ„ 5ê°œ ì„ ì •
3. Gemini API í˜¸ì¶œ:
   - 3ì¤„ ìš”ì•½ ìƒì„±
   - ê³¼ê±° ê´€ë ¨ ë‰´ìŠ¤ ê²€ìƒ‰ (DBì—ì„œ)
   - ì‹œê³„ì—´ ë§¥ë½ êµ¬ì„±
   - ì‰¬ìš´ ì–¸ì–´ë¡œ ì¬ì‘ì„± (ì¤‘í•™ìƒ ëˆˆë†’ì´)
   - ì „ë¬¸ ìš©ì–´ ìë™ ì¶”ì¶œ ë° í•´ì„¤ ìƒì„±

### 3ë‹¨ê³„: ì‹œê°í™”
1. ë‰´ìŠ¤ í‚¤ì›Œë“œ ê¸°ë°˜ ê´€ë ¨ ê²½ì œ ì§€í‘œ ì‹ë³„
2. í•œêµ­ì€í–‰ Open API ë˜ëŠ” Yahoo Financeì—ì„œ ë°ì´í„° ìˆ˜ì§‘
3. Plotlyë¡œ ì¸í„°ë™í‹°ë¸Œ ê·¸ë˜í”„ ìƒì„± (PNG + HTML)

### 4ë‹¨ê³„: ì›¹ í˜ì´ì§€ ìƒì„±
1. HTML í…œí”Œë¦¿ì— ë°ì´í„° ì‚½ì…
2. ê° ë‰´ìŠ¤ë§ˆë‹¤ ê³ ìœ  HTML íŒŒì¼ ìƒì„±
3. ê·¸ë˜í”„ ì´ë¯¸ì§€ í¬í•¨
4. ì¿ íŒ¡ íŒŒíŠ¸ë„ˆìŠ¤ ë§í¬ ì‚½ì…

### 5ë‹¨ê³„: ë°°í¬
1. GitHub Pages ì €ì¥ì†Œì— HTML íŒŒì¼ push
2. ê³ ìœ  URL ìƒì„± (ì˜ˆ: `https://username.github.io/news/20250110_news1.html`)

### 6ë‹¨ê³„: í…”ë ˆê·¸ë¨ ì „ì†¡
1. ê° ë‰´ìŠ¤ë§ˆë‹¤ ë©”ì‹œì§€ êµ¬ì„±:
   - ì œëª© + í•œì¤„ ìš”ì•½
   - ë²„íŠ¼ 1: "ìƒì„¸ ì¸ì‚¬ì´íŠ¸ ë³´ê¸°" (GitHub Pages URL)
   - ë²„íŠ¼ 2: "ê´€ë ¨ ìƒí’ˆ ë³´ê¸°" (ì¿ íŒ¡ íŒŒíŠ¸ë„ˆìŠ¤ URL)
2. êµ¬ë…ì ë¦¬ìŠ¤íŠ¸ì— ì¼ê´„ ì „ì†¡

---

## Phase 1: ë‰´ìŠ¤ ìˆ˜ì§‘ (Week 1)

### Step 1.1: í”„ë¡œì íŠ¸ ì´ˆê¸° ì„¤ì •
**ëª©í‘œ:** ê°œë°œ í™˜ê²½ êµ¬ì¶•
**ì†Œìš” ì‹œê°„:** 30ë¶„

**ì„¸ë¶€ ì‘ì—…:**
1. í´ë” êµ¬ì¡° ìƒì„±
   ```bash
   mkdir -p spread_insight/{scrapers,analyzers,visualizers,publishers,models,database,utils,templates,data,tests}
   cd spread_insight
   ```

2. Git ì´ˆê¸°í™”
   ```bash
   git init
   ```

3. `.gitignore` ì‘ì„±
   ```
   .env
   __pycache__/
   *.pyc
   data/
   *.db
   venv/
   .vscode/
   ```

4. ê°€ìƒí™˜ê²½ ìƒì„±
   ```bash
   python -m venv venv
   source venv/bin/activate  # Windows: venv\Scripts\activate
   ```

5. `requirements.txt` ì‘ì„± (Phase 1ìš©)
   ```
   beautifulsoup4==4.12.3
   requests==2.31.0
   lxml==5.1.0
   python-dotenv==1.0.1
   ```

6. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
   ```bash
   pip install -r requirements.txt
   ```

7. `.env` íŒŒì¼ ìƒì„±
   ```
   # ë‚˜ì¤‘ì— ì¶”ê°€í•  API í‚¤ë“¤
   GEMINI_API_KEY=
   TELEGRAM_BOT_TOKEN=
   COUPANG_ACCESS_KEY=
   ```

**ì„±ê³µ ê¸°ì¤€:**
- í´ë” êµ¬ì¡° ì™„ì„±
- ê°€ìƒí™˜ê²½ í™œì„±í™”
- ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì™„ë£Œ

---

### Step 1.2: ë‹¨ì¼ ê¸°ì‚¬ ìŠ¤í¬ë˜í•‘ (ë„¤ì´ë²„)
**ëª©í‘œ:** ë„¤ì´ë²„ ê²½ì œ ë‰´ìŠ¤ 1ê°œ ê¸°ì‚¬ì˜ ì œëª©, ë³¸ë¬¸, ë‚ ì§œ ì¶”ì¶œ
**ì†Œìš” ì‹œê°„:** 1ì‹œê°„

**ì„¸ë¶€ ì‘ì—…:**

1. `scrapers/base_scraper.py` ì‘ì„±
   ```python
   from abc import ABC, abstractmethod
   from dataclasses import dataclass
   from datetime import datetime

   @dataclass
   class NewsArticle:
       url: str
       title: str
       content: str
       published_at: datetime
       source: str
       keywords: list[str] = None

   class BaseScraper(ABC):
       @abstractmethod
       def scrape_article(self, url: str) -> NewsArticle:
           pass
   ```

2. `scrapers/naver_scraper.py` ì‘ì„±
   ```python
   import requests
   from bs4 import BeautifulSoup
   from .base_scraper import BaseScraper, NewsArticle
   from datetime import datetime

   class NaverScraper(BaseScraper):
       def scrape_article(self, url: str) -> NewsArticle:
           # User-Agent ì„¤ì • (ì°¨ë‹¨ ë°©ì§€)
           headers = {
               'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
           }

           # í˜ì´ì§€ ìš”ì²­
           response = requests.get(url, headers=headers)
           response.raise_for_status()

           # HTML íŒŒì‹±
           soup = BeautifulSoup(response.text, 'lxml')

           # ì œëª© ì¶”ì¶œ
           title = soup.select_one('#title_area span').get_text(strip=True)

           # ë³¸ë¬¸ ì¶”ì¶œ
           content_elem = soup.select('#dic_area')
           content = '\n'.join([p.get_text(strip=True) for p in content_elem])

           # ë‚ ì§œ ì¶”ì¶œ
           date_str = soup.select_one('.media_end_head_info_datestamp_time').get('data-date-time')
           published_at = datetime.fromisoformat(date_str)

           return NewsArticle(
               url=url,
               title=title,
               content=content,
               published_at=published_at,
               source='ë„¤ì´ë²„'
           )
   ```

3. `test_scraper.py` ì‘ì„± (ë£¨íŠ¸ ë””ë ‰í† ë¦¬)
   ```python
   from scrapers.naver_scraper import NaverScraper

   if __name__ == '__main__':
       # í…ŒìŠ¤íŠ¸ìš© ë„¤ì´ë²„ ë‰´ìŠ¤ URL (ì‹¤ì œ URLë¡œ êµì²´)
       test_url = 'https://n.news.naver.com/article/...'

       scraper = NaverScraper()
       article = scraper.scrape_article(test_url)

       print(f"ì œëª©: {article.title}")
       print(f"ë‚ ì§œ: {article.published_at}")
       print(f"ë³¸ë¬¸: {article.content[:200]}...")  # ì• 200ìë§Œ
   ```

4. ì‹¤í–‰ ë° ê²€ì¦
   ```bash
   python test_scraper.py
   ```

**ì˜ˆìƒ ì´ìŠˆ ë° í•´ê²°:**
- **ì´ìŠˆ 1:** ë„¤ì´ë²„ HTML êµ¬ì¡°ê°€ ë‹¬ë¼ì„œ ì„ íƒìê°€ ì•ˆ ë§ìŒ
  - **í•´ê²°:** ë¸Œë¼ìš°ì € ê°œë°œì ë„êµ¬ë¡œ ì‹¤ì œ HTML êµ¬ì¡° í™•ì¸ í›„ ì„ íƒì ìˆ˜ì •

- **ì´ìŠˆ 2:** 403 Forbidden ì—ëŸ¬
  - **í•´ê²°:** User-Agent í—¤ë” ì¶”ê°€ ë˜ëŠ” ë” ë³µì¡í•œ í—¤ë” ì„¤ì •

- **ì´ìŠˆ 3:** ë™ì  ì½˜í…ì¸ ë¼ requestsë¡œ ì•ˆ ë¨
  - **í•´ê²°:** Seleniumìœ¼ë¡œ ì „í™˜ (ë‚˜ì¤‘ì—)

**ì„±ê³µ ê¸°ì¤€:**
- ì½˜ì†”ì— ì œëª©, ë‚ ì§œ, ë³¸ë¬¸ ì¼ë¶€ ì¶œë ¥
- ì—ëŸ¬ ì—†ì´ ì‹¤í–‰ ì™„ë£Œ

---

### Step 1.3: ì—¬ëŸ¬ ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘
**ëª©í‘œ:** ë„¤ì´ë²„ ê²½ì œ ì„¹ì…˜ ë©”ì¸ í˜ì´ì§€ì—ì„œ ìµœì‹  10ê°œ ê¸°ì‚¬ URL ì¶”ì¶œ
**ì†Œìš” ì‹œê°„:** 1ì‹œê°„

**ì„¸ë¶€ ì‘ì—…:**

1. `scrapers/naver_scraper.py`ì— ë©”ì„œë“œ ì¶”ê°€
   ```python
   def get_article_list(self, category_url: str, limit: int = 10) -> list[str]:
       """
       ì¹´í…Œê³ ë¦¬ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ URL ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ

       Args:
           category_url: ë„¤ì´ë²„ ê²½ì œ ì„¹ì…˜ URL
           limit: ê°€ì ¸ì˜¬ ê¸°ì‚¬ ìˆ˜

       Returns:
           ê¸°ì‚¬ URL ë¦¬ìŠ¤íŠ¸
       """
       headers = {
           'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
       }

       response = requests.get(category_url, headers=headers)
       soup = BeautifulSoup(response.text, 'lxml')

       # ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ (ì‹¤ì œ ì„ íƒìëŠ” í™•ì¸ í•„ìš”)
       article_links = []
       for link in soup.select('.sa_text_title')[:limit]:
           href = link.get('href')
           if href:
               article_links.append(href)

       return article_links
   ```

2. `test_scraper.py` ì—…ë°ì´íŠ¸
   ```python
   from scrapers.naver_scraper import NaverScraper

   if __name__ == '__main__':
       scraper = NaverScraper()

       # ë„¤ì´ë²„ ê²½ì œ ì„¹ì…˜ URL
       economy_url = 'https://news.naver.com/section/101'

       # ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘
       article_urls = scraper.get_article_list(economy_url, limit=10)

       print(f"ì´ {len(article_urls)}ê°œ ê¸°ì‚¬ ë°œê²¬")
       for i, url in enumerate(article_urls, 1):
           print(f"{i}. {url}")

       # ì²« ë²ˆì§¸ ê¸°ì‚¬ ìƒì„¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
       if article_urls:
           print("\nì²« ë²ˆì§¸ ê¸°ì‚¬ ìƒì„¸:")
           article = scraper.scrape_article(article_urls[0])
           print(f"ì œëª©: {article.title}")
           print(f"ë³¸ë¬¸: {article.content[:100]}...")
   ```

3. ì‹¤í–‰
   ```bash
   python test_scraper.py
   ```

**ì„±ê³µ ê¸°ì¤€:**
- 10ê°œì˜ ìœ íš¨í•œ URL ì¶œë ¥
- ì²« ë²ˆì§¸ ê¸°ì‚¬ì˜ ìƒì„¸ ì •ë³´ ì¶œë ¥

---

### Step 1.4: ë°ì´í„° ëª¨ë¸ ì •ì˜ ë° JSON ì €ì¥
**ëª©í‘œ:** ìˆ˜ì§‘í•œ ê¸°ì‚¬ë¥¼ êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ JSON íŒŒì¼ì— ì €ì¥
**ì†Œìš” ì‹œê°„:** 45ë¶„

**ì„¸ë¶€ ì‘ì—…:**

1. `models/news_article.py` ì‘ì„± (ë” ì™„ì„±ë„ ë†’ê²Œ)
   ```python
   from dataclasses import dataclass, asdict
   from datetime import datetime
   from typing import Optional
   import json

   @dataclass
   class NewsArticle:
       url: str
       title: str
       content: str
       published_at: datetime
       source: str
       keywords: Optional[list[str]] = None
       summary: Optional[str] = None
       easy_explanation: Optional[str] = None
       terminology: Optional[dict[str, str]] = None

       def to_dict(self) -> dict:
           """datetimeì„ ISO í¬ë§· ë¬¸ìì—´ë¡œ ë³€í™˜"""
           data = asdict(self)
           data['published_at'] = self.published_at.isoformat()
           return data

       @classmethod
       def from_dict(cls, data: dict) -> 'NewsArticle':
           """ë”•ì…”ë„ˆë¦¬ì—ì„œ ê°ì²´ ë³µì›"""
           data['published_at'] = datetime.fromisoformat(data['published_at'])
           return cls(**data)

       def save_to_json(self, filepath: str):
           """JSON íŒŒì¼ë¡œ ì €ì¥"""
           with open(filepath, 'w', encoding='utf-8') as f:
               json.dump(self.to_dict(), f, ensure_ascii=False, indent=2)

       @classmethod
       def load_from_json(cls, filepath: str) -> 'NewsArticle':
           """JSON íŒŒì¼ì—ì„œ ë¡œë“œ"""
           with open(filepath, 'r', encoding='utf-8') as f:
               data = json.load(f)
           return cls.from_dict(data)
   ```

2. `utils/file_manager.py` ì‘ì„±
   ```python
   import json
   import os
   from typing import List
   from models.news_article import NewsArticle

   class FileManager:
       def __init__(self, data_dir: str = './data'):
           self.data_dir = data_dir
           self.raw_dir = os.path.join(data_dir, 'raw')
           os.makedirs(self.raw_dir, exist_ok=True)

       def save_articles(self, articles: List[NewsArticle], filename: str = 'news_data.json'):
           """ì—¬ëŸ¬ ê¸°ì‚¬ë¥¼ í•˜ë‚˜ì˜ JSON íŒŒì¼ë¡œ ì €ì¥"""
           filepath = os.path.join(self.raw_dir, filename)
           data = [article.to_dict() for article in articles]

           with open(filepath, 'w', encoding='utf-8') as f:
               json.dump(data, f, ensure_ascii=False, indent=2)

           print(f"âœ… {len(articles)}ê°œ ê¸°ì‚¬ë¥¼ {filepath}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

       def load_articles(self, filename: str = 'news_data.json') -> List[NewsArticle]:
           """JSON íŒŒì¼ì—ì„œ ê¸°ì‚¬ ë¡œë“œ"""
           filepath = os.path.join(self.raw_dir, filename)

           with open(filepath, 'r', encoding='utf-8') as f:
               data = json.load(f)

           return [NewsArticle.from_dict(item) for item in data]
   ```

3. `test_scraper.py` ì—…ë°ì´íŠ¸
   ```python
   from scrapers.naver_scraper import NaverScraper
   from utils.file_manager import FileManager
   from datetime import datetime

   if __name__ == '__main__':
       scraper = NaverScraper()
       file_manager = FileManager()

       # ê¸°ì‚¬ ëª©ë¡ ìˆ˜ì§‘
       economy_url = 'https://news.naver.com/section/101'
       article_urls = scraper.get_article_list(economy_url, limit=5)  # í…ŒìŠ¤íŠ¸ìš© 5ê°œ

       # ê° ê¸°ì‚¬ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
       articles = []
       for i, url in enumerate(article_urls, 1):
           try:
               print(f"[{i}/{len(article_urls)}] {url} ìˆ˜ì§‘ ì¤‘...")
               article = scraper.scrape_article(url)
               articles.append(article)
           except Exception as e:
               print(f"âŒ ì—ëŸ¬: {e}")
               continue

       # JSON íŒŒì¼ë¡œ ì €ì¥
       timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
       filename = f'naver_news_{timestamp}.json'
       file_manager.save_articles(articles, filename)

       # ì €ì¥ëœ íŒŒì¼ ë‹¤ì‹œ ì½ê¸° (ê²€ì¦)
       loaded_articles = file_manager.load_articles(filename)
       print(f"\nâœ… {len(loaded_articles)}ê°œ ê¸°ì‚¬ ë¡œë“œ ì™„ë£Œ")
       print(f"ì²« ë²ˆì§¸ ê¸°ì‚¬: {loaded_articles[0].title}")
   ```

4. ì‹¤í–‰
   ```bash
   python test_scraper.py
   ```

**ì„±ê³µ ê¸°ì¤€:**
- `data/raw/naver_news_YYYYMMDD_HHMMSS.json` íŒŒì¼ ìƒì„±
- JSON íŒŒì¼ ë‚´ìš© í™•ì¸ (UTF-8, ë“¤ì—¬ì“°ê¸° ì •ìƒ)
- ë‹¤ì‹œ ë¡œë“œí–ˆì„ ë•Œ ë°ì´í„° ì¼ì¹˜

---

### Step 1.5: ë‹¤ìŒ(Daum) ìŠ¤í¬ë˜í¼ ì¶”ê°€
**ëª©í‘œ:** ë„¤ì´ë²„ì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ë‹¤ìŒ ë‰´ìŠ¤ ìˆ˜ì§‘
**ì†Œìš” ì‹œê°„:** 1.5ì‹œê°„

**ì„¸ë¶€ ì‘ì—…:**

1. `scrapers/daum_scraper.py` ì‘ì„±
   ```python
   import requests
   from bs4 import BeautifulSoup
   from .base_scraper import BaseScraper
   from models.news_article import NewsArticle
   from datetime import datetime

   class DaumScraper(BaseScraper):
       def get_article_list(self, category_url: str, limit: int = 10) -> list[str]:
           """ë‹¤ìŒ ê²½ì œ ì„¹ì…˜ì—ì„œ ê¸°ì‚¬ URL ì¶”ì¶œ"""
           headers = {
               'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
           }

           response = requests.get(category_url, headers=headers)
           soup = BeautifulSoup(response.text, 'lxml')

           # ë‹¤ìŒ ë‰´ìŠ¤ ë§í¬ ì„ íƒì (ì‹¤ì œ í™•ì¸ í•„ìš”)
           article_links = []
           for link in soup.select('.link_txt')[:limit]:
               href = link.get('href')
               if href and href.startswith('http'):
                   article_links.append(href)

           return article_links

       def scrape_article(self, url: str) -> NewsArticle:
           """ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ ìƒì„¸ ì •ë³´ ì¶”ì¶œ"""
           headers = {
               'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
           }

           response = requests.get(url, headers=headers)
           response.raise_for_status()
           soup = BeautifulSoup(response.text, 'lxml')

           # ì œëª© (ë‹¤ìŒ ë‰´ìŠ¤ êµ¬ì¡°ì— ë§ê²Œ)
           title = soup.select_one('h3.tit_view').get_text(strip=True)

           # ë³¸ë¬¸
           content_elem = soup.select('#harmonyContainer p')
           content = '\n'.join([p.get_text(strip=True) for p in content_elem])

           # ë‚ ì§œ
           date_str = soup.select_one('.num_date').get_text(strip=True)
           # ë‚ ì§œ íŒŒì‹± (ì˜ˆ: "2025.01.10. ì˜¤í›„ 3:24")
           published_at = datetime.strptime(date_str.replace('.', '').strip(), '%Y%m%d ì˜¤í›„ %H:%M')

           return NewsArticle(
               url=url,
               title=title,
               content=content,
               published_at=published_at,
               source='ë‹¤ìŒ'
           )
   ```

2. `test_multi_scraper.py` ì‘ì„± (ì—¬ëŸ¬ ì†ŒìŠ¤ í†µí•© í…ŒìŠ¤íŠ¸)
   ```python
   from scrapers.naver_scraper import NaverScraper
   from scrapers.daum_scraper import DaumScraper
   from utils.file_manager import FileManager
   from datetime import datetime

   if __name__ == '__main__':
       file_manager = FileManager()
       all_articles = []

       # ë„¤ì´ë²„ ìˆ˜ì§‘
       print("=== ë„¤ì´ë²„ ë‰´ìŠ¤ ìˆ˜ì§‘ ===")
       naver_scraper = NaverScraper()
       naver_urls = naver_scraper.get_article_list('https://news.naver.com/section/101', limit=3)
       for url in naver_urls:
           try:
               article = naver_scraper.scrape_article(url)
               all_articles.append(article)
               print(f"âœ… {article.title[:30]}...")
           except Exception as e:
               print(f"âŒ {e}")

       # ë‹¤ìŒ ìˆ˜ì§‘
       print("\n=== ë‹¤ìŒ ë‰´ìŠ¤ ìˆ˜ì§‘ ===")
       daum_scraper = DaumScraper()
       daum_urls = daum_scraper.get_article_list('https://news.daum.net/economy', limit=3)
       for url in daum_urls:
           try:
               article = daum_scraper.scrape_article(url)
               all_articles.append(article)
               print(f"âœ… {article.title[:30]}...")
           except Exception as e:
               print(f"âŒ {e}")

       # í†µí•© ì €ì¥
       timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
       filename = f'combined_news_{timestamp}.json'
       file_manager.save_articles(all_articles, filename)

       print(f"\nì´ {len(all_articles)}ê°œ ê¸°ì‚¬ ì €ì¥ ì™„ë£Œ")
   ```

3. ì‹¤í–‰
   ```bash
   python test_multi_scraper.py
   ```

**ì„±ê³µ ê¸°ì¤€:**
- ë„¤ì´ë²„ 3ê°œ + ë‹¤ìŒ 3ê°œ = ì´ 6ê°œ ê¸°ì‚¬ ìˆ˜ì§‘
- `data/raw/combined_news_YYYYMMDD_HHMMSS.json` íŒŒì¼ ìƒì„±

---

### Step 1.6: ì¤‘ë³µ ì œê±° ë° ë°ì´í„° ì •ì œ
**ëª©í‘œ:** ë™ì¼ ê¸°ì‚¬(URL ê¸°ì¤€) ì¤‘ë³µ ì œê±°, ë¹ˆ ë³¸ë¬¸ í•„í„°ë§
**ì†Œìš” ì‹œê°„:** 45ë¶„

**ì„¸ë¶€ ì‘ì—…:**

1. `utils/data_cleaner.py` ì‘ì„±
   ```python
   from typing import List
   from models.news_article import NewsArticle

   class DataCleaner:
       @staticmethod
       def remove_duplicates(articles: List[NewsArticle]) -> List[NewsArticle]:
           """URL ê¸°ì¤€ ì¤‘ë³µ ì œê±°"""
           seen_urls = set()
           unique_articles = []

           for article in articles:
               if article.url not in seen_urls:
                   seen_urls.add(article.url)
                   unique_articles.append(article)

           print(f"ì¤‘ë³µ ì œê±°: {len(articles)} â†’ {len(unique_articles)}ê°œ")
           return unique_articles

       @staticmethod
       def filter_invalid(articles: List[NewsArticle]) -> List[NewsArticle]:
           """ìœ íš¨í•˜ì§€ ì•Šì€ ê¸°ì‚¬ í•„í„°ë§"""
           valid_articles = []

           for article in articles:
               # ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ì œì™¸
               if len(article.content) < 100:
                   print(f"âŒ ë³¸ë¬¸ ë„ˆë¬´ ì§§ìŒ: {article.title[:30]}")
                   continue

               # ì œëª©ì´ ë¹„ì–´ìˆìœ¼ë©´ ì œì™¸
               if not article.title.strip():
                   print(f"âŒ ì œëª© ì—†ìŒ: {article.url}")
                   continue

               valid_articles.append(article)

           print(f"ìœ íš¨ì„± ê²€ì¦: {len(articles)} â†’ {len(valid_articles)}ê°œ")
           return valid_articles

       @staticmethod
       def clean(articles: List[NewsArticle]) -> List[NewsArticle]:
           """ì „ì²´ ì •ì œ íŒŒì´í”„ë¼ì¸"""
           articles = DataCleaner.remove_duplicates(articles)
           articles = DataCleaner.filter_invalid(articles)
           return articles
   ```

2. `test_multi_scraper.py`ì— ì •ì œ ë¡œì§ ì¶”ê°€
   ```python
   # ... (ì´ì „ ì½”ë“œ)
   from utils.data_cleaner import DataCleaner

   # í†µí•© ì €ì¥ ì „ì— ì •ì œ
   print("\n=== ë°ì´í„° ì •ì œ ===")
   cleaned_articles = DataCleaner.clean(all_articles)

   timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
   filename = f'cleaned_news_{timestamp}.json'
   file_manager.save_articles(cleaned_articles, filename)
   ```

**ì„±ê³µ ê¸°ì¤€:**
- ì¤‘ë³µ ì œê±° ë™ì‘ í™•ì¸
- ì§§ì€ ë³¸ë¬¸ í•„í„°ë§ ë™ì‘ í™•ì¸

---

### Step 1.7: ì—ëŸ¬ í•¸ë“¤ë§ ë° ë¡œê¹…
**ëª©í‘œ:** ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ ì‹œ ë¡œê·¸ ê¸°ë¡, ì¬ì‹œë„ ë¡œì§
**ì†Œìš” ì‹œê°„:** 1ì‹œê°„

**ì„¸ë¶€ ì‘ì—…:**

1. `utils/logger.py` ì‘ì„±
   ```python
   import logging
   import os
   from datetime import datetime

   def setup_logger(name: str = 'spread_insight') -> logging.Logger:
       """ë¡œê±° ì„¤ì •"""
       # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
       log_dir = './logs'
       os.makedirs(log_dir, exist_ok=True)

       # ë¡œê·¸ íŒŒì¼ëª… (ë‚ ì§œë³„)
       log_file = os.path.join(log_dir, f'{datetime.now().strftime("%Y%m%d")}.log')

       # ë¡œê±° ìƒì„±
       logger = logging.getLogger(name)
       logger.setLevel(logging.DEBUG)

       # íŒŒì¼ í•¸ë“¤ëŸ¬
       file_handler = logging.FileHandler(log_file, encoding='utf-8')
       file_handler.setLevel(logging.DEBUG)

       # ì½˜ì†” í•¸ë“¤ëŸ¬
       console_handler = logging.StreamHandler()
       console_handler.setLevel(logging.INFO)

       # í¬ë§¤í„°
       formatter = logging.Formatter(
           '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
       )
       file_handler.setFormatter(formatter)
       console_handler.setFormatter(formatter)

       logger.addHandler(file_handler)
       logger.addHandler(console_handler)

       return logger
   ```

2. `scrapers/base_scraper.py`ì— ì¬ì‹œë„ ë¡œì§ ì¶”ê°€
   ```python
   import time
   from utils.logger import setup_logger

   class BaseScraper(ABC):
       def __init__(self):
           self.logger = setup_logger(self.__class__.__name__)

       def scrape_with_retry(self, url: str, max_retries: int = 3) -> NewsArticle:
           """ì¬ì‹œë„ ë¡œì§ í¬í•¨ ìŠ¤í¬ë˜í•‘"""
           for attempt in range(1, max_retries + 1):
               try:
                   self.logger.info(f"[ì‹œë„ {attempt}/{max_retries}] {url}")
                   article = self.scrape_article(url)
                   self.logger.info(f"âœ… ì„±ê³µ: {article.title}")
                   return article
               except Exception as e:
                   self.logger.error(f"âŒ ì‹¤íŒ¨: {e}")
                   if attempt < max_retries:
                       wait_time = 2 ** attempt  # ì§€ìˆ˜ ë°±ì˜¤í”„
                       self.logger.info(f"â³ {wait_time}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                       time.sleep(wait_time)
                   else:
                       self.logger.error(f"ìµœì¢… ì‹¤íŒ¨: {url}")
                       raise
   ```

3. `test_multi_scraper.py` ì—…ë°ì´íŠ¸ (ì¬ì‹œë„ ì ìš©)
   ```python
   # scraper.scrape_article(url) ëŒ€ì‹ 
   article = scraper.scrape_with_retry(url)
   ```

**ì„±ê³µ ê¸°ì¤€:**
- `logs/YYYYMMDD.log` íŒŒì¼ ìƒì„±
- ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ ë™ì‘ í™•ì¸
- ë¡œê·¸ì— ì‹œê°„, ë ˆë²¨, ë©”ì‹œì§€ ì •ìƒ ê¸°ë¡

---

## Phase 2: AI ë¶„ì„ (Week 2)

### Step 2.1: Gemini API ì—°ë™
**ëª©í‘œ:** 1ê°œ ê¸°ì‚¬ë¥¼ 3ì¤„ë¡œ ìš”ì•½
**ì†Œìš” ì‹œê°„:** 1ì‹œê°„

**ì„¸ë¶€ ì‘ì—…:**

1. `requirements.txt`ì— ì¶”ê°€
   ```
   google-generativeai==0.4.0
   ```

2. `.env`ì— API í‚¤ ì¶”ê°€
   ```
   GEMINI_API_KEY=your_api_key_here
   ```

3. `utils/config.py` ì‘ì„±
   ```python
   import os
   from dotenv import load_dotenv

   load_dotenv()

   class Config:
       GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
       GEMINI_MODEL = 'gemini-2.0-flash-exp'  # ë˜ëŠ” 'gemini-2.5-flash'

       # ìŠ¤í¬ë˜í•‘ ì„¤ì •
       MAX_ARTICLES_PER_SITE = 10
       SCRAPING_DELAY = 1  # ì´ˆ

       # ë°ì´í„° ê²½ë¡œ
       DATA_DIR = './data'
       RAW_DIR = './data/raw'
       PROCESSED_DIR = './data/processed'
   ```

4. `analyzers/gemini_analyzer.py` ì‘ì„±
   ```python
   import google.generativeai as genai
   from utils.config import Config
   from utils.logger import setup_logger
   from models.news_article import NewsArticle

   class GeminiAnalyzer:
       def __init__(self):
           genai.configure(api_key=Config.GEMINI_API_KEY)
           self.model = genai.GenerativeModel(Config.GEMINI_MODEL)
           self.logger = setup_logger(self.__class__.__name__)

       def summarize(self, article: NewsArticle, num_sentences: int = 3) -> str:
           """ê¸°ì‚¬ë¥¼ ì§€ì •ëœ ë¬¸ì¥ ìˆ˜ë¡œ ìš”ì•½"""
           prompt = f"""
           ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì •í™•íˆ {num_sentences}ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.
           í•µì‹¬ ë‚´ìš©ë§Œ ê°„ê²°í•˜ê²Œ ë‹´ì•„ì£¼ì„¸ìš”.

           ì œëª©: {article.title}
           ë³¸ë¬¸: {article.content}

           ìš”ì•½:
           """

           try:
               response = self.model.generate_content(prompt)
               summary = response.text.strip()
               self.logger.info(f"âœ… ìš”ì•½ ì™„ë£Œ: {article.title[:30]}")
               return summary
           except Exception as e:
               self.logger.error(f"âŒ ìš”ì•½ ì‹¤íŒ¨: {e}")
               raise
   ```

5. `test_gemini.py` ì‘ì„±
   ```python
   from utils.file_manager import FileManager
   from analyzers.gemini_analyzer import GeminiAnalyzer

   if __name__ == '__main__':
       # ì €ì¥ëœ ê¸°ì‚¬ ë¡œë“œ
       file_manager = FileManager()
       articles = file_manager.load_articles('cleaned_news_20250110_120000.json')  # ì‹¤ì œ íŒŒì¼ëª…

       # Gemini ë¶„ì„ê¸° ì´ˆê¸°í™”
       analyzer = GeminiAnalyzer()

       # ì²« ë²ˆì§¸ ê¸°ì‚¬ ìš”ì•½
       article = articles[0]
       print(f"ì›ë³¸ ì œëª©: {article.title}")
       print(f"ì›ë³¸ ë³¸ë¬¸ ê¸¸ì´: {len(article.content)}ì\n")

       summary = analyzer.summarize(article, num_sentences=3)
       print(f"ìš”ì•½:\n{summary}")

       # ê¸°ì‚¬ ê°ì²´ì— ìš”ì•½ ì¶”ê°€
       article.summary = summary

       # ì €ì¥
       file_manager.save_articles([article], 'summarized_news.json')
   ```

6. ì‹¤í–‰
   ```bash
   pip install google-generativeai
   python test_gemini.py
   ```

**ì„±ê³µ ê¸°ì¤€:**
- 3ì¤„ ìš”ì•½ë¬¸ ìƒì„±
- `data/raw/summarized_news.json`ì— summary í•„ë“œ í¬í•¨

---

### Step 2.2: ì‰¬ìš´ ì–¸ì–´ë¡œ ì¬ì‘ì„±
**ëª©í‘œ:** ì „ë¬¸ ìš©ì–´ë¥¼ ì¤‘í•™ìƒë„ ì´í•´í•  ìˆ˜ ìˆê²Œ ë³€í™˜
**ì†Œìš” ì‹œê°„:** 1.5ì‹œê°„

**ì„¸ë¶€ ì‘ì—…:**

1. `analyzers/gemini_analyzer.py`ì— ë©”ì„œë“œ ì¶”ê°€
   ```python
   def simplify_language(self, article: NewsArticle) -> str:
       """ì „ë¬¸ ìš©ì–´ë¥¼ ì‰¬ìš´ ì–¸ì–´ë¡œ ë³€í™˜"""
       prompt = f"""
       ë‹¤ìŒ ê²½ì œ ë‰´ìŠ¤ë¥¼ ì¤‘í•™ìƒë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì‰½ê²Œ ë‹¤ì‹œ ì‘ì„±í•´ì£¼ì„¸ìš”.

       ê·œì¹™:
       1. ì „ë¬¸ ìš©ì–´ëŠ” ê´„í˜¸ ì•ˆì— ì‰¬ìš´ ì„¤ëª… ì¶”ê°€
       2. ì˜ˆì‹œ: "ê¸°ì¤€ê¸ˆë¦¬(í•œêµ­ì€í–‰ì´ ì •í•˜ëŠ” ê¸°ë³¸ ì´ììœ¨)"
       3. ë¬¸ì¥ì€ ì§§ê³  ëª…í™•í•˜ê²Œ
       4. ë¹„ìœ ë‚˜ ì˜ˆì‹œë¥¼ í™œìš©

       ì›ë³¸:
       ì œëª©: {article.title}
       ë³¸ë¬¸: {article.content}

       ì‰¬ìš´ ì„¤ëª…:
       """

       try:
           response = self.model.generate_content(prompt)
           easy_text = response.text.strip()
           self.logger.info(f"âœ… ì‰¬ìš´ ì„¤ëª… ìƒì„±: {article.title[:30]}")
           return easy_text
       except Exception as e:
           self.logger.error(f"âŒ ì‰¬ìš´ ì„¤ëª… ìƒì„± ì‹¤íŒ¨: {e}")
           raise
   ```

2. `test_gemini.py` ì—…ë°ì´íŠ¸
   ```python
   # ìš”ì•½ í›„ì— ì¶”ê°€
   easy_explanation = analyzer.simplify_language(article)
   print(f"\nì‰¬ìš´ ì„¤ëª…:\n{easy_explanation}")

   article.easy_explanation = easy_explanation
   ```

**ì„±ê³µ ê¸°ì¤€:**
- ì „ë¬¸ ìš©ì–´ê°€ ê´„í˜¸ë¡œ ì„¤ëª…ë¨
- ì¤‘í•™ìƒ ìˆ˜ì¤€ì—ì„œ ì´í•´ ê°€ëŠ¥í•œ ë¬¸ì¥

---

### Step 2.3: ìš©ì–´ ìë™ ì¶”ì¶œ ë° í•´ì„¤
**ëª©í‘œ:** ì–´ë ¤ìš´ ë‹¨ì–´ë¥¼ ìë™ìœ¼ë¡œ ì°¾ì•„ì„œ ìš©ì–´ì§‘ ìƒì„±
**ì†Œìš” ì‹œê°„:** 1.5ì‹œê°„

**ì„¸ë¶€ ì‘ì—…:**

1. `analyzers/terminology.py` ì‘ì„±
   ```python
   import google.generativeai as genai
   from utils.config import Config
   from utils.logger import setup_logger
   import json

   class TerminologyExtractor:
       def __init__(self):
           genai.configure(api_key=Config.GEMINI_API_KEY)
           self.model = genai.GenerativeModel(Config.GEMINI_MODEL)
           self.logger = setup_logger(self.__class__.__name__)

       def extract_and_explain(self, text: str) -> dict[str, str]:
           """í…ìŠ¤íŠ¸ì—ì„œ ì „ë¬¸ ìš©ì–´ ì¶”ì¶œ ë° ì„¤ëª…"""
           prompt = f"""
           ë‹¤ìŒ ê²½ì œ ë‰´ìŠ¤ì—ì„œ ì¼ë°˜ì¸ì´ ì–´ë ¤ì›Œí•  ë§Œí•œ ì „ë¬¸ ìš©ì–´ë¥¼ ì°¾ì•„ì„œ
           JSON í˜•ì‹ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.

           í…ìŠ¤íŠ¸:
           {text}

           ì¶œë ¥ í˜•ì‹ (ë°˜ë“œì‹œ ìœ íš¨í•œ JSON):
           {{
               "ê¸°ì¤€ê¸ˆë¦¬": "í•œêµ­ì€í–‰ì´ ì •í•˜ëŠ” ê¸°ë³¸ ì´ììœ¨ë¡œ, ì€í–‰ë“¤ì´ ëŒ€ì¶œí•  ë•Œ ì°¸ê³ í•˜ëŠ” ê¸°ì¤€",
               "í™˜ìœ¨": "ë‹¤ë¥¸ ë‚˜ë¼ ëˆê³¼ ìš°ë¦¬ë‚˜ë¼ ëˆì„ ë°”ê¿€ ë•Œì˜ ë¹„ìœ¨"
           }}

           JSON:
           """

           try:
               response = self.model.generate_content(prompt)
               # JSON íŒŒì‹±
               json_text = response.text.strip()
               # Markdown ì½”ë“œ ë¸”ë¡ ì œê±°
               if json_text.startswith('```'):
                   json_text = json_text.split('```')[1]
                   if json_text.startswith('json'):
                       json_text = json_text[4:]

               terminology = json.loads(json_text)
               self.logger.info(f"âœ… {len(terminology)}ê°œ ìš©ì–´ ì¶”ì¶œ")
               return terminology
           except Exception as e:
               self.logger.error(f"âŒ ìš©ì–´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
               return {}
   ```

2. `test_gemini.py`ì— ì¶”ê°€
   ```python
   from analyzers.terminology import TerminologyExtractor

   # ... (ì´ì „ ì½”ë“œ)

   # ìš©ì–´ ì¶”ì¶œ
   term_extractor = TerminologyExtractor()
   terminology = term_extractor.extract_and_explain(article.content)

   print(f"\nìš©ì–´ í•´ì„¤:")
   for term, explanation in terminology.items():
       print(f"  â€¢ {term}: {explanation}")

   article.terminology = terminology
   ```

**ì„±ê³µ ê¸°ì¤€:**
- 3~5ê°œì˜ ì „ë¬¸ ìš©ì–´ ìë™ ì¶”ì¶œ
- ê° ìš©ì–´ì— ëŒ€í•œ ì‰¬ìš´ ì„¤ëª…
- JSON íŒŒì‹± ì˜¤ë¥˜ ì—†ìŒ

---

### Step 2.4: ê³¼ê±° ë§¥ë½ êµ¬ì„± (íƒ€ì„ë¼ì¸)
**ëª©í‘œ:** í˜„ì¬ ë‰´ìŠ¤ì™€ ê´€ë ¨ëœ ê³¼ê±° ë‰´ìŠ¤ë¥¼ ì°¾ì•„ ì‹œê³„ì—´ ë§¥ë½ ìƒì„±
**ì†Œìš” ì‹œê°„:** 2ì‹œê°„

**ì„¸ë¶€ ì‘ì—…:**

1. `database/db_manager.py` ì‘ì„± (SQLite)
   ```python
   import sqlite3
   from typing import List
   from models.news_article import NewsArticle
   from datetime import datetime
   import json

   class DatabaseManager:
       def __init__(self, db_path: str = './database/news.db'):
           self.db_path = db_path
           self._create_table()

       def _create_table(self):
           """ë‰´ìŠ¤ í…Œì´ë¸” ìƒì„±"""
           conn = sqlite3.connect(self.db_path)
           cursor = conn.cursor()
           cursor.execute('''
               CREATE TABLE IF NOT EXISTS articles (
                   id INTEGER PRIMARY KEY AUTOINCREMENT,
                   url TEXT UNIQUE,
                   title TEXT,
                   content TEXT,
                   published_at TEXT,
                   source TEXT,
                   keywords TEXT,
                   summary TEXT,
                   easy_explanation TEXT,
                   terminology TEXT,
                   created_at TEXT DEFAULT CURRENT_TIMESTAMP
               )
           ''')
           conn.commit()
           conn.close()

       def insert_article(self, article: NewsArticle):
           """ê¸°ì‚¬ ì‚½ì… (ì¤‘ë³µ ì‹œ ë¬´ì‹œ)"""
           conn = sqlite3.connect(self.db_path)
           cursor = conn.cursor()

           try:
               cursor.execute('''
                   INSERT OR IGNORE INTO articles
                   (url, title, content, published_at, source, keywords, summary, easy_explanation, terminology)
                   VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
               ''', (
                   article.url,
                   article.title,
                   article.content,
                   article.published_at.isoformat(),
                   article.source,
                   json.dumps(article.keywords) if article.keywords else None,
                   article.summary,
                   article.easy_explanation,
                   json.dumps(article.terminology) if article.terminology else None
               ))
               conn.commit()
           finally:
               conn.close()

       def search_by_keyword(self, keyword: str, limit: int = 10) -> List[NewsArticle]:
           """í‚¤ì›Œë“œë¡œ ê¸°ì‚¬ ê²€ìƒ‰"""
           conn = sqlite3.connect(self.db_path)
           cursor = conn.cursor()

           cursor.execute('''
               SELECT url, title, content, published_at, source, keywords, summary, easy_explanation, terminology
               FROM articles
               WHERE title LIKE ? OR content LIKE ?
               ORDER BY published_at DESC
               LIMIT ?
           ''', (f'%{keyword}%', f'%{keyword}%', limit))

           rows = cursor.fetchall()
           conn.close()

           articles = []
           for row in rows:
               article = NewsArticle(
                   url=row[0],
                   title=row[1],
                   content=row[2],
                   published_at=datetime.fromisoformat(row[3]),
                   source=row[4],
                   keywords=json.loads(row[5]) if row[5] else None,
                   summary=row[6],
                   easy_explanation=row[7],
                   terminology=json.loads(row[8]) if row[8] else None
               )
               articles.append(article)

           return articles
   ```

2. `analyzers/context_builder.py` ì‘ì„±
   ```python
   import google.generativeai as genai
   from utils.config import Config
   from utils.logger import setup_logger
   from database.db_manager import DatabaseManager
   from models.news_article import NewsArticle

   class ContextBuilder:
       def __init__(self):
           genai.configure(api_key=Config.GEMINI_API_KEY)
           self.model = genai.GenerativeModel(Config.GEMINI_MODEL)
           self.logger = setup_logger(self.__class__.__name__)
           self.db = DatabaseManager()

       def extract_keywords(self, article: NewsArticle) -> list[str]:
           """ê¸°ì‚¬ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
           prompt = f"""
           ë‹¤ìŒ ê²½ì œ ë‰´ìŠ¤ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ 3~5ê°œë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
           ì‰¼í‘œë¡œ êµ¬ë¶„í•˜ì—¬ ì¶œë ¥í•˜ì„¸ìš”.

           ì œëª©: {article.title}
           ë³¸ë¬¸: {article.content[:500]}

           í‚¤ì›Œë“œ:
           """

           try:
               response = self.model.generate_content(prompt)
               keywords = [k.strip() for k in response.text.strip().split(',')]
               self.logger.info(f"âœ… í‚¤ì›Œë“œ ì¶”ì¶œ: {keywords}")
               return keywords
           except Exception as e:
               self.logger.error(f"âŒ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
               return []

       def build_timeline(self, article: NewsArticle) -> str:
           """ê³¼ê±° ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ íƒ€ì„ë¼ì¸ ìƒì„±"""
           # 1. í‚¤ì›Œë“œ ì¶”ì¶œ
           keywords = self.extract_keywords(article)
           article.keywords = keywords

           # 2. ê³¼ê±° ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰
           related_articles = []
           for keyword in keywords:
               articles = self.db.search_by_keyword(keyword, limit=3)
               related_articles.extend(articles)

           # ì¤‘ë³µ ì œê±°
           seen_urls = {article.url}
           unique_related = []
           for rel_article in related_articles:
               if rel_article.url not in seen_urls:
                   seen_urls.add(rel_article.url)
                   unique_related.append(rel_article)

           if not unique_related:
               self.logger.info("ê´€ë ¨ ê³¼ê±° ê¸°ì‚¬ ì—†ìŒ")
               return "ì´ ë‰´ìŠ¤ëŠ” ìµœê·¼ì— ë“±ì¥í•œ ìƒˆë¡œìš´ ì´ìŠˆì…ë‹ˆë‹¤."

           # 3. íƒ€ì„ë¼ì¸ ìƒì„±
           related_text = "\n\n".join([
               f"[{rel.published_at.strftime('%Y-%m-%d')}] {rel.title}\n{rel.summary or rel.content[:200]}"
               for rel in unique_related[:5]
           ])

           prompt = f"""
           í˜„ì¬ ë‰´ìŠ¤ì™€ ê´€ë ¨ëœ ê³¼ê±° ê¸°ì‚¬ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì´ ì´ìŠˆê°€ ì–´ë–»ê²Œ ë°œì „í•´ì™”ëŠ”ì§€
           ì‹œê³„ì—´ íƒ€ì„ë¼ì¸ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.

           í˜„ì¬ ë‰´ìŠ¤:
           ì œëª©: {article.title}
           ë‚´ìš©: {article.content[:300]}

           ê³¼ê±° ê´€ë ¨ ê¸°ì‚¬:
           {related_text}

           íƒ€ì„ë¼ì¸ (3~5ë¬¸ì¥):
           """

           try:
               response = self.model.generate_content(prompt)
               timeline = response.text.strip()
               self.logger.info(f"âœ… íƒ€ì„ë¼ì¸ ìƒì„± ì™„ë£Œ")
               return timeline
           except Exception as e:
               self.logger.error(f"âŒ íƒ€ì„ë¼ì¸ ìƒì„± ì‹¤íŒ¨: {e}")
               return ""
   ```

3. `test_context.py` ì‘ì„±
   ```python
   from utils.file_manager import FileManager
   from analyzers.context_builder import ContextBuilder
   from database.db_manager import DatabaseManager

   if __name__ == '__main__':
       # ê³¼ê±° ê¸°ì‚¬ë“¤ì„ DBì— ì €ì¥ (ìµœì´ˆ 1íšŒ)
       file_manager = FileManager()
       db = DatabaseManager()

       # ì´ì „ì— ìˆ˜ì§‘í•œ ê¸°ì‚¬ë“¤ ë¡œë“œ
       articles = file_manager.load_articles('cleaned_news_20250110_120000.json')
       for article in articles:
           db.insert_article(article)

       print(f"âœ… {len(articles)}ê°œ ê¸°ì‚¬ DB ì €ì¥ ì™„ë£Œ\n")

       # ìƒˆ ê¸°ì‚¬ì˜ íƒ€ì„ë¼ì¸ ìƒì„±
       context_builder = ContextBuilder()
       new_article = articles[0]  # í…ŒìŠ¤íŠ¸ìš©

       timeline = context_builder.build_timeline(new_article)
       print(f"íƒ€ì„ë¼ì¸:\n{timeline}")
   ```

**ì„±ê³µ ê¸°ì¤€:**
- `database/news.db` íŒŒì¼ ìƒì„±
- ê³¼ê±° ê¸°ì‚¬ ê²€ìƒ‰ ë™ì‘
- íƒ€ì„ë¼ì¸ í…ìŠ¤íŠ¸ ìƒì„±

---

### Step 2.5: ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°
**ëª©í‘œ:** ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ì¤‘ ìƒìœ„ 5ê°œ ì„ ì •
**ì†Œìš” ì‹œê°„:** 1.5ì‹œê°„

**ì„¸ë¶€ ì‘ì—…:**

1. `analyzers/importance_ranker.py` ì‘ì„±
   ```python
   from typing import List
   from models.news_article import NewsArticle
   import re

   class ImportanceRanker:
       # ì¤‘ìš” í‚¤ì›Œë“œ (ê°€ì¤‘ì¹˜ ë¶€ì—¬)
       HIGH_PRIORITY_KEYWORDS = {
           'í•œêµ­ì€í–‰': 3,
           'ê¸°ì¤€ê¸ˆë¦¬': 3,
           'í™˜ìœ¨': 2,
           'ì£¼ê°€': 2,
           'ì½”ìŠ¤í”¼': 2,
           'GDP': 3,
           'ë¬´ì—­ìˆ˜ì§€': 2,
           'ì‹¤ì—…ë¥ ': 2,
           'ë¬¼ê°€': 2,
           'ì¸í”Œë ˆì´ì…˜': 3
       }

       @staticmethod
       def calculate_score(article: NewsArticle) -> float:
           """ê¸°ì‚¬ ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°"""
           score = 0.0

           # 1. í‚¤ì›Œë“œ ë§¤ì¹­ (ì œëª© 2ë°° ê°€ì¤‘ì¹˜)
           title_lower = article.title.lower()
           content_lower = article.content.lower()

           for keyword, weight in ImportanceRanker.HIGH_PRIORITY_KEYWORDS.items():
               if keyword.lower() in title_lower:
                   score += weight * 2
               elif keyword.lower() in content_lower:
                   score += weight

           # 2. ë³¸ë¬¸ ê¸¸ì´ (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸¸ë©´ ê°ì )
           content_len = len(article.content)
           if 500 <= content_len <= 3000:
               score += 1

           # 3. ìˆ«ì/ë°ì´í„° í¬í•¨ ì—¬ë¶€ (í†µê³„ ê¸°ì‚¬ ìš°ëŒ€)
           numbers = re.findall(r'\d+\.?\d*%?', article.content)
           if len(numbers) >= 3:
               score += 1

           return score

       @staticmethod
       def rank_articles(articles: List[NewsArticle], top_n: int = 5) -> List[NewsArticle]:
           """ê¸°ì‚¬ë“¤ì„ ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬ í›„ ìƒìœ„ Nê°œ ë°˜í™˜"""
           # ì ìˆ˜ ê³„ì‚°
           scored_articles = [(article, ImportanceRanker.calculate_score(article)) for article in articles]

           # ì •ë ¬
           scored_articles.sort(key=lambda x: x[1], reverse=True)

           # ìƒìœ„ Nê°œ ë°˜í™˜
           top_articles = [article for article, score in scored_articles[:top_n]]

           print(f"ì¤‘ìš”ë„ ë­í‚¹:")
           for i, (article, score) in enumerate(scored_articles[:top_n], 1):
               print(f"  {i}. [{score:.1f}ì ] {article.title[:40]}")

           return top_articles
   ```

2. `test_ranking.py` ì‘ì„±
   ```python
   from utils.file_manager import FileManager
   from analyzers.importance_ranker import ImportanceRanker

   if __name__ == '__main__':
       file_manager = FileManager()
       articles = file_manager.load_articles('cleaned_news_20250110_120000.json')

       print(f"ì´ {len(articles)}ê°œ ê¸°ì‚¬ ì¤‘ ìƒìœ„ 5ê°œ ì„ ì •:\n")
       top_articles = ImportanceRanker.rank_articles(articles, top_n=5)

       # ê²°ê³¼ ì €ì¥
       file_manager.save_articles(top_articles, 'top_news.json')
   ```

**ì„±ê³µ ê¸°ì¤€:**
- ì ìˆ˜ ê¸°ë°˜ ì •ë ¬ ë™ì‘
- ìƒìœ„ 5ê°œ ì„ ì •
- ì ìˆ˜ì™€ ì œëª© ì¶œë ¥

---

## Phase 3: ë°ì´í„° ì‹œê°í™” (Week 3)

### Step 3.1: ê¸°ë³¸ ê·¸ë˜í”„ ìƒì„± (ë”ë¯¸ ë°ì´í„°)
**ëª©í‘œ:** Matplotlibìœ¼ë¡œ ì„  ê·¸ë˜í”„ PNG ìƒì„±
**ì†Œìš” ì‹œê°„:** 45ë¶„

**ì„¸ë¶€ ì‘ì—…:**

1. `requirements.txt`ì— ì¶”ê°€
   ```
   matplotlib==3.8.3
   plotly==5.19.0
   ```

2. `visualizers/chart_generator.py` ì‘ì„±
   ```python
   import matplotlib.pyplot as plt
   import matplotlib.dates as mdates
   from datetime import datetime, timedelta
   import os

   class ChartGenerator:
       def __init__(self, output_dir: str = './data/charts'):
           self.output_dir = output_dir
           os.makedirs(output_dir, exist_ok=True)

           # í•œê¸€ í°íŠ¸ ì„¤ì • (Windows)
           plt.rcParams['font.family'] = 'Malgun Gothic'
           plt.rcParams['axes.unicode_minus'] = False

       def create_line_chart(
           self,
           dates: list[datetime],
           values: list[float],
           title: str,
           ylabel: str,
           filename: str
       ) -> str:
           """ì„  ê·¸ë˜í”„ ìƒì„±"""
           fig, ax = plt.subplots(figsize=(10, 6))

           # ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
           ax.plot(dates, values, marker='o', linewidth=2, markersize=6)

           # ì œëª© ë° ë¼ë²¨
           ax.set_title(title, fontsize=16, fontweight='bold')
           ax.set_ylabel(ylabel, fontsize=12)
           ax.set_xlabel('ë‚ ì§œ', fontsize=12)

           # ë‚ ì§œ í¬ë§·
           ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
           ax.xaxis.set_major_locator(mdates.DayLocator(interval=3))
           plt.xticks(rotation=45)

           # ê·¸ë¦¬ë“œ
           ax.grid(True, alpha=0.3)

           # ë ˆì´ì•„ì›ƒ ì¡°ì •
           plt.tight_layout()

           # ì €ì¥
           filepath = os.path.join(self.output_dir, filename)
           plt.savefig(filepath, dpi=300, bbox_inches='tight')
           plt.close()

           print(f"âœ… ê·¸ë˜í”„ ì €ì¥: {filepath}")
           return filepath
   ```

3. `test_chart.py` ì‘ì„±
   ```python
   from visualizers.chart_generator import ChartGenerator
   from datetime import datetime, timedelta

   if __name__ == '__main__':
       # ë”ë¯¸ ë°ì´í„° ìƒì„±
       start_date = datetime(2025, 1, 1)
       dates = [start_date + timedelta(days=i) for i in range(30)]
       values = [1300 + i * 2 + (i % 5) * 10 for i in range(30)]  # í™˜ìœ¨ ê°€ì •

       # ê·¸ë˜í”„ ìƒì„±
       chart_gen = ChartGenerator()
       chart_gen.create_line_chart(
           dates=dates,
           values=values,
           title='ìµœê·¼ 30ì¼ ì›/ë‹¬ëŸ¬ í™˜ìœ¨ ì¶”ì´',
           ylabel='í™˜ìœ¨ (ì›)',
           filename='test_exchange_rate.png'
       )

       print("ê·¸ë˜í”„ íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”: data/charts/test_exchange_rate.png")
   ```

4. ì‹¤í–‰
   ```bash
   pip install matplotlib plotly
   python test_chart.py
   ```

**ì„±ê³µ ê¸°ì¤€:**
- `data/charts/test_exchange_rate.png` ìƒì„±
- í•œê¸€ ì œëª© ì •ìƒ í‘œì‹œ
- ì„  ê·¸ë˜í”„ ì‹œê°ì ìœ¼ë¡œ ê¹”ë”

---

### Step 3.2: ì‹¤ì œ ê²½ì œ ë°ì´í„° ìˆ˜ì§‘ (í™˜ìœ¨)
**ëª©í‘œ:** í•œêµ­ì€í–‰ Open API ë˜ëŠ” Yahoo Financeì—ì„œ ì‹¤ì œ í™˜ìœ¨ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
**ì†Œìš” ì‹œê°„:** 2ì‹œê°„

**ì„¸ë¶€ ì‘ì—…:**

1. `requirements.txt`ì— ì¶”ê°€
   ```
   yfinance==0.2.36
   ```

2. `visualizers/data_fetcher.py` ì‘ì„±
   ```python
   import yfinance as yf
   from datetime import datetime, timedelta
   from typing import Tuple, List
   from utils.logger import setup_logger

   class EconomicDataFetcher:
       def __init__(self):
           self.logger = setup_logger(self.__class__.__name__)

       def get_exchange_rate(
           self,
           days: int = 30,
           currency_pair: str = 'KRW=X'  # USD/KRW
       ) -> Tuple[List[datetime], List[float]]:
           """í™˜ìœ¨ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
           try:
               end_date = datetime.now()
               start_date = end_date - timedelta(days=days)

               # Yahoo Financeì—ì„œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
               ticker = yf.Ticker(currency_pair)
               hist = ticker.history(start=start_date, end=end_date)

               # ë‚ ì§œì™€ ì¢…ê°€ ì¶”ì¶œ
               dates = [date.to_pydatetime() for date in hist.index]
               values = hist['Close'].tolist()

               self.logger.info(f"âœ… í™˜ìœ¨ ë°ì´í„° {len(dates)}ê°œ ìˆ˜ì§‘")
               return dates, values
           except Exception as e:
               self.logger.error(f"âŒ í™˜ìœ¨ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
               raise

       def get_kospi(self, days: int = 30) -> Tuple[List[datetime], List[float]]:
           """ì½”ìŠ¤í”¼ ì§€ìˆ˜ ë°ì´í„°"""
           try:
               end_date = datetime.now()
               start_date = end_date - timedelta(days=days)

               ticker = yf.Ticker('^KS11')  # KOSPI
               hist = ticker.history(start=start_date, end=end_date)

               dates = [date.to_pydatetime() for date in hist.index]
               values = hist['Close'].tolist()

               self.logger.info(f"âœ… ì½”ìŠ¤í”¼ ë°ì´í„° {len(dates)}ê°œ ìˆ˜ì§‘")
               return dates, values
           except Exception as e:
               self.logger.error(f"âŒ ì½”ìŠ¤í”¼ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
               raise
   ```

3. `test_real_chart.py` ì‘ì„±
   ```python
   from visualizers.chart_generator import ChartGenerator
   from visualizers.data_fetcher import EconomicDataFetcher

   if __name__ == '__main__':
       # ì‹¤ì œ ë°ì´í„° ìˆ˜ì§‘
       fetcher = EconomicDataFetcher()
       dates, values = fetcher.get_exchange_rate(days=30)

       # ê·¸ë˜í”„ ìƒì„±
       chart_gen = ChartGenerator()
       chart_gen.create_line_chart(
           dates=dates,
           values=values,
           title='ìµœê·¼ 30ì¼ ì›/ë‹¬ëŸ¬ í™˜ìœ¨ ì¶”ì´ (ì‹¤ì œ ë°ì´í„°)',
           ylabel='í™˜ìœ¨ (ì›)',
           filename='real_exchange_rate.png'
       )

       # ì½”ìŠ¤í”¼ë„ ìƒì„±
       kospi_dates, kospi_values = fetcher.get_kospi(days=30)
       chart_gen.create_line_chart(
           dates=kospi_dates,
           values=kospi_values,
           title='ìµœê·¼ 30ì¼ ì½”ìŠ¤í”¼ ì§€ìˆ˜',
           ylabel='ì§€ìˆ˜',
           filename='kospi_index.png'
       )
   ```

**ì„±ê³µ ê¸°ì¤€:**
- ì‹¤ì œ í™˜ìœ¨ ë°ì´í„°ë¡œ ê·¸ë˜í”„ ìƒì„±
- ì½”ìŠ¤í”¼ ì§€ìˆ˜ ê·¸ë˜í”„ ìƒì„±

---

### Step 3.3: ë‰´ìŠ¤ í‚¤ì›Œë“œ ê¸°ë°˜ ìë™ ê·¸ë˜í”„ ì„ íƒ
**ëª©í‘œ:** ë‰´ìŠ¤ ë‚´ìš©ì„ ë¶„ì„í•´ ì ì ˆí•œ ê·¸ë˜í”„ ìë™ ìƒì„±
**ì†Œìš” ì‹œê°„:** 2ì‹œê°„

**ì„¸ë¶€ ì‘ì—…:**

1. `visualizers/auto_visualizer.py` ì‘ì„±
   ```python
   from models.news_article import NewsArticle
   from visualizers.data_fetcher import EconomicDataFetcher
   from visualizers.chart_generator import ChartGenerator
   from typing import Optional
   from utils.logger import setup_logger

   class AutoVisualizer:
       # í‚¤ì›Œë“œë³„ ì‹œê°í™” ë§¤í•‘
       KEYWORD_TO_CHART = {
           'í™˜ìœ¨': ('exchange_rate', 'ì›/ë‹¬ëŸ¬ í™˜ìœ¨'),
           'ë‹¬ëŸ¬': ('exchange_rate', 'ì›/ë‹¬ëŸ¬ í™˜ìœ¨'),
           'ì½”ìŠ¤í”¼': ('kospi', 'ì½”ìŠ¤í”¼ ì§€ìˆ˜'),
           'ì£¼ê°€': ('kospi', 'ì½”ìŠ¤í”¼ ì§€ìˆ˜'),
           'ì¦ì‹œ': ('kospi', 'ì½”ìŠ¤í”¼ ì§€ìˆ˜'),
       }

       def __init__(self):
           self.fetcher = EconomicDataFetcher()
           self.chart_gen = ChartGenerator()
           self.logger = setup_logger(self.__class__.__name__)

       def detect_chart_type(self, article: NewsArticle) -> Optional[tuple[str, str]]:
           """ê¸°ì‚¬ ë‚´ìš©ì„ ë¶„ì„í•´ ì ì ˆí•œ ì°¨íŠ¸ íƒ€ì… ê°ì§€"""
           text = (article.title + ' ' + article.content).lower()

           for keyword, (chart_type, title) in self.KEYWORD_TO_CHART.items():
               if keyword in text:
                   self.logger.info(f"í‚¤ì›Œë“œ '{keyword}' ê°ì§€ â†’ {chart_type} ì°¨íŠ¸")
                   return chart_type, title

           return None

       def generate_chart_for_article(self, article: NewsArticle) -> Optional[str]:
           """ê¸°ì‚¬ì— ì í•©í•œ ì°¨íŠ¸ ìë™ ìƒì„±"""
           chart_info = self.detect_chart_type(article)

           if not chart_info:
               self.logger.info("ì í•©í•œ ì°¨íŠ¸ ì—†ìŒ")
               return None

           chart_type, title = chart_info

           try:
               if chart_type == 'exchange_rate':
                   dates, values = self.fetcher.get_exchange_rate(days=30)
                   ylabel = 'í™˜ìœ¨ (ì›)'
               elif chart_type == 'kospi':
                   dates, values = self.fetcher.get_kospi(days=30)
                   ylabel = 'ì§€ìˆ˜'
               else:
                   return None

               # íŒŒì¼ëª… ìƒì„± (ê¸°ì‚¬ ì œëª© ê¸°ë°˜)
               safe_title = "".join(c for c in article.title[:30] if c.isalnum() or c in (' ', '_')).rstrip()
               filename = f"{safe_title}_{chart_type}.png"

               # ì°¨íŠ¸ ìƒì„±
               filepath = self.chart_gen.create_line_chart(
                   dates=dates,
                   values=values,
                   title=title,
                   ylabel=ylabel,
                   filename=filename
               )

               return filepath
           except Exception as e:
               self.logger.error(f"ì°¨íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
               return None
   ```

2. `test_auto_visualizer.py` ì‘ì„±
   ```python
   from utils.file_manager import FileManager
   from visualizers.auto_visualizer import AutoVisualizer

   if __name__ == '__main__':
       file_manager = FileManager()
       articles = file_manager.load_articles('top_news.json')

       auto_viz = AutoVisualizer()

       for article in articles:
           print(f"\nê¸°ì‚¬: {article.title}")
           chart_path = auto_viz.generate_chart_for_article(article)
           if chart_path:
               print(f"  âœ… ì°¨íŠ¸ ìƒì„±: {chart_path}")
           else:
               print(f"  âš ï¸  ì í•©í•œ ì°¨íŠ¸ ì—†ìŒ")
   ```

**ì„±ê³µ ê¸°ì¤€:**
- í™˜ìœ¨ ê´€ë ¨ ë‰´ìŠ¤ â†’ í™˜ìœ¨ ê·¸ë˜í”„ ìë™ ìƒì„±
- ì£¼ê°€ ê´€ë ¨ ë‰´ìŠ¤ â†’ ì½”ìŠ¤í”¼ ê·¸ë˜í”„ ìë™ ìƒì„±
- ë§¤ì¹­ ì•ˆ ë˜ë©´ None ë°˜í™˜

---

## Phase 4: ì›¹ í˜ì´ì§€ ìƒì„± (Week 3-4)

### Step 4.1: HTML í…œí”Œë¦¿ ë””ìì¸
**ëª©í‘œ:** ë³´ê¸° ì¢‹ì€ HTML í…œí”Œë¦¿ ì‘ì„±
**ì†Œìš” ì‹œê°„:** 2ì‹œê°„

**ì„¸ë¶€ ì‘ì—…:**

1. `templates/base.html` ì‘ì„±
   ```html
   <!DOCTYPE html>
   <html lang="ko">
   <head>
       <meta charset="UTF-8">
       <meta name="viewport" content="width=device-width, initial-scale=1.0">
       <title>{{ title }} | Spread Insight</title>
       <style>
           * {
               margin: 0;
               padding: 0;
               box-sizing: border-box;
           }

           body {
               font-family: 'Malgun Gothic', -apple-system, BlinkMacSystemFont, sans-serif;
               line-height: 1.6;
               color: #333;
               background: #f5f5f5;
           }

           .container {
               max-width: 800px;
               margin: 0 auto;
               padding: 20px;
               background: white;
               box-shadow: 0 2px 10px rgba(0,0,0,0.1);
           }

           header {
               border-bottom: 3px solid #007bff;
               margin-bottom: 30px;
               padding-bottom: 20px;
           }

           h1 {
               font-size: 2em;
               color: #222;
               margin-bottom: 10px;
           }

           .meta {
               color: #666;
               font-size: 0.9em;
           }

           .summary {
               background: #e3f2fd;
               padding: 20px;
               border-left: 4px solid #007bff;
               margin: 30px 0;
               font-size: 1.1em;
           }

           .content {
               font-size: 1.05em;
               line-height: 1.8;
               margin: 30px 0;
           }

           .terminology {
               background: #fff3cd;
               padding: 20px;
               margin: 30px 0;
               border-radius: 5px;
           }

           .terminology h2 {
               font-size: 1.3em;
               margin-bottom: 15px;
               color: #856404;
           }

           .term-item {
               margin-bottom: 10px;
           }

           .term-word {
               font-weight: bold;
               color: #856404;
           }

           .timeline {
               background: #f8f9fa;
               padding: 20px;
               margin: 30px 0;
               border-left: 4px solid #28a745;
           }

           .timeline h2 {
               font-size: 1.3em;
               margin-bottom: 15px;
               color: #155724;
           }

           .chart {
               margin: 30px 0;
               text-align: center;
           }

           .chart img {
               max-width: 100%;
               height: auto;
               border: 1px solid #ddd;
           }

           .coupang-link {
               display: block;
               background: #ff6f61;
               color: white;
               text-align: center;
               padding: 15px;
               text-decoration: none;
               border-radius: 5px;
               font-size: 1.1em;
               font-weight: bold;
               margin: 30px 0;
           }

           .coupang-link:hover {
               background: #e55a50;
           }

           footer {
               margin-top: 50px;
               padding-top: 20px;
               border-top: 1px solid #ddd;
               text-align: center;
               color: #666;
               font-size: 0.9em;
           }
       </style>
   </head>
   <body>
       <div class="container">
           <header>
               <h1>{{ title }}</h1>
               <div class="meta">
                   {{ source }} | {{ date }}
               </div>
           </header>

           {% if summary %}
           <div class="summary">
               <strong>ğŸ“Œ í•œì¤„ ìš”ì•½</strong><br>
               {{ summary }}
           </div>
           {% endif %}

           {% if chart_path %}
           <div class="chart">
               <img src="{{ chart_path }}" alt="ê´€ë ¨ ì°¨íŠ¸">
           </div>
           {% endif %}

           <div class="content">
               <h2>ì‰½ê²Œ í’€ì–´ë³´ê¸°</h2>
               {{ easy_explanation }}
           </div>

           {% if timeline %}
           <div class="timeline">
               <h2>ğŸ“… ì´ìŠˆ íƒ€ì„ë¼ì¸</h2>
               {{ timeline }}
           </div>
           {% endif %}

           {% if terminology %}
           <div class="terminology">
               <h2>ğŸ’¡ ìš©ì–´ í•´ì„¤</h2>
               {% for term, explanation in terminology.items() %}
               <div class="term-item">
                   <span class="term-word">{{ term }}</span>: {{ explanation }}
               </div>
               {% endfor %}
           </div>
           {% endif %}

           {% if coupang_link %}
           <a href="{{ coupang_link }}" target="_blank" class="coupang-link">
               ğŸ›’ ê´€ë ¨ ì¶”ì²œ ìƒí’ˆ ë³´ëŸ¬ê°€ê¸°
           </a>
           {% endif %}

           <footer>
               <p>ì´ í¬ìŠ¤íŒ…ì€ ì¿ íŒ¡ íŒŒíŠ¸ë„ˆìŠ¤ í™œë™ì˜ ì¼í™˜ìœ¼ë¡œ, ì´ì— ë”°ë¥¸ ì¼ì •ì•¡ì˜ ìˆ˜ìˆ˜ë£Œë¥¼ ì œê³µë°›ìŠµë‹ˆë‹¤.</p>
               <p>&copy; 2025 Spread Insight. All rights reserved.</p>
           </footer>
       </div>
   </body>
   </html>
   ```

2. `publishers/html_generator.py` ì‘ì„±
   ```python
   from jinja2 import Template
   from models.news_article import NewsArticle
   from typing import Optional
   import os
   from utils.logger import setup_logger

   class HTMLGenerator:
       def __init__(self, template_path: str = './templates/base.html', output_dir: str = './data/html'):
           self.template_path = template_path
           self.output_dir = output_dir
           os.makedirs(output_dir, exist_ok=True)
           self.logger = setup_logger(self.__class__.__name__)

           # í…œí”Œë¦¿ ë¡œë“œ
           with open(template_path, 'r', encoding='utf-8') as f:
               self.template = Template(f.read())

       def generate(
           self,
           article: NewsArticle,
           chart_path: Optional[str] = None,
           timeline: Optional[str] = None,
           coupang_link: Optional[str] = None
       ) -> str:
           """HTML íŒŒì¼ ìƒì„±"""
           # íŒŒì¼ëª… ìƒì„±
           safe_title = "".join(c for c in article.title[:50] if c.isalnum() or c in (' ', '_')).rstrip()
           filename = f"{safe_title}.html".replace(' ', '_')
           filepath = os.path.join(self.output_dir, filename)

           # HTML ë Œë”ë§
           html_content = self.template.render(
               title=article.title,
               source=article.source,
               date=article.published_at.strftime('%Yë…„ %mì›” %dì¼'),
               summary=article.summary,
               easy_explanation=article.easy_explanation,
               timeline=timeline,
               terminology=article.terminology,
               chart_path=os.path.basename(chart_path) if chart_path else None,
               coupang_link=coupang_link
           )

           # íŒŒì¼ ì €ì¥
           with open(filepath, 'w', encoding='utf-8') as f:
               f.write(html_content)

           self.logger.info(f"âœ… HTML ìƒì„±: {filepath}")
           return filepath
   ```

3. `requirements.txt`ì— ì¶”ê°€
   ```
   jinja2==3.1.3
   ```

4. `test_html.py` ì‘ì„±
   ```python
   from utils.file_manager import FileManager
   from publishers.html_generator import HTMLGenerator

   if __name__ == '__main__':
       file_manager = FileManager()
       articles = file_manager.load_articles('top_news.json')

       html_gen = HTMLGenerator()

       # ì²« ë²ˆì§¸ ê¸°ì‚¬ HTML ìƒì„±
       article = articles[0]
       html_path = html_gen.generate(
           article=article,
           chart_path='./data/charts/test_exchange_rate.png',
           timeline='ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ íƒ€ì„ë¼ì¸ì…ë‹ˆë‹¤.',
           coupang_link='https://www.coupang.com/np/search?q=test'
       )

       print(f"HTML íŒŒì¼ì„ ë¸Œë¼ìš°ì €ë¡œ ì—´ì–´ë³´ì„¸ìš”: {html_path}")
   ```

**ì„±ê³µ ê¸°ì¤€:**
- HTML íŒŒì¼ ìƒì„±
- ë¸Œë¼ìš°ì €ì—ì„œ ë³´ê¸° ì¢‹ê²Œ í‘œì‹œ
- í•œê¸€ ê¹¨ì§ ì—†ìŒ

---

### Step 4.2: GitHub Pages ë°°í¬
**ëª©í‘œ:** ìƒì„±ëœ HTMLì„ ì¸í„°ë„·ì— ê³µê°œ
**ì†Œìš” ì‹œê°„:** 1.5ì‹œê°„

**ì„¸ë¶€ ì‘ì—…:**

1. GitHub ì €ì¥ì†Œ ìƒì„±
   - `spread-insight-pages` ì´ë¦„ìœ¼ë¡œ public ì €ì¥ì†Œ ìƒì„±

2. `publishers/github_deployer.py` ì‘ì„±
   ```python
   import os
   import subprocess
   from utils.logger import setup_logger
   from typing import List

   class GitHubDeployer:
       def __init__(self, repo_dir: str = './github_pages'):
           self.repo_dir = repo_dir
           self.logger = setup_logger(self.__class__.__name__)

       def setup_repo(self, repo_url: str):
           """ì €ì¥ì†Œ ì´ˆê¸° ì„¤ì • (ìµœì´ˆ 1íšŒ)"""
           if not os.path.exists(self.repo_dir):
               # í´ë¡ 
               subprocess.run(['git', 'clone', repo_url, self.repo_dir], check=True)
               self.logger.info(f"âœ… ì €ì¥ì†Œ í´ë¡  ì™„ë£Œ")
           else:
               # Pull
               subprocess.run(['git', '-C', self.repo_dir, 'pull'], check=True)
               self.logger.info(f"âœ… ì €ì¥ì†Œ ì—…ë°ì´íŠ¸ ì™„ë£Œ")

       def deploy(self, html_files: List[str], chart_files: List[str]):
           """HTML ë° ì°¨íŠ¸ íŒŒì¼ ë°°í¬"""
           # HTML íŒŒì¼ ë³µì‚¬
           for html_file in html_files:
               dest = os.path.join(self.repo_dir, os.path.basename(html_file))
               subprocess.run(['cp', html_file, dest], check=True)
               self.logger.info(f"âœ… ë³µì‚¬: {html_file}")

           # ì°¨íŠ¸ íŒŒì¼ ë³µì‚¬
           charts_dir = os.path.join(self.repo_dir, 'charts')
           os.makedirs(charts_dir, exist_ok=True)
           for chart_file in chart_files:
               dest = os.path.join(charts_dir, os.path.basename(chart_file))
               subprocess.run(['cp', chart_file, dest], check=True)

           # Git ì»¤ë°‹ ë° í‘¸ì‹œ
           subprocess.run(['git', '-C', self.repo_dir, 'add', '.'], check=True)
           subprocess.run(['git', '-C', self.repo_dir, 'commit', '-m', 'Update news'], check=True)
           subprocess.run(['git', '-C', self.repo_dir, 'push'], check=True)

           self.logger.info(f"âœ… GitHub Pages ë°°í¬ ì™„ë£Œ")
   ```

3. `test_deploy.py` ì‘ì„±
   ```python
   from publishers.github_deployer import GitHubDeployer

   if __name__ == '__main__':
       deployer = GitHubDeployer()

       # ìµœì´ˆ 1íšŒ ì‹¤í–‰ (ì €ì¥ì†Œ URL ì…ë ¥)
       repo_url = 'https://github.com/YOUR_USERNAME/spread-insight-pages.git'
       deployer.setup_repo(repo_url)

       # ë°°í¬
       html_files = ['./data/html/test_article.html']
       chart_files = ['./data/charts/test_exchange_rate.png']
       deployer.deploy(html_files, chart_files)

       print("\në°°í¬ ì™„ë£Œ! ë‹¤ìŒ URLì—ì„œ í™•ì¸í•˜ì„¸ìš”:")
       print("https://YOUR_USERNAME.github.io/spread-insight-pages/test_article.html")
   ```

**ì„±ê³µ ê¸°ì¤€:**
- GitHub Pagesì—ì„œ HTML ì ‘ì† ê°€ëŠ¥
- ì´ë¯¸ì§€ ì •ìƒ í‘œì‹œ

---

## Phase 5: í…”ë ˆê·¸ë¨ ì—°ë™ (Week 4)

### Step 5.1: í…”ë ˆê·¸ë¨ ë´‡ ìƒì„± ë° ë©”ì‹œì§€ ì „ì†¡
**ëª©í‘œ:** "Hello World" ë©”ì‹œì§€ ì „ì†¡
**ì†Œìš” ì‹œê°„:** 1ì‹œê°„

**ì„¸ë¶€ ì‘ì—…:**

1. í…”ë ˆê·¸ë¨ ë´‡ ìƒì„±
   - í…”ë ˆê·¸ë¨ì—ì„œ @BotFather ê²€ìƒ‰
   - `/newbot` ëª…ë ¹ì–´
   - ë´‡ ì´ë¦„ ì…ë ¥
   - í† í° ë³µì‚¬ â†’ `.env`ì— ì €ì¥

2. `.env`ì— ì¶”ê°€
   ```
   TELEGRAM_BOT_TOKEN=your_bot_token_here
   TELEGRAM_CHAT_ID=your_chat_id_here
   ```

3. `publishers/telegram_bot.py` ì‘ì„±
   ```python
   from telegram import Bot, InlineKeyboardButton, InlineKeyboardMarkup
   from utils.config import Config
   from utils.logger import setup_logger
   import asyncio

   class TelegramPublisher:
       def __init__(self):
           self.bot = Bot(token=Config.TELEGRAM_BOT_TOKEN)
           self.logger = setup_logger(self.__class__.__name__)

       async def send_message(self, chat_id: str, text: str):
           """ë‹¨ìˆœ ë©”ì‹œì§€ ì „ì†¡"""
           try:
               await self.bot.send_message(chat_id=chat_id, text=text)
               self.logger.info(f"âœ… ë©”ì‹œì§€ ì „ì†¡ ì™„ë£Œ")
           except Exception as e:
               self.logger.error(f"âŒ ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨: {e}")

       async def send_news(
           self,
           chat_id: str,
           title: str,
           summary: str,
           detail_url: str,
           coupang_url: str
       ):
           """ë‰´ìŠ¤ ë©”ì‹œì§€ (ë²„íŠ¼ í¬í•¨) ì „ì†¡"""
           message = f"ğŸ“° <b>{title}</b>\n\n{summary}"

           # ë²„íŠ¼ ìƒì„±
           keyboard = [
               [InlineKeyboardButton("ğŸ“„ ìƒì„¸ ì¸ì‚¬ì´íŠ¸ ë³´ê¸°", url=detail_url)],
               [InlineKeyboardButton("ğŸ›’ ê´€ë ¨ ìƒí’ˆ ë³´ê¸°", url=coupang_url)]
           ]
           reply_markup = InlineKeyboardMarkup(keyboard)

           try:
               await self.bot.send_message(
                   chat_id=chat_id,
                   text=message,
                   parse_mode='HTML',
                   reply_markup=reply_markup
               )
               self.logger.info(f"âœ… ë‰´ìŠ¤ ì „ì†¡ ì™„ë£Œ: {title}")
           except Exception as e:
               self.logger.error(f"âŒ ë‰´ìŠ¤ ì „ì†¡ ì‹¤íŒ¨: {e}")
   ```

4. `test_telegram.py` ì‘ì„±
   ```python
   from publishers.telegram_bot import TelegramPublisher
   from utils.config import Config
   import asyncio

   async def main():
       bot = TelegramPublisher()

       # ë‹¨ìˆœ ë©”ì‹œì§€ í…ŒìŠ¤íŠ¸
       await bot.send_message(
           chat_id=Config.TELEGRAM_CHAT_ID,
           text="Hello World from Spread Insight!"
       )

       # ë‰´ìŠ¤ ë©”ì‹œì§€ í…ŒìŠ¤íŠ¸
       await bot.send_news(
           chat_id=Config.TELEGRAM_CHAT_ID,
           title="í™˜ìœ¨ ê¸‰ë“±, ë‹¬ëŸ¬ë‹¹ 1400ì› ëŒíŒŒ",
           summary="ìµœê·¼ ë¯¸êµ­ ê¸ˆë¦¬ ì¸ìƒìœ¼ë¡œ ì›/ë‹¬ëŸ¬ í™˜ìœ¨ì´ 1400ì›ì„ ë„˜ì–´ì„°ìŠµë‹ˆë‹¤.",
           detail_url="https://YOUR_USERNAME.github.io/spread-insight-pages/test.html",
           coupang_url="https://www.coupang.com/np/search?q=í™˜ì „"
       )

   if __name__ == '__main__':
       asyncio.run(main())
   ```

5. ì‹¤í–‰
   ```bash
   pip install python-telegram-bot
   python test_telegram.py
   ```

**ì„±ê³µ ê¸°ì¤€:**
- í…”ë ˆê·¸ë¨ì— ë©”ì‹œì§€ ë„ì°©
- ë²„íŠ¼ í´ë¦­ ì‹œ URL ì´ë™

---

### Step 5.2: êµ¬ë…ì ê´€ë¦¬
**ëª©í‘œ:** ì—¬ëŸ¬ ì‚¬ìš©ìì—ê²Œ ì¼ê´„ ì „ì†¡
**ì†Œìš” ì‹œê°„:** 1ì‹œê°„

**ì„¸ë¶€ ì‘ì—…:**

1. `database/subscriber_manager.py` ì‘ì„±
   ```python
   import sqlite3
   from typing import List

   class SubscriberManager:
       def __init__(self, db_path: str = './database/news.db'):
           self.db_path = db_path
           self._create_table()

       def _create_table(self):
           """êµ¬ë…ì í…Œì´ë¸” ìƒì„±"""
           conn = sqlite3.connect(self.db_path)
           cursor = conn.cursor()
           cursor.execute('''
               CREATE TABLE IF NOT EXISTS subscribers (
                   chat_id TEXT PRIMARY KEY,
                   username TEXT,
                   subscribed_at TEXT DEFAULT CURRENT_TIMESTAMP,
                   active INTEGER DEFAULT 1
               )
           ''')
           conn.commit()
           conn.close()

       def add_subscriber(self, chat_id: str, username: str = None):
           """êµ¬ë…ì ì¶”ê°€"""
           conn = sqlite3.connect(self.db_path)
           cursor = conn.cursor()
           cursor.execute('''
               INSERT OR IGNORE INTO subscribers (chat_id, username)
               VALUES (?, ?)
           ''', (chat_id, username))
           conn.commit()
           conn.close()

       def get_active_subscribers(self) -> List[str]:
           """í™œì„± êµ¬ë…ì ëª©ë¡"""
           conn = sqlite3.connect(self.db_path)
           cursor = conn.cursor()
           cursor.execute('SELECT chat_id FROM subscribers WHERE active = 1')
           rows = cursor.fetchall()
           conn.close()
           return [row[0] for row in rows]
   ```

2. `publishers/telegram_bot.py`ì— ì¶”ê°€
   ```python
   async def broadcast_news(
       self,
       subscribers: List[str],
       title: str,
       summary: str,
       detail_url: str,
       coupang_url: str
   ):
       """ì—¬ëŸ¬ êµ¬ë…ìì—ê²Œ ë‰´ìŠ¤ ì „ì†¡"""
       for chat_id in subscribers:
           await self.send_news(chat_id, title, summary, detail_url, coupang_url)
           await asyncio.sleep(0.5)  # ìŠ¤íŒ¸ ë°©ì§€ ë”œë ˆì´

       self.logger.info(f"âœ… {len(subscribers)}ëª…ì—ê²Œ ì „ì†¡ ì™„ë£Œ")
   ```

**ì„±ê³µ ê¸°ì¤€:**
- êµ¬ë…ì DB ê´€ë¦¬
- ì—¬ëŸ¬ ì‚¬ìš©ìì—ê²Œ ë™ì‹œ ì „ì†¡

---

## Phase 6: ì¿ íŒ¡ íŒŒíŠ¸ë„ˆìŠ¤ í†µí•© (Week 4)

### Step 6.1: ì¿ íŒ¡ ë§í¬ ìƒì„±
**ëª©í‘œ:** ë‰´ìŠ¤ í‚¤ì›Œë“œ ê¸°ë°˜ ì¿ íŒ¡ ìƒí’ˆ ë§í¬ ìƒì„±
**ì†Œìš” ì‹œê°„:** 2ì‹œê°„

**ì„¸ë¶€ ì‘ì—…:**

1. `publishers/coupang_partner.py` ì‘ì„±
   ```python
   from typing import Optional
   from utils.logger import setup_logger

   class CoupangPartner:
       # í‚¤ì›Œë“œ â†’ ìƒí’ˆ ê²€ìƒ‰ì–´ ë§¤í•‘
       KEYWORD_TO_PRODUCT = {
           'í™˜ìœ¨': 'ì—¬í–‰ ê°€ë°©',
           'ë‹¬ëŸ¬': 'í™˜ì „',
           'ì£¼ê°€': 'ì¬í…Œí¬ ì±…',
           'ë¶€ë™ì‚°': 'ì¸í…Œë¦¬ì–´',
           'ê¸ˆë¦¬': 'ì ê¸ˆ í†µì¥',
       }

       def __init__(self, partner_id: str = 'YOUR_PARTNER_ID'):
           self.partner_id = partner_id
           self.base_url = 'https://www.coupang.com/np/search'
           self.logger = setup_logger(self.__class__.__name__)

       def generate_link(self, keywords: list[str]) -> Optional[str]:
           """í‚¤ì›Œë“œ ê¸°ë°˜ ì¿ íŒ¡ ë§í¬ ìƒì„±"""
           # í‚¤ì›Œë“œ ë§¤ì¹­
           for keyword in keywords:
               if keyword in self.KEYWORD_TO_PRODUCT:
                   product_query = self.KEYWORD_TO_PRODUCT[keyword]
                   link = f"{self.base_url}?q={product_query}&channel=user"
                   self.logger.info(f"âœ… ì¿ íŒ¡ ë§í¬ ìƒì„±: {keyword} â†’ {product_query}")
                   return link

           # ê¸°ë³¸ ë§í¬ (ë§¤ì¹­ ì•ˆ ë  ê²½ìš°)
           default_link = f"{self.base_url}?q=ìƒí™œìš©í’ˆ&channel=user"
           return default_link
   ```

2. `test_coupang.py` ì‘ì„±
   ```python
   from publishers.coupang_partner import CoupangPartner

   if __name__ == '__main__':
       coupang = CoupangPartner()

       test_keywords = ['í™˜ìœ¨', 'ê¸ˆë¦¬', 'ë¶€ë™ì‚°']

       for keywords in [test_keywords[:1], test_keywords[1:2], ['ì•Œìˆ˜ì—†ìŒ']]:
           link = coupang.generate_link(keywords)
           print(f"í‚¤ì›Œë“œ: {keywords} â†’ ë§í¬: {link}")
   ```

**ì„±ê³µ ê¸°ì¤€:**
- í‚¤ì›Œë“œë³„ ë‹¤ë¥¸ ìƒí’ˆ ë§í¬ ìƒì„±
- ë§¤ì¹­ ì•ˆ ë  ê²½ìš° ê¸°ë³¸ ë§í¬

---

## Phase 7: ì „ì²´ í†µí•© ë° ìë™í™” (Week 5)

### Step 7.1: ë©”ì¸ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
**ëª©í‘œ:** ì „ì²´ ì›Œí¬í”Œë¡œìš°ë¥¼ 1ë²ˆì— ì‹¤í–‰
**ì†Œìš” ì‹œê°„:** 3ì‹œê°„

**ì„¸ë¶€ ì‘ì—…:**

1. `main.py` ì‘ì„±
   ```python
   from scrapers.naver_scraper import NaverScraper
   from scrapers.daum_scraper import DaumScraper
   from analyzers.gemini_analyzer import GeminiAnalyzer
   from analyzers.context_builder import ContextBuilder
   from analyzers.importance_ranker import ImportanceRanker
   from visualizers.auto_visualizer import AutoVisualizer
   from publishers.html_generator import HTMLGenerator
   from publishers.telegram_bot import TelegramPublisher
   from publishers.coupang_partner import CoupangPartner
   from publishers.github_deployer import GitHubDeployer
   from database.db_manager import DatabaseManager
   from database.subscriber_manager import SubscriberManager
   from utils.data_cleaner import DataCleaner
   from utils.logger import setup_logger
   import asyncio

   class SpreadInsight:
       def __init__(self):
           self.logger = setup_logger('SpreadInsight')

           # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
           self.scrapers = [NaverScraper(), DaumScraper()]
           self.analyzer = GeminiAnalyzer()
           self.context_builder = ContextBuilder()
           self.visualizer = AutoVisualizer()
           self.html_gen = HTMLGenerator()
           self.telegram = TelegramPublisher()
           self.coupang = CoupangPartner()
           self.deployer = GitHubDeployer()
           self.db = DatabaseManager()
           self.sub_manager = SubscriberManager()

       async def run(self):
           """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
           self.logger.info("=== Spread Insight ì‹œì‘ ===")

           # 1. ë‰´ìŠ¤ ìˆ˜ì§‘
           self.logger.info("1ë‹¨ê³„: ë‰´ìŠ¤ ìˆ˜ì§‘")
           all_articles = []
           for scraper in self.scrapers:
               urls = scraper.get_article_list(limit=5)
               for url in urls:
                   try:
                       article = scraper.scrape_with_retry(url)
                       all_articles.append(article)
                   except Exception as e:
                       self.logger.error(f"ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")

           # 2. ë°ì´í„° ì •ì œ
           self.logger.info("2ë‹¨ê³„: ë°ì´í„° ì •ì œ")
           articles = DataCleaner.clean(all_articles)

           # 3. ì¤‘ìš”ë„ ë­í‚¹
           self.logger.info("3ë‹¨ê³„: ì¤‘ìš”ë„ ë­í‚¹")
           top_articles = ImportanceRanker.rank_articles(articles, top_n=5)

           # 4. AI ë¶„ì„
           self.logger.info("4ë‹¨ê³„: AI ë¶„ì„")
           for article in top_articles:
               # ìš”ì•½
               article.summary = self.analyzer.summarize(article)

               # ì‰¬ìš´ ì„¤ëª…
               article.easy_explanation = self.analyzer.simplify_language(article)

               # ìš©ì–´ í•´ì„¤
               from analyzers.terminology import TerminologyExtractor
               term_extractor = TerminologyExtractor()
               article.terminology = term_extractor.extract_and_explain(article.content)

               # DB ì €ì¥
               self.db.insert_article(article)

           # 5. ì‹œê°í™” ë° HTML ìƒì„±
           self.logger.info("5ë‹¨ê³„: ì‹œê°í™” ë° HTML ìƒì„±")
           html_files = []
           chart_files = []

           for article in top_articles:
               # íƒ€ì„ë¼ì¸
               timeline = self.context_builder.build_timeline(article)

               # ì°¨íŠ¸
               chart_path = self.visualizer.generate_chart_for_article(article)
               if chart_path:
                   chart_files.append(chart_path)

               # ì¿ íŒ¡ ë§í¬
               coupang_link = self.coupang.generate_link(article.keywords or [])

               # HTML ìƒì„±
               html_path = self.html_gen.generate(
                   article=article,
                   chart_path=chart_path,
                   timeline=timeline,
                   coupang_link=coupang_link
               )
               html_files.append(html_path)

           # 6. GitHub Pages ë°°í¬
           self.logger.info("6ë‹¨ê³„: GitHub Pages ë°°í¬")
           self.deployer.deploy(html_files, chart_files)

           # 7. í…”ë ˆê·¸ë¨ ì „ì†¡
           self.logger.info("7ë‹¨ê³„: í…”ë ˆê·¸ë¨ ì „ì†¡")
           subscribers = self.sub_manager.get_active_subscribers()

           for i, article in enumerate(top_articles):
               detail_url = f"https://YOUR_USERNAME.github.io/spread-insight-pages/{os.path.basename(html_files[i])}"
               coupang_url = self.coupang.generate_link(article.keywords or [])

               await self.telegram.broadcast_news(
                   subscribers=subscribers,
                   title=article.title,
                   summary=article.summary,
                   detail_url=detail_url,
                   coupang_url=coupang_url
               )

           self.logger.info("=== ì™„ë£Œ ===")

   if __name__ == '__main__':
       app = SpreadInsight()
       asyncio.run(app.run())
   ```

2. ì‹¤í–‰
   ```bash
   python main.py
   ```

**ì„±ê³µ ê¸°ì¤€:**
- ì „ì²´ íŒŒì´í”„ë¼ì¸ì´ ì—ëŸ¬ ì—†ì´ ì™„ë£Œ
- í…”ë ˆê·¸ë¨ì— ë‰´ìŠ¤ ë„ì°©
- GitHub Pagesì— HTML ë°°í¬

---

### Step 7.2: ìŠ¤ì¼€ì¤„ëŸ¬ ì¶”ê°€
**ëª©í‘œ:** ë§¤ì¼ ì˜¤ì „ 7ì‹œ, ì˜¤í›„ 6ì‹œ ìë™ ì‹¤í–‰
**ì†Œìš” ì‹œê°„:** 1ì‹œê°„

**ì„¸ë¶€ ì‘ì—…:**

1. `scheduler.py` ì‘ì„±
   ```python
   from apscheduler.schedulers.blocking import BlockingScheduler
   from main import SpreadInsight
   import asyncio
   from utils.logger import setup_logger

   logger = setup_logger('Scheduler')

   def job():
       """ìŠ¤ì¼€ì¤„ëŸ¬ ì‘ì—…"""
       logger.info("â° ìŠ¤ì¼€ì¤„ëŸ¬ ì‘ì—… ì‹œì‘")
       app = SpreadInsight()
       asyncio.run(app.run())

   if __name__ == '__main__':
       scheduler = BlockingScheduler()

       # ë§¤ì¼ ì˜¤ì „ 7ì‹œ, ì˜¤í›„ 6ì‹œ ì‹¤í–‰
       scheduler.add_job(job, 'cron', hour=7, minute=0)
       scheduler.add_job(job, 'cron', hour=18, minute=0)

       logger.info("ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ (ì˜¤ì „ 7ì‹œ, ì˜¤í›„ 6ì‹œ ì‹¤í–‰)")

       try:
           scheduler.start()
       except (KeyboardInterrupt, SystemExit):
           logger.info("ìŠ¤ì¼€ì¤„ëŸ¬ ì¢…ë£Œ")
   ```

2. ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ (Linux/Mac)
   ```bash
   nohup python scheduler.py > scheduler.log 2>&1 &
   ```

**ì„±ê³µ ê¸°ì¤€:**
- ì„¤ì •í•œ ì‹œê°„ì— ìë™ ì‹¤í–‰
- ë¡œê·¸ íŒŒì¼ì— ì‹¤í–‰ ê¸°ë¡

---

## ì˜ˆìƒ ë¹„ìš© ë° ë°°í¬ ì˜µì…˜

### ë¹„ìš©
- **Gemini API:** ë¬´ë£Œ í• ë‹¹ëŸ‰ ë‚´ (ë¶„ë‹¹ 15 ìš”ì²­)
- **GitHub Pages:** ë¬´ë£Œ
- **í…”ë ˆê·¸ë¨ ë´‡:** ë¬´ë£Œ
- **ì„œë²„ (ì˜µì…˜):**
  - ë¡œì»¬ PC: ë¬´ë£Œ
  - AWS EC2 t3.micro: ~$10/ì›”
  - GCP Compute Engine f1-micro: ë¬´ë£Œ (1ê°œ)

### ë°°í¬ ì˜µì…˜
1. **ë¡œì»¬ PC:** ê°€ì¥ ì €ë ´, 24ì‹œê°„ ì¼œë‘¬ì•¼ í•¨
2. **í´ë¼ìš°ë“œ VM:** ì•ˆì •ì , ì›” $10 ë‚´ì™¸
3. **AWS Lambda:** ì„œë²„ë¦¬ìŠ¤, ì‹¤í–‰ ì‹œê°„ë§Œ ê³¼ê¸ˆ

---

ì´ìƒì…ë‹ˆë‹¤! ê° ë‹¨ê³„ë¥¼ ì‘ì€ ì„±ê³µ ë‹¨ìœ„ë¡œ ìª¼ê°œì„œ í•˜ë‚˜ì”© ë‹¬ì„±í•˜ë©´ ë©ë‹ˆë‹¤.
