# Phase 2: AI ë¶„ì„ (Week 2)

> Gemini 2.0/2.5 Flashë¥¼ ì‚¬ìš©í•œ ë‰´ìŠ¤ ë¶„ì„ ë° ì¸ì‚¬ì´íŠ¸ ìƒì„±

---

## âœ… Step 2.1: Gemini API ì—°ë™ ë° ìš”ì•½ (ì™„ë£Œ)

*(ì´ë¯¸ DETAILED_STEPS.mdì— ì‘ì„±ë˜ì–´ ìˆìŒ)*

---

## âœ… Step 2.2: ì‰¬ìš´ ì–¸ì–´ë¡œ ì¬ì‘ì„±
**ëª©í‘œ:** ì „ë¬¸ ìš©ì–´ë¥¼ ì¤‘í•™ìƒë„ ì´í•´í•  ìˆ˜ ìˆê²Œ ë³€í™˜
**ì†Œìš” ì‹œê°„:** 1.5ì‹œê°„

### ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] `gemini_analyzer.py`ì— `simplify_language()` ë©”ì„œë“œ ì¶”ê°€
- [ ] í”„ë¡¬í”„íŠ¸ ìµœì í™” (ì¤‘í•™ìƒ ëˆˆë†’ì´)
- [ ] í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
- [ ] ì‹¤í–‰ ë° ê²€ì¦

### ğŸ› ï¸ ì‹¤í–‰ ìˆœì„œ

**1. gemini_analyzer.py ì—…ë°ì´íŠ¸**
íŒŒì¼ ê²½ë¡œ: `g:\ë‚´ ë“œë¼ì´ë¸Œ\08.Programming\spread_insight\analyzers\gemini_analyzer.py`

ê¸°ì¡´ íŒŒì¼ì— ë‹¤ìŒ ë©”ì„œë“œë¥¼ ì¶”ê°€:
```python
def simplify_language(self, article: NewsArticle) -> str:
    """ì „ë¬¸ ìš©ì–´ë¥¼ ì‰¬ìš´ ì–¸ì–´ë¡œ ë³€í™˜"""
    prompt = f"""
ë‹¤ìŒ ê²½ì œ ë‰´ìŠ¤ë¥¼ ì¤‘í•™ìƒë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì‰½ê²Œ ë‹¤ì‹œ ì‘ì„±í•´ì£¼ì„¸ìš”.

**ê·œì¹™:**
1. ì „ë¬¸ ìš©ì–´ëŠ” ê´„í˜¸ ì•ˆì— ì‰¬ìš´ ì„¤ëª… ì¶”ê°€
   ì˜ˆ: "ê¸°ì¤€ê¸ˆë¦¬(í•œêµ­ì€í–‰ì´ ì •í•˜ëŠ” ê¸°ë³¸ ì´ììœ¨)"
2. ë¬¸ì¥ì€ ì§§ê³  ëª…í™•í•˜ê²Œ (í•œ ë¬¸ì¥ì— í•œ ê°€ì§€ ë‚´ìš©)
3. ë¹„ìœ ë‚˜ êµ¬ì²´ì  ì˜ˆì‹œë¥¼ í™œìš©
4. "~ì…ë‹ˆë‹¤", "~ë©ë‹ˆë‹¤" ê°™ì€ ì •ì¤‘í•œ ì–´íˆ¬ ì‚¬ìš©
5. ìˆ«ìê°€ ë‚˜ì˜¤ë©´ ë¹„êµ ëŒ€ìƒ ì œì‹œ (ì˜ˆ: "ì‘ë…„ë³´ë‹¤ 10% ì¦ê°€")

ì›ë³¸ ê¸°ì‚¬:
ì œëª©: {article.title}
ë³¸ë¬¸: {article.content}

ì‰¬ìš´ ì„¤ëª…:
    """.strip()

    try:
        response = self.model.generate_content(prompt)
        easy_text = response.text.strip()
        return easy_text

    except Exception as e:
        raise Exception(f"ì‰¬ìš´ ì„¤ëª… ìƒì„± ì‹¤íŒ¨: {e}")
```

**2. í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±**
íŒŒì¼ ê²½ë¡œ: `g:\ë‚´ ë“œë¼ì´ë¸Œ\08.Programming\spread_insight\test_simplify.py`
```python
from utils.file_manager import FileManager
from analyzers.gemini_analyzer import GeminiAnalyzer


if __name__ == '__main__':
    print("=" * 60)
    print("ì‰¬ìš´ ì–¸ì–´ë¡œ ì¬ì‘ì„± í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    # íŒŒì¼ ë§¤ë‹ˆì € ë° ë¶„ì„ê¸° ì´ˆê¸°í™”
    file_manager = FileManager()
    analyzer = GeminiAnalyzer()

    # ì €ì¥ëœ ê¸°ì‚¬ ë¡œë“œ
    saved_files = file_manager.list_saved_files()
    if not saved_files:
        print("âŒ ì €ì¥ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € test_save_articles.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        exit(1)

    latest_file = saved_files[0]
    print(f"ë¡œë“œ íŒŒì¼: {latest_file}\n")

    articles = file_manager.load_articles(latest_file)
    article = articles[0]

    # ì›ë³¸ ê¸°ì‚¬ ì¶œë ¥
    print("=" * 60)
    print("ì›ë³¸ ê¸°ì‚¬")
    print("=" * 60)
    print(f"ì œëª©: {article.title}")
    print(f"\në³¸ë¬¸ (ì• 500ì):")
    print(article.content[:500])
    print("...\n")

    # ì‰¬ìš´ ì–¸ì–´ë¡œ ë³€í™˜
    print("=" * 60)
    print("ë³€í™˜ ì¤‘...")
    print("=" * 60)

    try:
        easy_explanation = analyzer.simplify_language(article)

        print("\nâœ… ì‰¬ìš´ ì„¤ëª…:\n")
        print("=" * 60)
        print(easy_explanation)
        print("=" * 60)

        # ê¸°ì‚¬ ê°ì²´ì— ì €ì¥
        article.easy_explanation = easy_explanation

        # ê¸°ì¡´ ìš”ì•½ë„ í•¨ê»˜ ì €ì¥
        if not article.summary:
            print("\nìš”ì•½ë„ í•¨ê»˜ ìƒì„± ì¤‘...")
            article.summary = analyzer.summarize(article, num_sentences=3)

        # ì €ì¥
        file_manager.save_articles([article], filename='simplified_article.json')
        print(f"\nğŸ’¾ ì €ì¥ ì™„ë£Œ: simplified_article.json")

    except Exception as e:
        print(f"âŒ ë³€í™˜ ì‹¤íŒ¨: {e}")
        exit(1)
```

**3. ì‹¤í–‰**
```bash
python test_simplify.py
```

### âœ… ì„±ê³µ ê¸°ì¤€
- [ ] ì „ë¬¸ ìš©ì–´ê°€ ê´„í˜¸ë¡œ ì„¤ëª…ë¨
- [ ] ë¬¸ì¥ì´ ì›ë³¸ë³´ë‹¤ ì§§ê³  ëª…í™•í•¨
- [ ] ì¤‘í•™ìƒ ìˆ˜ì¤€ì—ì„œ ì´í•´ ê°€ëŠ¥
- [ ] `data/raw/simplified_article.json`ì— easy_explanation í•„ë“œ í¬í•¨

### âš ï¸ ì˜ˆìƒ ì˜¤ë¥˜ ë° í•´ê²°

**ì˜¤ë¥˜ 1:** ì‘ë‹µì´ ë„ˆë¬´ ê¸¸ì–´ì„œ ì˜ë¦¼
- **ì›ì¸:** ë³¸ë¬¸ì´ ë„ˆë¬´ ê¹€ (Geminiì˜ ì¶œë ¥ ì œí•œ)
- **í•´ê²°:** ë³¸ë¬¸ì„ 2000ìë¡œ ì œí•œ
  ```python
  content_preview = article.content[:2000]
  prompt = f"ë³¸ë¬¸: {content_preview}..."
  ```

**ì˜¤ë¥˜ 2:** ì—¬ì „íˆ ì–´ë ¤ìš´ ìš©ì–´ ì‚¬ìš©
- **ì›ì¸:** í”„ë¡¬í”„íŠ¸ ë¶ˆì¶©ë¶„
- **í•´ê²°:** í”„ë¡¬í”„íŠ¸ì— êµ¬ì²´ì  ì˜ˆì‹œ ì¶”ê°€
  ```python
  ì˜ˆì‹œ:
  ì›ë³¸: "í•œêµ­ì€í–‰ì€ ê¸°ì¤€ê¸ˆë¦¬ë¥¼ 3.5%ë¡œ ë™ê²°í–ˆë‹¤"
  ì‰¬ìš´ ë²„ì „: "í•œêµ­ì€í–‰ì€ ê¸°ì¤€ê¸ˆë¦¬(ì€í–‰ë“¤ì´ ëŒ€ì¶œí•  ë•Œ ì°¸ê³ í•˜ëŠ” ê¸°ë³¸ ì´ììœ¨)ë¥¼ 3.5%ë¡œ ìœ ì§€í•˜ê¸°ë¡œ í–ˆìŠµë‹ˆë‹¤."
  ```

---

## âœ… Step 2.3: ìš©ì–´ ìë™ ì¶”ì¶œ ë° í•´ì„¤
**ëª©í‘œ:** ì–´ë ¤ìš´ ë‹¨ì–´ë¥¼ ìë™ìœ¼ë¡œ ì°¾ì•„ì„œ ìš©ì–´ì§‘ ìƒì„±
**ì†Œìš” ì‹œê°„:** 1.5ì‹œê°„

### ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] `analyzers/terminology.py` ì‘ì„±
- [ ] JSON íŒŒì‹± ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
- [ ] í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
- [ ] ì‹¤í–‰ ë° ê²€ì¦

### ğŸ› ï¸ ì‹¤í–‰ ìˆœì„œ

**1. ìš©ì–´ ì¶”ì¶œê¸° ì‘ì„±**
íŒŒì¼ ê²½ë¡œ: `g:\ë‚´ ë“œë¼ì´ë¸Œ\08.Programming\spread_insight\analyzers\terminology.py`
```python
import google.generativeai as genai
from utils.config import Config
import json
import re


class TerminologyExtractor:
    """ì „ë¬¸ ìš©ì–´ ìë™ ì¶”ì¶œ ë° ì„¤ëª… ìƒì„±"""

    def __init__(self):
        genai.configure(api_key=Config.GEMINI_API_KEY)
        self.model = genai.GenerativeModel(Config.GEMINI_MODEL)

    def extract_and_explain(self, text: str, max_terms: int = 7) -> dict[str, str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì „ë¬¸ ìš©ì–´ ì¶”ì¶œ ë° ì„¤ëª…"""
        prompt = f"""
ë‹¤ìŒ ê²½ì œ ë‰´ìŠ¤ì—ì„œ ì¼ë°˜ì¸ì´ ì–´ë ¤ì›Œí•  ë§Œí•œ ì „ë¬¸ ìš©ì–´ë¥¼ ìµœëŒ€ {max_terms}ê°œ ì°¾ì•„ì„œ
JSON í˜•ì‹ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.

**ì„ ì • ê¸°ì¤€:**
- ê²½ì œ/ê¸ˆìœµ ì „ë¬¸ ìš©ì–´
- ì•½ì–´ (GDP, IMF ë“±)
- ì¼ë°˜ì¸ì´ ì˜ ëª¨ë¥¼ ë²•í•œ ë‹¨ì–´
- ì¤‘ìš”ë„ê°€ ë†’ì€ ìˆœì„œëŒ€ë¡œ

**ì„¤ëª… ê¸°ì¤€:**
- ì¤‘í•™ìƒë„ ì´í•´í•  ìˆ˜ ìˆê²Œ
- 1~2ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ
- êµ¬ì²´ì  ì˜ˆì‹œë‚˜ ë¹„ìœ  í¬í•¨

í…ìŠ¤íŠ¸:
{text[:2000]}

**ì¶œë ¥ í˜•ì‹ (ë°˜ë“œì‹œ ìœ íš¨í•œ JSON):**
{{
    "ê¸°ì¤€ê¸ˆë¦¬": "í•œêµ­ì€í–‰ì´ ì •í•˜ëŠ” ê¸°ë³¸ ì´ììœ¨ë¡œ, ì€í–‰ë“¤ì´ ëŒ€ì¶œí•  ë•Œ ì°¸ê³ í•˜ëŠ” ê¸°ì¤€ì…ë‹ˆë‹¤.",
    "í™˜ìœ¨": "ë‹¤ë¥¸ ë‚˜ë¼ ëˆê³¼ ìš°ë¦¬ë‚˜ë¼ ëˆì„ ë°”ê¿€ ë•Œì˜ ë¹„ìœ¨ì…ë‹ˆë‹¤. ì˜ˆ: ë‹¬ëŸ¬ë‹¹ 1,300ì›",
    "GDP": "êµ­ë‚´ì´ìƒì‚°ì˜ ì•½ìë¡œ, í•œ ë‚˜ë¼ê°€ 1ë…„ ë™ì•ˆ ë§Œë“¤ì–´ë‚¸ ëª¨ë“  ìƒí’ˆê³¼ ì„œë¹„ìŠ¤ì˜ ê°€ì¹˜ë¥¼ í•©ì¹œ ê²ƒì…ë‹ˆë‹¤."
}}

JSON:
        """.strip()

        try:
            response = self.model.generate_content(prompt)
            json_text = response.text.strip()

            # Markdown ì½”ë“œ ë¸”ë¡ ì œê±°
            if '```' in json_text:
                # ```json ... ``` í˜•íƒœ ì²˜ë¦¬
                json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', json_text, re.DOTALL)
                if json_match:
                    json_text = json_match.group(1)
                else:
                    # ``` ì´í›„ ì²« ë²ˆì§¸ { ë¶€í„° ë§ˆì§€ë§‰ } ê¹Œì§€
                    json_text = re.sub(r'^```(?:json)?', '', json_text)
                    json_text = re.sub(r'```$', '', json_text)

            # JSON íŒŒì‹±
            json_text = json_text.strip()
            terminology = json.loads(json_text)

            # ë”•ì…”ë„ˆë¦¬ ê²€ì¦
            if not isinstance(terminology, dict):
                raise ValueError("ì‘ë‹µì´ ë”•ì…”ë„ˆë¦¬ í˜•íƒœê°€ ì•„ë‹™ë‹ˆë‹¤.")

            print(f"âœ… {len(terminology)}ê°œ ìš©ì–´ ì¶”ì¶œ")
            return terminology

        except json.JSONDecodeError as e:
            print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            print(f"ì‘ë‹µ: {response.text[:500]}")
            return {}
        except Exception as e:
            print(f"âŒ ìš©ì–´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {}

    def get_terminology_for_article(self, article) -> dict[str, str]:
        """ê¸°ì‚¬ ì „ì²´ì—ì„œ ìš©ì–´ ì¶”ì¶œ (ì œëª© + ë³¸ë¬¸)"""
        full_text = f"{article.title}\n\n{article.content}"
        return self.extract_and_explain(full_text)
```

**2. í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±**
íŒŒì¼ ê²½ë¡œ: `g:\ë‚´ ë“œë¼ì´ë¸Œ\08.Programming\spread_insight\test_terminology.py`
```python
from utils.file_manager import FileManager
from analyzers.terminology import TerminologyExtractor


if __name__ == '__main__':
    print("=" * 60)
    print("ìš©ì–´ ì¶”ì¶œ ë° í•´ì„¤ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    # íŒŒì¼ ë§¤ë‹ˆì € ë° ì¶”ì¶œê¸° ì´ˆê¸°í™”
    file_manager = FileManager()
    term_extractor = TerminologyExtractor()

    # ì €ì¥ëœ ê¸°ì‚¬ ë¡œë“œ
    saved_files = file_manager.list_saved_files()
    if not saved_files:
        print("âŒ ì €ì¥ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        exit(1)

    latest_file = saved_files[0]
    print(f"ë¡œë“œ íŒŒì¼: {latest_file}\n")

    articles = file_manager.load_articles(latest_file)
    article = articles[0]

    # ì›ë³¸ ê¸°ì‚¬ ì¶œë ¥
    print("=" * 60)
    print("ì›ë³¸ ê¸°ì‚¬")
    print("=" * 60)
    print(f"ì œëª©: {article.title}")
    print(f"ë³¸ë¬¸ ê¸¸ì´: {len(article.content)}ì\n")

    # ìš©ì–´ ì¶”ì¶œ
    print("=" * 60)
    print("ìš©ì–´ ì¶”ì¶œ ì¤‘...")
    print("=" * 60)

    terminology = term_extractor.get_terminology_for_article(article)

    if terminology:
        print(f"\nâœ… ì¶”ì¶œëœ ìš©ì–´ ({len(terminology)}ê°œ):\n")
        for i, (term, explanation) in enumerate(terminology.items(), 1):
            print(f"{i}. {term}")
            print(f"   â†’ {explanation}\n")

        # ê¸°ì‚¬ ê°ì²´ì— ì €ì¥
        article.terminology = terminology

        # ì €ì¥
        file_manager.save_articles([article], filename='article_with_terminology.json')
        print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: article_with_terminology.json")
    else:
        print("âŒ ìš©ì–´ë¥¼ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
```

**3. ì‹¤í–‰**
```bash
python test_terminology.py
```

### âœ… ì„±ê³µ ê¸°ì¤€
- [ ] 3~7ê°œì˜ ì „ë¬¸ ìš©ì–´ ì¶”ì¶œ
- [ ] ê° ìš©ì–´ì— ëŒ€í•œ ì‰¬ìš´ ì„¤ëª…
- [ ] JSON íŒŒì‹± ì˜¤ë¥˜ ì—†ìŒ
- [ ] `data/raw/article_with_terminology.json` ìƒì„±

### âš ï¸ ì˜ˆìƒ ì˜¤ë¥˜ ë° í•´ê²°

**ì˜¤ë¥˜ 1:** `JSON íŒŒì‹± ì‹¤íŒ¨`
- **ì›ì¸:** Geminiê°€ JSON ì™¸ì— ë‹¤ë¥¸ í…ìŠ¤íŠ¸ë„ í•¨ê»˜ ì¶œë ¥
- **í•´ê²°:** ì •ê·œì‹ìœ¼ë¡œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (ì´ë¯¸ ì½”ë“œì— í¬í•¨)

**ì˜¤ë¥˜ 2:** ì‘ë‹µì´ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹Œ ë¦¬ìŠ¤íŠ¸
- **ì›ì¸:** í”„ë¡¬í”„íŠ¸ í•´ì„ ì˜¤ë¥˜
- **í•´ê²°:** í”„ë¡¬í”„íŠ¸ì— ì˜ˆì‹œ ë” ëª…í™•íˆ ì œì‹œ

**ì˜¤ë¥˜ 3:** ìš©ì–´ê°€ ë„ˆë¬´ ì ìŒ (1~2ê°œ)
- **ì›ì¸:** `max_terms` íŒŒë¼ë¯¸í„° ë¬´ì‹œ
- **í•´ê²°:** í”„ë¡¬í”„íŠ¸ì— "ì •í™•íˆ 5~7ê°œ" ê°™ì´ êµ¬ì²´ì ìœ¼ë¡œ ëª…ì‹œ

---

## âœ… Step 2.4: ê³¼ê±° ë§¥ë½ êµ¬ì„± (íƒ€ì„ë¼ì¸)
**ëª©í‘œ:** í˜„ì¬ ë‰´ìŠ¤ì™€ ê´€ë ¨ëœ ê³¼ê±° ë‰´ìŠ¤ë¥¼ ì°¾ì•„ ì‹œê³„ì—´ ë§¥ë½ ìƒì„±
**ì†Œìš” ì‹œê°„:** 2ì‹œê°„

### ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] `database/db_manager.py` ì‘ì„± (SQLite)
- [ ] `analyzers/context_builder.py` ì‘ì„±
- [ ] í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
- [ ] ì‹¤í–‰ ë° ê²€ì¦

### ğŸ› ï¸ ì‹¤í–‰ ìˆœì„œ

**1. ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € ì‘ì„±**
íŒŒì¼ ê²½ë¡œ: `g:\ë‚´ ë“œë¼ì´ë¸Œ\08.Programming\spread_insight\database\db_manager.py`
```python
import sqlite3
from typing import List
from models.news_article import NewsArticle
from datetime import datetime
import json
import os


class DatabaseManager:
    """SQLite ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬"""

    def __init__(self, db_path: str = './database/news.db'):
        self.db_path = db_path

        # database í´ë” ìƒì„±
        os.makedirs(os.path.dirname(db_path), exist_ok=True)

        self._create_table()

    def _create_table(self):
        """ë‰´ìŠ¤ í…Œì´ë¸” ìƒì„±"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute('''
            CREATE TABLE IF NOT EXISTS articles (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                url TEXT UNIQUE NOT NULL,
                title TEXT NOT NULL,
                content TEXT NOT NULL,
                published_at TEXT NOT NULL,
                source TEXT NOT NULL,
                keywords TEXT,
                summary TEXT,
                easy_explanation TEXT,
                terminology TEXT,
                created_at TEXT DEFAULT CURRENT_TIMESTAMP
            )
        ''')

        # ì¸ë±ìŠ¤ ìƒì„± (ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ)
        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_published_at
            ON articles(published_at DESC)
        ''')

        cursor.execute('''
            CREATE INDEX IF NOT EXISTS idx_title
            ON articles(title)
        ''')

        conn.commit()
        conn.close()
        print("âœ… ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ìƒì„± ì™„ë£Œ")

    def insert_article(self, article: NewsArticle) -> bool:
        """ê¸°ì‚¬ ì‚½ì… (ì¤‘ë³µ ì‹œ ë¬´ì‹œ)"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        try:
            cursor.execute('''
                INSERT OR IGNORE INTO articles
                (url, title, content, published_at, source, keywords, summary, easy_explanation, terminology)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                article.url,
                article.title,
                article.content,
                article.published_at.isoformat(),
                article.source,
                json.dumps(article.keywords, ensure_ascii=False) if article.keywords else None,
                article.summary,
                article.easy_explanation,
                json.dumps(article.terminology, ensure_ascii=False) if article.terminology else None
            ))

            inserted = cursor.rowcount > 0
            conn.commit()

            if inserted:
                print(f"  âœ… DB ì €ì¥: {article.title[:40]}")
            else:
                print(f"  âš ï¸  ì¤‘ë³µ: {article.title[:40]}")

            return inserted

        except Exception as e:
            print(f"  âŒ DB ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
        finally:
            conn.close()

    def insert_articles(self, articles: List[NewsArticle]) -> int:
        """ì—¬ëŸ¬ ê¸°ì‚¬ ì¼ê´„ ì‚½ì…"""
        count = 0
        for article in articles:
            if self.insert_article(article):
                count += 1
        return count

    def search_by_keyword(self, keyword: str, limit: int = 10, exclude_url: str = None) -> List[NewsArticle]:
        """í‚¤ì›Œë“œë¡œ ê¸°ì‚¬ ê²€ìƒ‰ (ì œëª© + ë³¸ë¬¸)"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        query = '''
            SELECT url, title, content, published_at, source, keywords, summary, easy_explanation, terminology
            FROM articles
            WHERE (title LIKE ? OR content LIKE ?)
        '''
        params = [f'%{keyword}%', f'%{keyword}%']

        # í˜„ì¬ ê¸°ì‚¬ ì œì™¸
        if exclude_url:
            query += ' AND url != ?'
            params.append(exclude_url)

        query += ' ORDER BY published_at DESC LIMIT ?'
        params.append(limit)

        cursor.execute(query, params)
        rows = cursor.fetchall()
        conn.close()

        articles = []
        for row in rows:
            article = NewsArticle(
                url=row[0],
                title=row[1],
                content=row[2],
                published_at=datetime.fromisoformat(row[3]),
                source=row[4],
                keywords=json.loads(row[5]) if row[5] else None,
                summary=row[6],
                easy_explanation=row[7],
                terminology=json.loads(row[8]) if row[8] else None
            )
            articles.append(article)

        return articles

    def get_total_count(self) -> int:
        """ì „ì²´ ê¸°ì‚¬ ìˆ˜"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('SELECT COUNT(*) FROM articles')
        count = cursor.fetchone()[0]
        conn.close()
        return count
```

**2. ì»¨í…ìŠ¤íŠ¸ ë¹Œë” ì‘ì„±**
íŒŒì¼ ê²½ë¡œ: `g:\ë‚´ ë“œë¼ì´ë¸Œ\08.Programming\spread_insight\analyzers\context_builder.py`
```python
import google.generativeai as genai
from utils.config import Config
from database.db_manager import DatabaseManager
from models.news_article import NewsArticle
from typing import List


class ContextBuilder:
    """ê³¼ê±° ë§¥ë½ êµ¬ì„± ë° íƒ€ì„ë¼ì¸ ìƒì„±"""

    def __init__(self):
        genai.configure(api_key=Config.GEMINI_API_KEY)
        self.model = genai.GenerativeModel(Config.GEMINI_MODEL)
        self.db = DatabaseManager()

    def extract_keywords(self, article: NewsArticle, max_keywords: int = 5) -> list[str]:
        """ê¸°ì‚¬ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        prompt = f"""
ë‹¤ìŒ ê²½ì œ ë‰´ìŠ¤ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì •í™•íˆ {max_keywords}ê°œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

**ì„ ì • ê¸°ì¤€:**
- ê¸°ì‚¬ì˜ ì£¼ì œë¥¼ ëŒ€í‘œí•˜ëŠ” ë‹¨ì–´
- ê²€ìƒ‰ì— ìœ ìš©í•œ ëª…ì‚¬ (ê³ ìœ ëª…ì‚¬, ê²½ì œ ìš©ì–´ ë“±)
- ë„ˆë¬´ ì¼ë°˜ì ì¸ ë‹¨ì–´ ì œì™¸ (ì˜ˆ: "ê²½ì œ", "ë‰´ìŠ¤")

ì œëª©: {article.title}
ë³¸ë¬¸ ì•ë¶€ë¶„: {article.content[:500]}

**ì¶œë ¥ í˜•ì‹:** ì‰¼í‘œë¡œ êµ¬ë¶„ëœ í‚¤ì›Œë“œë§Œ ì¶œë ¥
ì˜ˆ: ê¸°ì¤€ê¸ˆë¦¬, í•œêµ­ì€í–‰, ì¸í”Œë ˆì´ì…˜, ë¬¼ê°€ìƒìŠ¹ë¥ , í†µí™”ì •ì±…

í‚¤ì›Œë“œ:
        """.strip()

        try:
            response = self.model.generate_content(prompt)
            keywords_text = response.text.strip()

            # ì‰¼í‘œë¡œ ë¶„ë¦¬
            keywords = [k.strip() for k in keywords_text.split(',')]
            keywords = [k for k in keywords if k]  # ë¹ˆ ë¬¸ìì—´ ì œê±°

            print(f"  âœ… í‚¤ì›Œë“œ ì¶”ì¶œ: {', '.join(keywords)}")
            return keywords[:max_keywords]

        except Exception as e:
            print(f"  âŒ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []

    def find_related_articles(self, article: NewsArticle) -> List[NewsArticle]:
        """ê´€ë ¨ ê³¼ê±° ê¸°ì‚¬ ê²€ìƒ‰"""
        if not article.keywords:
            article.keywords = self.extract_keywords(article)

        if not article.keywords:
            return []

        # ê° í‚¤ì›Œë“œë¡œ ê²€ìƒ‰
        related_articles = []
        seen_urls = {article.url}

        for keyword in article.keywords:
            articles = self.db.search_by_keyword(
                keyword=keyword,
                limit=3,
                exclude_url=article.url
            )

            for art in articles:
                if art.url not in seen_urls:
                    seen_urls.add(art.url)
                    related_articles.append(art)

        # ë‚ ì§œìˆœ ì •ë ¬ (ì˜¤ë˜ëœ ê²ƒë¶€í„°)
        related_articles.sort(key=lambda a: a.published_at)

        print(f"  âœ… ê´€ë ¨ ê¸°ì‚¬ {len(related_articles)}ê°œ ë°œê²¬")
        return related_articles[:5]  # ìµœëŒ€ 5ê°œ

    def build_timeline(self, article: NewsArticle) -> str:
        """ê³¼ê±° ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ íƒ€ì„ë¼ì¸ ìƒì„±"""
        # 1. í‚¤ì›Œë“œ ì¶”ì¶œ
        if not article.keywords:
            article.keywords = self.extract_keywords(article)

        # 2. ê³¼ê±° ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰
        related_articles = self.find_related_articles(article)

        if not related_articles:
            print("  âš ï¸  ê´€ë ¨ ê³¼ê±° ê¸°ì‚¬ ì—†ìŒ")
            return "ì´ ë‰´ìŠ¤ëŠ” ìµœê·¼ì— ë“±ì¥í•œ ìƒˆë¡œìš´ ì´ìŠˆì…ë‹ˆë‹¤."

        # 3. íƒ€ì„ë¼ì¸ ìƒì„±
        related_text = "\n\n".join([
            f"[{art.published_at.strftime('%Yë…„ %mì›” %dì¼')}] {art.title}\n{art.summary or art.content[:200]}"
            for art in related_articles
        ])

        prompt = f"""
í˜„ì¬ ë‰´ìŠ¤ì™€ ê´€ë ¨ëœ ê³¼ê±° ê¸°ì‚¬ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì´ ì´ìŠˆê°€ ì–´ë–»ê²Œ ë°œì „í•´ì™”ëŠ”ì§€
ì‹œê³„ì—´ íƒ€ì„ë¼ì¸ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.

**ê·œì¹™:**
- 3~5ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±
- ì‹œê°„ íë¦„ì— ë”°ë¼ ì„¤ëª…
- ì¤‘í•™ìƒë„ ì´í•´í•  ìˆ˜ ìˆê²Œ
- "ì²˜ìŒì—ëŠ”...", "ì´í›„...", "ìµœê·¼ì—ëŠ”..." ê°™ì€ ì—°ê²°ì–´ ì‚¬ìš©

**í˜„ì¬ ë‰´ìŠ¤:**
[{article.published_at.strftime('%Yë…„ %mì›” %dì¼')}] {article.title}
ë‚´ìš©: {article.content[:300]}

**ê³¼ê±° ê´€ë ¨ ê¸°ì‚¬:**
{related_text}

íƒ€ì„ë¼ì¸ ì„¤ëª…:
        """.strip()

        try:
            response = self.model.generate_content(prompt)
            timeline = response.text.strip()
            print(f"  âœ… íƒ€ì„ë¼ì¸ ìƒì„± ì™„ë£Œ")
            return timeline

        except Exception as e:
            print(f"  âŒ íƒ€ì„ë¼ì¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return "íƒ€ì„ë¼ì¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
```

**3. í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±**
íŒŒì¼ ê²½ë¡œ: `g:\ë‚´ ë“œë¼ì´ë¸Œ\08.Programming\spread_insight\test_context.py`
```python
from utils.file_manager import FileManager
from analyzers.context_builder import ContextBuilder
from database.db_manager import DatabaseManager


if __name__ == '__main__':
    print("=" * 60)
    print("ê³¼ê±° ë§¥ë½ êµ¬ì„± ë° íƒ€ì„ë¼ì¸ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    # ì´ˆê¸°í™”
    file_manager = FileManager()
    db = DatabaseManager()
    context_builder = ContextBuilder()

    # 1ë‹¨ê³„: ê³¼ê±° ê¸°ì‚¬ë“¤ì„ DBì— ì €ì¥
    print("\n1ë‹¨ê³„: ê³¼ê±° ê¸°ì‚¬ DB ì €ì¥")
    print("-" * 60)

    saved_files = file_manager.list_saved_files()
    if not saved_files:
        print("âŒ ì €ì¥ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € test_multi_scraper.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        exit(1)

    # ëª¨ë“  ì €ì¥ëœ íŒŒì¼ì˜ ê¸°ì‚¬ë¥¼ DBì— ì¶”ê°€
    total_inserted = 0
    for filename in saved_files[:3]:  # ìµœê·¼ 3ê°œ íŒŒì¼ë§Œ
        try:
            articles = file_manager.load_articles(filename)
            inserted = db.insert_articles(articles)
            total_inserted += inserted
            print(f"  {filename}: {inserted}/{len(articles)}ê°œ ì €ì¥")
        except Exception as e:
            print(f"  âŒ {filename}: {e}")

    print(f"\nâœ… ì´ {total_inserted}ê°œ ê¸°ì‚¬ DB ì €ì¥")
    print(f"ì „ì²´ DB ê¸°ì‚¬ ìˆ˜: {db.get_total_count()}ê°œ\n")

    # 2ë‹¨ê³„: ìµœì‹  ê¸°ì‚¬ ì„ íƒ
    print("2ë‹¨ê³„: ìµœì‹  ê¸°ì‚¬ ì„ íƒ")
    print("-" * 60)

    latest_file = saved_files[0]
    articles = file_manager.load_articles(latest_file)
    test_article = articles[0]

    print(f"ì„ íƒëœ ê¸°ì‚¬: {test_article.title}\n")

    # 3ë‹¨ê³„: í‚¤ì›Œë“œ ì¶”ì¶œ
    print("3ë‹¨ê³„: í‚¤ì›Œë“œ ì¶”ì¶œ")
    print("-" * 60)

    keywords = context_builder.extract_keywords(test_article)
    test_article.keywords = keywords
    print(f"í‚¤ì›Œë“œ: {', '.join(keywords)}\n")

    # 4ë‹¨ê³„: ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰
    print("4ë‹¨ê³„: ê´€ë ¨ ê³¼ê±° ê¸°ì‚¬ ê²€ìƒ‰")
    print("-" * 60)

    related_articles = context_builder.find_related_articles(test_article)

    if related_articles:
        for i, art in enumerate(related_articles, 1):
            print(f"{i}. [{art.published_at.strftime('%Y-%m-%d')}] {art.title[:50]}")
    else:
        print("âš ï¸  ê´€ë ¨ ê¸°ì‚¬ ì—†ìŒ")

    print()

    # 5ë‹¨ê³„: íƒ€ì„ë¼ì¸ ìƒì„±
    print("5ë‹¨ê³„: íƒ€ì„ë¼ì¸ ìƒì„±")
    print("-" * 60)

    timeline = context_builder.build_timeline(test_article)

    print("\nâœ… íƒ€ì„ë¼ì¸:\n")
    print("=" * 60)
    print(timeline)
    print("=" * 60)

    # ì €ì¥
    print("\n6ë‹¨ê³„: ì €ì¥")
    print("-" * 60)

    # íƒ€ì„ë¼ì¸ ì •ë³´ë¥¼ ì–´ë””ì— ì €ì¥í• ì§€ëŠ” ë‚˜ì¤‘ì— ê²°ì •
    # ì§€ê¸ˆì€ ì½˜ì†”ì—ë§Œ ì¶œë ¥
    print(f"âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
```

**4. ì‹¤í–‰**
```bash
python test_context.py
```

### âœ… ì„±ê³µ ê¸°ì¤€
- [ ] `database/news.db` íŒŒì¼ ìƒì„±
- [ ] ê³¼ê±° ê¸°ì‚¬ë“¤ì´ DBì— ì €ì¥ë¨
- [ ] í‚¤ì›Œë“œ ê¸°ë°˜ ê´€ë ¨ ê¸°ì‚¬ ê²€ìƒ‰ ë™ì‘
- [ ] íƒ€ì„ë¼ì¸ í…ìŠ¤íŠ¸ ìƒì„± (3~5ë¬¸ì¥)

### âš ï¸ ì˜ˆìƒ ì˜¤ë¥˜ ë° í•´ê²°

**ì˜¤ë¥˜ 1:** `no such table: articles`
- **ì›ì¸:** DB í…Œì´ë¸” ìƒì„± ì•ˆ ë¨
- **í•´ê²°:** `database` í´ë”ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸, `_create_table()` í˜¸ì¶œ í™•ì¸

**ì˜¤ë¥˜ 2:** ê´€ë ¨ ê¸°ì‚¬ê°€ í•˜ë‚˜ë„ ì—†ìŒ
- **ì›ì¸:** DBì— ê¸°ì‚¬ê°€ ë„ˆë¬´ ì ìŒ ë˜ëŠ” í‚¤ì›Œë“œ ë§¤ì¹­ ì•ˆ ë¨
- **í•´ê²°:** ë¨¼ì € `test_multi_scraper.py`ë¥¼ ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰í•´ì„œ ê¸°ì‚¬ ì¶•ì 

**ì˜¤ë¥˜ 3:** í‚¤ì›Œë“œê°€ ë„ˆë¬´ ì¼ë°˜ì  (ì˜ˆ: "ê²½ì œ", "ë‰´ìŠ¤")
- **ì›ì¸:** Geminiê°€ êµ¬ì²´ì ì¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì•ˆ í•¨
- **í•´ê²°:** í”„ë¡¬í”„íŠ¸ì— "êµ¬ì²´ì ì´ê³  ê³ ìœ í•œ í‚¤ì›Œë“œ" ê°•ì¡°

---

## âœ… Step 2.5: ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚° ë° ìƒìœ„ ë‰´ìŠ¤ ì„ ì •
**ëª©í‘œ:** ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ì¤‘ ì¤‘ìš”ë„ê°€ ë†’ì€ ìƒìœ„ 5ê°œ ì„ ì •
**ì†Œìš” ì‹œê°„:** 1.5ì‹œê°„

### ğŸ“ ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] `analyzers/importance_ranker.py` ì‘ì„±
- [ ] ì ìˆ˜ ê³„ì‚° ë¡œì§ êµ¬í˜„
- [ ] í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
- [ ] ì‹¤í–‰ ë° ê²€ì¦

### ğŸ› ï¸ ì‹¤í–‰ ìˆœì„œ

**1. ì¤‘ìš”ë„ ë­ì»¤ ì‘ì„±**
íŒŒì¼ ê²½ë¡œ: `g:\ë‚´ ë“œë¼ì´ë¸Œ\08.Programming\spread_insight\analyzers\importance_ranker.py`
```python
from typing import List
from models.news_article import NewsArticle
import re


class ImportanceRanker:
    """ê¸°ì‚¬ ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚° ë° ë­í‚¹"""

    # ì¤‘ìš” í‚¤ì›Œë“œ (ê°€ì¤‘ì¹˜ ë¶€ì—¬)
    HIGH_PRIORITY_KEYWORDS = {
        # ê¸ˆìœµ/í†µí™”
        'í•œêµ­ì€í–‰': 4,
        'ê¸°ì¤€ê¸ˆë¦¬': 4,
        'ê¸ˆë¦¬': 3,
        'í†µí™”ì •ì±…': 3,

        # í™˜ìœ¨/ë¬´ì—­
        'í™˜ìœ¨': 3,
        'ë‹¬ëŸ¬': 2,
        'ì›í™”': 2,
        'ë¬´ì—­ìˆ˜ì§€': 3,
        'ìˆ˜ì¶œ': 2,
        'ìˆ˜ì…': 2,

        # ì£¼ì‹/ì¦ì‹œ
        'ì½”ìŠ¤í”¼': 3,
        'ì½”ìŠ¤ë‹¥': 3,
        'ì£¼ê°€': 2,
        'ì¦ì‹œ': 2,
        'ì£¼ì‹': 2,

        # ê²½ì œì§€í‘œ
        'GDP': 4,
        'ì„±ì¥ë¥ ': 3,
        'ë¬¼ê°€': 3,
        'ì¸í”Œë ˆì´ì…˜': 4,
        'CPI': 3,
        'ì†Œë¹„ìë¬¼ê°€': 3,
        'ì‹¤ì—…ë¥ ': 3,
        'ê³ ìš©': 2,

        # ë¶€ë™ì‚°
        'ë¶€ë™ì‚°': 2,
        'ì§‘ê°’': 2,
        'ì•„íŒŒíŠ¸': 2,

        # ê¸°ì—…/ì‚°ì—…
        'ì‚¼ì„±': 2,
        'í˜„ëŒ€': 2,
        'LG': 2,
        'ë°˜ë„ì²´': 2,
        'ìë™ì°¨': 2,
    }

    @staticmethod
    def calculate_score(article: NewsArticle) -> float:
        """ê¸°ì‚¬ ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚° (0~100ì )"""
        score = 0.0

        # === 1. í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ (ìµœëŒ€ 50ì ) ===
        title_lower = article.title.lower()
        content_lower = article.content.lower()

        keyword_score = 0
        for keyword, weight in ImportanceRanker.HIGH_PRIORITY_KEYWORDS.items():
            if keyword.lower() in title_lower:
                keyword_score += weight * 3  # ì œëª©ì— ìˆìœ¼ë©´ 3ë°° ê°€ì¤‘
            elif keyword.lower() in content_lower:
                keyword_score += weight

        # í‚¤ì›Œë“œ ì ìˆ˜ ì •ê·œí™” (ìµœëŒ€ 50ì )
        keyword_score = min(keyword_score, 50)
        score += keyword_score

        # === 2. ë³¸ë¬¸ í’ˆì§ˆ ì ìˆ˜ (ìµœëŒ€ 20ì ) ===
        content_len = len(article.content)

        if content_len < 300:
            quality_score = 0  # ë„ˆë¬´ ì§§ìŒ
        elif 300 <= content_len < 800:
            quality_score = 10  # ì§§ìŒ
        elif 800 <= content_len < 3000:
            quality_score = 20  # ì ë‹¹
        else:
            quality_score = 15  # ë„ˆë¬´ ê¹€ (ìš”ì•½í•˜ê¸° ì–´ë ¤ì›€)

        score += quality_score

        # === 3. ìˆ«ì/ë°ì´í„° í¬í•¨ ì ìˆ˜ (ìµœëŒ€ 15ì ) ===
        # í†µê³„ ê¸°ì‚¬ëŠ” ì‹ ë¢°ë„ê°€ ë†’ìŒ
        numbers = re.findall(r'\d+\.?\d*%?', article.content)

        if len(numbers) >= 5:
            data_score = 15
        elif len(numbers) >= 3:
            data_score = 10
        elif len(numbers) >= 1:
            data_score = 5
        else:
            data_score = 0

        score += data_score

        # === 4. ì œëª© í’ˆì§ˆ ì ìˆ˜ (ìµœëŒ€ 15ì ) ===
        title_len = len(article.title)

        if 15 <= title_len <= 50:
            title_score = 15  # ì ë‹¹í•œ ê¸¸ì´
        elif 10 <= title_len < 15 or 50 < title_len <= 70:
            title_score = 10  # ì•½ê°„ ì§§ê±°ë‚˜ ê¹€
        else:
            title_score = 5  # ë„ˆë¬´ ì§§ê±°ë‚˜ ê¹€

        score += title_score

        return round(score, 1)

    @staticmethod
    def rank_articles(articles: List[NewsArticle], top_n: int = 5) -> List[NewsArticle]:
        """ê¸°ì‚¬ë“¤ì„ ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬ í›„ ìƒìœ„ Nê°œ ë°˜í™˜"""
        if not articles:
            return []

        # ì ìˆ˜ ê³„ì‚°
        scored_articles = []
        for article in articles:
            score = ImportanceRanker.calculate_score(article)
            scored_articles.append((article, score))

        # ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
        scored_articles.sort(key=lambda x: x[1], reverse=True)

        # ê²°ê³¼ ì¶œë ¥
        print(f"\nì¤‘ìš”ë„ ë­í‚¹ (ìƒìœ„ {min(top_n, len(scored_articles))}ê°œ):")
        print("=" * 80)

        for i, (article, score) in enumerate(scored_articles[:top_n], 1):
            print(f"{i}. [{score:5.1f}ì ] {article.source:6s} | {article.title[:50]}")

        print("=" * 80)

        # ìƒìœ„ Nê°œ ë°˜í™˜
        top_articles = [article for article, score in scored_articles[:top_n]]
        return top_articles

    @staticmethod
    def get_score_breakdown(article: NewsArticle) -> dict:
        """ì ìˆ˜ ì„¸ë¶€ ë‚´ì—­ í™•ì¸ìš©"""
        breakdown = {
            'total': 0,
            'keyword': 0,
            'quality': 0,
            'data': 0,
            'title': 0
        }

        # í‚¤ì›Œë“œ ì ìˆ˜
        title_lower = article.title.lower()
        content_lower = article.content.lower()

        keyword_score = 0
        matched_keywords = []
        for keyword, weight in ImportanceRanker.HIGH_PRIORITY_KEYWORDS.items():
            if keyword.lower() in title_lower:
                keyword_score += weight * 3
                matched_keywords.append(f"{keyword}(ì œëª©)")
            elif keyword.lower() in content_lower:
                keyword_score += weight
                matched_keywords.append(keyword)

        breakdown['keyword'] = min(keyword_score, 50)
        breakdown['matched_keywords'] = matched_keywords

        # ë³¸ë¬¸ í’ˆì§ˆ
        content_len = len(article.content)
        if content_len < 300:
            breakdown['quality'] = 0
        elif 300 <= content_len < 800:
            breakdown['quality'] = 10
        elif 800 <= content_len < 3000:
            breakdown['quality'] = 20
        else:
            breakdown['quality'] = 15

        # ë°ì´í„° ì ìˆ˜
        numbers = re.findall(r'\d+\.?\d*%?', article.content)
        if len(numbers) >= 5:
            breakdown['data'] = 15
        elif len(numbers) >= 3:
            breakdown['data'] = 10
        elif len(numbers) >= 1:
            breakdown['data'] = 5

        # ì œëª© ì ìˆ˜
        title_len = len(article.title)
        if 15 <= title_len <= 50:
            breakdown['title'] = 15
        elif 10 <= title_len < 15 or 50 < title_len <= 70:
            breakdown['title'] = 10
        else:
            breakdown['title'] = 5

        # ì´ì 
        breakdown['total'] = breakdown['keyword'] + breakdown['quality'] + breakdown['data'] + breakdown['title']

        return breakdown
```

**2. í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±**
íŒŒì¼ ê²½ë¡œ: `g:\ë‚´ ë“œë¼ì´ë¸Œ\08.Programming\spread_insight\test_ranking.py`
```python
from utils.file_manager import FileManager
from analyzers.importance_ranker import ImportanceRanker


if __name__ == '__main__':
    print("=" * 80)
    print("ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚° ë° ë­í‚¹ í…ŒìŠ¤íŠ¸")
    print("=" * 80)

    # íŒŒì¼ ë§¤ë‹ˆì € ë° ë­ì»¤ ì´ˆê¸°í™”
    file_manager = FileManager()

    # ì €ì¥ëœ ê¸°ì‚¬ ë¡œë“œ
    saved_files = file_manager.list_saved_files()
    if not saved_files:
        print("âŒ ì €ì¥ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € test_multi_scraper.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        exit(1)

    # ì—¬ëŸ¬ íŒŒì¼ì˜ ê¸°ì‚¬ë¥¼ ëª¨ë‘ ë¡œë“œ
    all_articles = []
    print("\nê¸°ì‚¬ ë¡œë“œ ì¤‘...")
    for filename in saved_files[:3]:  # ìµœê·¼ 3ê°œ íŒŒì¼
        try:
            articles = file_manager.load_articles(filename)
            all_articles.extend(articles)
            print(f"  âœ… {filename}: {len(articles)}ê°œ")
        except Exception as e:
            print(f"  âŒ {filename}: {e}")

    print(f"\nì´ {len(all_articles)}ê°œ ê¸°ì‚¬ ë¡œë“œ ì™„ë£Œ\n")

    # ì¤‘ìš”ë„ ë­í‚¹
    top_articles = ImportanceRanker.rank_articles(all_articles, top_n=5)

    # ìƒìœ„ 3ê°œ ê¸°ì‚¬ì˜ ì ìˆ˜ ì„¸ë¶€ ë‚´ì—­
    print("\n\nìƒìœ„ 3ê°œ ê¸°ì‚¬ ì ìˆ˜ ì„¸ë¶€ ë‚´ì—­:")
    print("=" * 80)

    for i, article in enumerate(top_articles[:3], 1):
        breakdown = ImportanceRanker.get_score_breakdown(article)

        print(f"\n{i}. {article.title}")
        print(f"   ì¶œì²˜: {article.source} | ë‚ ì§œ: {article.published_at.strftime('%Y-%m-%d')}")
        print(f"   ë³¸ë¬¸ ê¸¸ì´: {len(article.content)}ì")
        print(f"\n   ì ìˆ˜ ì„¸ë¶€:")
        print(f"     â€¢ í‚¤ì›Œë“œ: {breakdown['keyword']}/50ì ")
        if breakdown.get('matched_keywords'):
            print(f"       ë§¤ì¹­: {', '.join(breakdown['matched_keywords'][:5])}")
        print(f"     â€¢ ë³¸ë¬¸ í’ˆì§ˆ: {breakdown['quality']}/20ì ")
        print(f"     â€¢ ë°ì´í„°: {breakdown['data']}/15ì ")
        print(f"     â€¢ ì œëª©: {breakdown['title']}/15ì ")
        print(f"     ---")
        print(f"     ì´ì : {breakdown['total']}/100ì ")
        print("-" * 80)

    # ìƒìœ„ 5ê°œ ì €ì¥
    print("\n\nìƒìœ„ 5ê°œ ê¸°ì‚¬ ì €ì¥ ì¤‘...")
    file_manager.save_articles(top_articles, filename='top_news.json')
    print("âœ… ì €ì¥ ì™„ë£Œ: top_news.json")
```

**3. ì‹¤í–‰**
```bash
python test_ranking.py
```

### âœ… ì„±ê³µ ê¸°ì¤€
- [ ] ëª¨ë“  ê¸°ì‚¬ì˜ ì ìˆ˜ ê³„ì‚° ì™„ë£Œ
- [ ] ì ìˆ˜ ê¸°ë°˜ ì •ë ¬ ë™ì‘
- [ ] ìƒìœ„ 5ê°œ ê¸°ì‚¬ ì„ ì •
- [ ] ì ìˆ˜ ì„¸ë¶€ ë‚´ì—­ í‘œì‹œ
- [ ] `data/raw/top_news.json` íŒŒì¼ ìƒì„±

### âš ï¸ ì˜ˆìƒ ì˜¤ë¥˜ ë° í•´ê²°

**ì˜¤ë¥˜ 1:** ëª¨ë“  ê¸°ì‚¬ ì ìˆ˜ê°€ ë‚®ìŒ (10ì  ë¯¸ë§Œ)
- **ì›ì¸:** í‚¤ì›Œë“œ ë§¤ì¹­ì´ ì•ˆ ë¨
- **í•´ê²°:** `HIGH_PRIORITY_KEYWORDS`ì— ë” ë‹¤ì–‘í•œ í‚¤ì›Œë“œ ì¶”ê°€

**ì˜¤ë¥˜ 2:** íŠ¹ì • ì¶œì²˜ì˜ ê¸°ì‚¬ë§Œ ìƒìœ„ê¶Œ
- **ì›ì¸:** í•´ë‹¹ ì¶œì²˜ê°€ í‚¤ì›Œë“œë¥¼ ë§ì´ ì‚¬ìš©
- **í•´ê²°:** ì¶œì²˜ ë‹¤ì–‘ì„± ë³´ë„ˆìŠ¤ ì¶”ê°€ (ê³ ê¸‰ ê¸°ëŠ¥)

**ì˜¤ë¥˜ 3:** ì ìˆ˜ê°€ ë„ˆë¬´ ë¹„ìŠ·í•¨ (ì°¨ì´ 1ì  ë¯¸ë§Œ)
- **ì›ì¸:** ê°€ì¤‘ì¹˜ ì¡°ì • í•„ìš”
- **í•´ê²°:** `HIGH_PRIORITY_KEYWORDS`ì˜ ê°€ì¤‘ì¹˜ ê°’ ì¡°ì •

---

## ğŸ‰ Phase 2 ì™„ë£Œ!

ë‹¤ìŒ ë‹¨ê³„: [Phase 3: ë°ì´í„° ì‹œê°í™”](PHASE3_VISUALIZATION.md)
